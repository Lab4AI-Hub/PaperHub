# PaperHub: é«˜è´¨é‡çš„è®ºæ–‡å¤ç°ä¸­å¿ƒ

æ¬¢è¿æ¥åˆ° **PaperHub**ï¼è¿™æ˜¯ **Lab4AI-Hub** ç¤¾åŒºçš„ç®—æ³•å¤ç°åº“ï¼Œè‡´åŠ›äºæä¾›é«˜è´¨é‡çš„AIç®—æ³•å¤ç°ã€‚

---

<div align="center" style="padding: 20px; background-color: #F0F9FF; border: 2px solid #3B82F6; border-radius: 8px;">
  <h2 style="margin-top:0; color: #1E40AF;">å‰å¾€â€œå¤§æ¨¡å‹å®éªŒå®¤â€è·å–æœ€ä½³ä½“éªŒ</h2>
  <p>æ‰€æœ‰é¡¹ç›®å‡å¯åœ¨æˆ‘ä»¬çš„â€œå¤§æ¨¡å‹å®éªŒå®¤â€å†…å®¹ç¤¾åŒºä¸­æ‰¾åˆ°å¯¹åº”çš„æ•™ç¨‹å’Œä¸€é”®å¼è¿è¡Œç¯å¢ƒï¼Œä¸ºæ‚¨æä¾›å¼ºå¤§çš„ç®—åŠ›æ”¯æŒã€‚</p>
  <br>
  <a href="https://www.lab4ai.cn/home" style="display: inline-block; padding: 12px 24px; background-color: #2563EB; color: white; text-decoration: none; font-weight: bold; border-radius: 6px; font-size: 16px;">
    è®¿é—®å¤§æ¨¡å‹å®éªŒå®¤ â¡ï¸
  </a>
</div>

---

## ğŸ‘‹ å¦‚ä½•æˆä¸ºè´¡çŒ®è€…ï¼Ÿ(How to Contribute)

æˆ‘ä»¬çƒ­çƒˆæ¬¢è¿æ¯ä¸€ä½å¯¹AIå……æ»¡çƒ­æƒ…çš„ä½ åŠ å…¥æˆ‘ä»¬çš„è´¡çŒ®è€…è¡Œåˆ—ï¼æˆ‘ä»¬ä¸ºæ‚¨è®¾è®¡äº†ä¸€å¥—æ¸…æ™°ã€æ ‡å‡†åŒ–çš„åä½œæµç¨‹ï¼Œå¹¶æä¾›ä¸°åšçš„ç®—åŠ›å¥–åŠ±ã€‚
<table align="center" width="100%" style="border: none; margin-top: 20px; margin-bottom: 20px;">
  <tr style="border: none;">
    <td width="33.3%" align="left" style="padding: 15px; vertical-align: top;">
      <img src="https://api.iconify.design/ph/book-open-bold.svg?color=gray" width="40">
      <h3 style="margin-top: 10px; margin-bottom: 10px;">ç¬¬ä¸€æ­¥ï¼šé˜…è¯»æŒ‡å—</h3>
      <p>è¿™æ˜¯<strong>ã€å¿…åšä¹‹äº‹ã€‘</strong>ï¼è¯·é¦–å…ˆä»”ç»†é˜…è¯»æˆ‘ä»¬è¯¦ç»†çš„<a href="https://github.com/Lab4AI-Hub/PaperHub/blob/main/CONTRIBUTING.md"><strong>è´¡çŒ®è€…æŒ‡å—</strong></a>ï¼Œå®ƒå°†å‘Šè¯‰æ‚¨æ‰€æœ‰æµç¨‹ç»†èŠ‚ã€å¤ç°æ ‡å‡†å’Œå¥–åŠ±è§„åˆ™ã€‚</p>
    </td>
    <td width="33.3%" align="left" style="padding: 15px; vertical-align: top;">
      <img src="https://api.iconify.design/ph/note-pencil-bold.svg?color=gray" width="40">
      <h3 style="margin-top: 10px; margin-bottom: 10px;">ç¬¬äºŒæ­¥ï¼šå‡†å¤‡ç”³è¯·</h3>
      <p>æ‚¨å¯ä»¥ä»æˆ‘ä»¬çš„<a href="https://lab4ai-hub.github.io/PaperHub/"><strong>å¾…å¤ç°æ¸…å•</strong></a>ä¸­é€‰æ‹©è¯¾é¢˜ï¼Œä¹Ÿå¯ä»¥æ¨èæ‚¨è‡ªå·±æ„Ÿå…´è¶£çš„è®ºæ–‡ã€‚æ— è®ºå“ªç§æ–¹å¼ï¼Œéƒ½éœ€ä¸‹è½½å¹¶å¡«å†™<strong>ã€Š2-è®ºæ–‡ç­›é€‰è¡¨ã€‹<strong>ã€‚</p>
    </td>
    <td width="33.3%" align="left" style="padding: 15px; vertical-align: top;">
      <img src="https://api.iconify.design/ph/rocket-launch-bold.svg?color=gray" width="40">
      <h3 style="margin-top: 10px; margin-bottom: 10px;">ç¬¬ä¸‰æ­¥ï¼šæäº¤ç”³è¯·</h3>
      <p>å‡†å¤‡å¥½å¡«å†™å®Œæ•´çš„è¡¨æ ¼åï¼Œè¯·å‰å¾€æˆ‘ä»¬çš„ <a href="https://github.com/Lab4AI-Hub/PaperHub/issues"><strong>IssueåŒº</strong></a>ï¼Œé€‰æ‹©å¯¹åº”çš„æ¨¡æ¿ï¼Œæäº¤æ‚¨çš„æ­£å¼ç”³è¯·ï¼Œå¼€å¯æ‚¨çš„å¤ç°ä¹‹æ—…ï¼</p>
    </td>
  </tr>
</table>

---

### âœ… å·²å®Œæˆçš„å¤ç°é¡¹ç›® (Completed Reproductions)

| è®ºæ–‡åç§° & ä½œè€… | ä¼šè®®æ¥æº & å¹´ä»½ | è®ºæ–‡é“¾æ¥ | å‰å¾€å¹³å°ä½“éªŒ |
| :--- | :--- | :--- | :--- |
| **Attention Is All You Need** <br> *Ashish Vaswani, et al.* | NeurIPS 2017| [ğŸ“„ arXiv](https://arxiv.org/abs/1706.03762) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=e90aa38fdff9420e8902bc71909fa005&type=paper) |
| **Can We Get Rid of Handcrafted Feature Extractors?** <br> *Lei Su, et al.* | AAAI 2025| [ğŸ“„ arXiv](https://arxiv.org/abs/2412.14598) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=97a182e56e904e92a0fe240f1f114709&type=paper) |
| **MOMENT: A Family of Open Time-series Foundation Models** <br> *Mononito Goswami, et al.* | ICML 2025| [ğŸ“„ arXiv](https://arxiv.org/abs/2402.03885) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=05087484a3264a9c8b8a2c616e7cce0b&type=paper) |
| **Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data** <br> *Kashun Shum,et al.* | EMNLP 2023 | [ğŸ“„ arXiv](https://arxiv.org/abs/2302.12822) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=c76b88e732cf41949b54515bdd319808&type=paper) |
| **Chronos: Learning the Language of Time Series** <br> *Abdul Fatir Ansari, et al.* | other 2024| [ğŸ“„ arXiv](https://arxiv.org/pdf/2403.07815) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=6dd7daeec6584f61856876474b860e09&type=paper) |
| **Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis** <br> *Yu Yuan, et al.* | CVPR 2025| [ğŸ“„ arXiv](https://arxiv.org/abs/2412.02168) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=2d412a08880f477ca5362af3ef8c14f2&type=paper) |
| **PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data** <br> *Shijie Huang, et al.* | ICCV 2025| [ğŸ“„ arXiv](https://arxiv.org/abs/2502.14397) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=673e78d51fcc4bca89b636a52affa6b4&type=paper) |
| **Self-Instruct: Aligning Language Models with Self-Generated Instructions** <br> *Yizhong Wang, et al.* | ACL 2023| [ğŸ“„ arXiv](https://arxiv.org/abs/2212.10560) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=2bbf2f4971f74c6e8def26879233f2fe&type=paper) |
| **RobustSAM: Segment Anything Robustly on Degraded Images** <br> *Wei-Ting Chen, et al.* | CVPR 2024| [ğŸ“„ arXiv](https://arxiv.org/abs/2406.09627) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=7a23bf525a38476c952df14e72ecce23&type=paper) |
| **Side Adapter Network for Open-Vocabulary Semantic Segmentation** <br> *Mengde Xu, et al.* |CVPR 2023| [ğŸ“„ arXiv](https://arxiv.org/pdf/2302.12242) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=5944c280114e41508d99e1fd85cbf78e&type=paper) |
| **Improving day-ahead Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context** <br> *Oussama Boussif, et al.* | NIPS 2023| [ğŸ“„ arXiv](https://arxiv.org/abs/2306.01112) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=77ab6c82cc9444938c4bbbdd6709709a&type=paper) |
| **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting** <br> *Kashif Rasul, et al.* | other 2023| [ğŸ“„ arXiv](https://arxiv.org/abs/2310.08278) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=a05e09588c4c475aac14354cae04986d&type=paper) |
| **CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos** <br> *Lei Nikita Karaev, et al.* | CVPR 2024| [ğŸ“„ arXiv](https://arxiv.org/abs/2410.11831) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=269a8dc6d24a49b29d39de138b443a43&type=paper) |
| **Unified Training of Universal Time Series Forecasting Transformers** <br> *Gerald Woo, et al.* | ICML 2024| [ğŸ“„ arXiv](https://arxiv.org/abs/2402.02592) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=af34d430edf14f56910c6cfc05c4ee89&type=paper) |
| **A decoder-only foundation model for time-series forecasting** <br> *Abhimanyu Das, et al.* | ICML 2024| [ğŸ“„ arXiv](https://arxiv.org/abs/2310.10688) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=e78bad4ea95944af8fa0eeb98f69179f&type=paper) |
| **Timer: Generative Pre-trained Transformers Are Large Time Series Models** <br> *Yong Liu, et al.* | ICML 2024| [ğŸ“„ arXiv](https://arxiv.org/abs/2402.02368) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=34d7c1e35c514d77b88762f18298e999&type=paper) |
| **LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant** <br> *Yikun Liu, et al.* | other 2024| [ğŸ“„ arXiv](https://arxiv.org/abs/2412.01720) | [â¡ï¸ **ç«‹å³ä½“éªŒ**](https://www.lab4ai.cn/paper/detail?id=3c0069c96f60404b948ed30fd498fe7f&type=paper) |




---

### ğŸ—ºï¸ ç¤¾åŒºå¤ç°è·¯çº¿å›¾ (Community Roadmap)

æˆ‘ä»¬å·²ç»ç­›é€‰å¹¶æ•´ç†äº†ä¸€ä»½è¯¦ç»†çš„å¾…å¤ç°è®ºæ–‡æ¸…å•ã€‚è¿™ä¸ä»…æ˜¯æˆ‘ä»¬çš„å·¥ä½œè®¡åˆ’ï¼Œæ›´æ˜¯æˆ‘ä»¬é‚€è¯·æ‚¨å‚ä¸å…±å»ºçš„è“å›¾ã€‚

<div align="center" style="margin-top: 20px;">
  <a href="https://lab4ai-hub.github.io/PaperHub/" style="display: inline-block; padding: 12px 24px; border: 2px solid #4B5563; color: #1F2937; text-decoration: none; font-weight: bold; border-radius: 6px;">
    æŸ¥çœ‹å¾…å¤ç°è®ºæ–‡æ¸…å• (Roadmap)
  </a>
</div>

<br>

å¦‚æœæ‚¨å¯¹è·¯çº¿å›¾ä¸­çš„é¡¹ç›®æ„Ÿå…´è¶£ï¼Œæˆ–æœ‰æ–°çš„è®ºæ–‡æ¨èï¼Œè¯·åœ¨æˆ‘ä»¬çš„ [**Issue åˆ—è¡¨**](https://github.com/Lab4AI-Hub/PaperHub/issues) ä¸­å‘èµ·è®¨è®ºã€‚


