# PaperHub: 高质量的论文复现中心

欢迎来到 **PaperHub**！这是 **Lab4AI-Hub** 社区的算法复现库，致力于提供高质量的AI算法复现。

---

<div align="center" style="padding: 20px; background-color: #F0F9FF; border: 2px solid #3B82F6; border-radius: 8px;">
  <h2 style="margin-top:0; color: #1E40AF;">前往“大模型实验室”获取最佳体验</h2>
  <p>所有项目均可在我们的“大模型实验室”内容社区中找到对应的教程和一键式运行环境，为您提供强大的算力支持。</p>
  <br>
  <a href="https://www.lab4ai.cn/home" style="display: inline-block; padding: 12px 24px; background-color: #2563EB; color: white; text-decoration: none; font-weight: bold; border-radius: 6px; font-size: 16px;">
    访问大模型实验室 ➡️
  </a>
</div>

---

## 👋 如何成为贡献者？(How to Contribute)

我们热烈欢迎每一位对AI充满热情的你加入我们的贡献者行列！我们为您设计了一套清晰、标准化的协作流程，并提供丰厚的算力奖励。
<table align="center" width="100%" style="border: none; margin-top: 20px; margin-bottom: 20px;">
  <tr style="border: none;">
    <td width="33.3%" align="left" style="padding: 15px; vertical-align: top;">
      <img src="https://api.iconify.design/ph/book-open-bold.svg?color=gray" width="40">
      <h3 style="margin-top: 10px; margin-bottom: 10px;">第一步：阅读指南</h3>
      <p>这是<strong>【必做之事】</strong>！请首先仔细阅读我们详细的<a href="https://github.com/Lab4AI-Hub/PaperHub/blob/main/CONTRIBUTING.md"><strong>贡献者指南</strong></a>，它将告诉您所有流程细节、复现标准和奖励规则。</p>
    </td>
    <td width="33.3%" align="left" style="padding: 15px; vertical-align: top;">
      <img src="https://api.iconify.design/ph/note-pencil-bold.svg?color=gray" width="40">
      <h3 style="margin-top: 10px; margin-bottom: 10px;">第二步：准备申请</h3>
      <p>您可以从我们的<a href="https://lab4ai-hub.github.io/PaperHub/"><strong>待复现清单</strong></a>中选择课题，也可以推荐您自己感兴趣的论文。无论哪种方式，都需下载并填写<strong>《2-论文筛选表》<strong>。</p>
    </td>
    <td width="33.3%" align="left" style="padding: 15px; vertical-align: top;">
      <img src="https://api.iconify.design/ph/rocket-launch-bold.svg?color=gray" width="40">
      <h3 style="margin-top: 10px; margin-bottom: 10px;">第三步：提交申请</h3>
      <p>准备好填写完整的表格后，请前往我们的 <a href="https://github.com/Lab4AI-Hub/PaperHub/issues"><strong>Issue区</strong></a>，选择对应的模板，提交您的正式申请，开启您的复现之旅！</p>
    </td>
  </tr>
</table>

---

### ✅ 已完成的复现项目 (Completed Reproductions)

| 论文名称 & 作者 | 会议来源 & 年份 | 论文链接 | 前往平台体验 |
| :--- | :--- | :--- | :--- |
| **Attention Is All You Need** <br> *Ashish Vaswani, et al.* | NeurIPS 2017| [📄 arXiv](https://arxiv.org/abs/1706.03762) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=e90aa38fdff9420e8902bc71909fa005&type=paper) |
| **Can We Get Rid of Handcrafted Feature Extractors?** <br> *Lei Su, et al.* | AAAI 2025| [📄 arXiv](https://arxiv.org/abs/2412.14598) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=97a182e56e904e92a0fe240f1f114709&type=paper) |
| **MOMENT: A Family of Open Time-series Foundation Models** <br> *Mononito Goswami, et al.* | ICML 2025| [📄 arXiv](https://arxiv.org/abs/2402.03885) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=05087484a3264a9c8b8a2c616e7cce0b&type=paper) |
| **Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data** <br> *Kashun Shum,et al.* | EMNLP 2023 | [📄 arXiv](https://arxiv.org/abs/2302.12822) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=c76b88e732cf41949b54515bdd319808&type=paper) |
| **Chronos: Learning the Language of Time Series** <br> *Abdul Fatir Ansari, et al.* | other 2024| [📄 arXiv](https://arxiv.org/pdf/2403.07815) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=6dd7daeec6584f61856876474b860e09&type=paper) |
| **Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis** <br> *Yu Yuan, et al.* | CVPR 2025| [📄 arXiv](https://arxiv.org/abs/2412.02168) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=2d412a08880f477ca5362af3ef8c14f2&type=paper) |
| **PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data** <br> *Shijie Huang, et al.* | ICCV 2025| [📄 arXiv](https://arxiv.org/abs/2502.14397) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=673e78d51fcc4bca89b636a52affa6b4&type=paper) |
| **Self-Instruct: Aligning Language Models with Self-Generated Instructions** <br> *Yizhong Wang, et al.* | ACL 2023| [📄 arXiv](https://arxiv.org/abs/2212.10560) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=2bbf2f4971f74c6e8def26879233f2fe&type=paper) |
| **RobustSAM: Segment Anything Robustly on Degraded Images** <br> *Wei-Ting Chen, et al.* | CVPR 2024| [📄 arXiv](https://arxiv.org/abs/2406.09627) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=7a23bf525a38476c952df14e72ecce23&type=paper) |
| **Side Adapter Network for Open-Vocabulary Semantic Segmentation** <br> *Mengde Xu, et al.* |CVPR 2023| [📄 arXiv](https://arxiv.org/pdf/2302.12242) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=5944c280114e41508d99e1fd85cbf78e&type=paper) |
| **Improving day-ahead Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context** <br> *Oussama Boussif, et al.* | NIPS 2023| [📄 arXiv](https://arxiv.org/abs/2306.01112) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=77ab6c82cc9444938c4bbbdd6709709a&type=paper) |
| **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting** <br> *Kashif Rasul, et al.* | other 2023| [📄 arXiv](https://arxiv.org/abs/2310.08278) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=a05e09588c4c475aac14354cae04986d&type=paper) |
| **CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos** <br> *Lei Nikita Karaev, et al.* | CVPR 2024| [📄 arXiv](https://arxiv.org/abs/2410.11831) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=269a8dc6d24a49b29d39de138b443a43&type=paper) |
| **Unified Training of Universal Time Series Forecasting Transformers** <br> *Gerald Woo, et al.* | ICML 2024| [📄 arXiv](https://arxiv.org/abs/2402.02592) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=af34d430edf14f56910c6cfc05c4ee89&type=paper) |
| **A decoder-only foundation model for time-series forecasting** <br> *Abhimanyu Das, et al.* | ICML 2024| [📄 arXiv](https://arxiv.org/abs/2310.10688) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=e78bad4ea95944af8fa0eeb98f69179f&type=paper) |
| **Timer: Generative Pre-trained Transformers Are Large Time Series Models** <br> *Yong Liu, et al.* | ICML 2024| [📄 arXiv](https://arxiv.org/abs/2402.02368) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=34d7c1e35c514d77b88762f18298e999&type=paper) |
| **LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant** <br> *Yikun Liu, et al.* | other 2024| [📄 arXiv](https://arxiv.org/abs/2412.01720) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=3c0069c96f60404b948ed30fd498fe7f&type=paper) |




---

### 🗺️ 社区复现路线图 (Community Roadmap)

我们已经筛选并整理了一份详细的待复现论文清单。这不仅是我们的工作计划，更是我们邀请您参与共建的蓝图。

<div align="center" style="margin-top: 20px;">
  <a href="https://lab4ai-hub.github.io/PaperHub/" style="display: inline-block; padding: 12px 24px; border: 2px solid #4B5563; color: #1F2937; text-decoration: none; font-weight: bold; border-radius: 6px;">
    查看待复现论文清单 (Roadmap)
  </a>
</div>

<br>

如果您对路线图中的项目感兴趣，或有新的论文推荐，请在我们的 [**Issue 列表**](https://github.com/Lab4AI-Hub/PaperHub/issues) 中发起讨论。


