# PaperHub: 高质量的论文复现中心

欢迎来到 **PaperHub**！这是 **Lab4AI-Hub** 社区的算法复现库，致力于提供高质量的AI算法复现。

---

<div align="center" style="padding: 20px; background-color: #F0F9FF; border: 2px solid #3B82F6; border-radius: 8px;">
  <h2 style="margin-top:0; color: #1E40AF;">前往“大模型实验室”获取最佳体验</h2>
  <p>所有项目均可在我们的“大模型实验室”内容社区中找到对应的教程和一键式运行环境，为您提供强大的算力支持。</p>
  <br>
  <a href="https://www.lab4ai.cn/home" style="display: inline-block; padding: 12px 24px; background-color: #2563EB; color: white; text-decoration: none; font-weight: bold; border-radius: 6px; font-size: 16px;">
    访问大模型实验室 ➡️
  </a>
</div>

---

## 👋 如何贡献一篇论文复现？

我们热烈欢迎每一位对AI充满热情的你，加入我们的论文复现贡献者行列！我们为您设计了一套清晰的贡献路径，并准备了丰厚的算力奖励，期待您的参与能点亮社区。

**【关于提交流程的重要说明】**：我们正在持续优化贡献者体验，未来将上线**平台内的一站式创作与提交**功能。在此之前，请暂时按照以下流程进行成果提交。

### **贡献步骤概览**

<table width="100%" align="center" style="border: none;">
  <tr style="border: none;">
    <td align="center" width="10%" style="vertical-align: top; padding-top: 20px;">
      <img src="https://api.iconify.design/material-symbols/filter-1-rounded.svg?color=%238957e5" width="40">
    </td>
    <td width="90%" style="padding: 10px 10px 20px 10px;">
      <h3 style="margin-top: 0px; margin-bottom: 5px;">第一步：选题与申请</h3>
      <p style="color: #57606a;">
        <strong>前置要求：</strong>请先为本项目点亮一颗 <strong>Star ⭐</strong>！
        <br>
        您可以从<a href="https://lab4ai-hub.github.io/PaperHub/">官方清单</a>选择或推荐新论文（推荐需先微信联系）。确定后，请通过 <a href="【请替换为PaperHub的Issue区链接】"><strong>提交Issue</strong></a> 的方式进行正式申请。
      </p>
    </td>
  </tr>
  <tr style="border: none;">
    <td align="center" width="10%" style="vertical-align: top; padding-top: 20px;">
      <img src="https://api.iconify.design/material-symbols/filter-2-rounded.svg?color=%238957e5" width="40">
    </td>
    <td width="90%" style="padding: 10px 10px 20px 10px;">
      <h3 style="margin-top: 0px; margin-bottom: 5px;">第二步：在线复现</h3>
      <p style="color: #57606a;">
        申请通过后，您将获得启动算力。请根据<a href="./docs/WORKFLOW.md"><strong>《复现者指南》</strong></a>中的文件结构要求，在**您自己的Lab4AI平台工作空间**内开始复现。
      </p>
    </td>
  </tr>
  <tr style="border: none;">
    <td align="center" width="10%" style="vertical-align: top; padding-top: 20px;">
      <img src="https://api.iconify.design/material-symbols/filter-3-rounded.svg?color=%238957e5" width="40">
    </td>
    <td width="90%" style="padding: 10px 10px 20px 10px;">
      <h3 style="margin-top: 0px; margin-bottom: 5px;">第三步：提交审核</h3>
      <p style="color: #57606a;">
        复现完成后，请按照<a href="./docs/DELIVERABLES.md"><strong>《成果提交说明》</strong></a>，准备好所有材料，特别是填写完整的**《论文上架信息表》**。
        <br>
        然后，请回到您当初申请任务的**GitHub Issue**，在评论区 **@ 我们的管理员**，并将**《论文上架信息表》作为附件上传**，通知我们进行审核。
        <br>
        *（我们正在努力开发平台内的在线提交通道，未来将更加便捷！）*
      </p>
    </td>
  </tr>
  <tr style="border: none;">
    <td align="center" width="10%" style="vertical-align: top; padding-top: 20px;">
      <img src="https://api.iconify.design/material-symbols/filter-4-rounded.svg?color=%238957e5" width="40">
    </td>
    <td width="90%" style="padding: 10px 10px 20px 10px;">
      <h3 style="margin-top: 0px; margin-bottom: 5px;">第四步：成功发布</h3>
      <p style="color: #57606a;">
        审核通过后，您的成果将被官方收录和展示，同时您将根据<a href="./docs/REWARDS.md"><strong>《创作者激励计划》</strong></a>获得丰厚奖励！
      </p>
    </td>
  </tr>
</table>

### **核心文档库 & 申请入口**

在开始您的贡献之旅前，请务必仔细阅读以下核心指南。**所有详细信息和申请入口都在这里**。

<table width="100%" style="border: none; margin-top:15px;">
  <tr style="border: none;">
    <td width="50%" style="padding: 10px; vertical-align: top;">
      <h4 style="margin-top: 0px; margin-bottom: 5px;"><a href="./docs/WORKFLOW.md">📄 复现者指南 (Reproducer's Guide)</a></h4>
      <p style="color: #57606a; font-size: 0.9em;"><em><strong>行动手册(SOP)</strong>：详细介绍从申请到提交的每一步。</em></p>
    </td>
    <td width="50%" style="padding: 10px; vertical-align: top;">
      <h4 style="margin-top: 0px; margin-bottom: 5px;"><a href="./docs/DELIVERABLES.md">📝 成果提交说明 (Deliverables Guide)</a></h4>
      <p style="color: #57606a; font-size: 0.9em;"><em><strong>交付物清单</strong>：清晰列出您需要准备的文件，以及需要**填写并附加到Issue**的上架信息表。</em></p>
    </td>
  </tr>
  <tr style="border: none;">
    <td width="50%" style="padding: 10px; vertical-align: top;">
      <h4 style="margin-top: 0px; margin-bottom: 5px;"><a href="./docs/CRITERIA.md">✅ 论文筛选标准 (Criteria)</a></h4>
      <p style="color: #57606a; font-size: 0.9em;"><em><strong>选题准则</strong>：定义了一篇论文是否值得被复现的前置条件。</em></p>
    </td>
    <td width="50%" style="padding: 10px; vertical-align: top;">
      <h4 style="margin-top: 0px; margin-bottom: 5px;"><a href="./docs/REWARDS.md">💎 创作者激励计划 (Rewards)</a></h4>
      <p style="color: #57606a; font-size: 0.9em;"><em><strong>权益手册</strong>：详细说明不同贡献所能获得的丰厚算力奖励。</em></p>
    </td>
  </tr>
</table>

<p align="center" style="margin-top: 20px; margin-bottom: 20px;">
  <a href="【请替换为PaperHub的Issue申请模板链接】">
    <img src="https://img.shields.io/badge/✅%20%E6%88%91%E5·І%E9%98…%E8Ї%BB%E6%8C‡%E5%8D—%EF%BC%8C%E5%BC%80%E5%A7‹%E7”%B3%E8Ї%B7-238636?style=for-the-badge&logo=github" alt="开始申请">
  </a>
</p>

---

### ✅ 已完成的复现项目 (Completed Reproductions)

| 论文名称 & 作者 | 会议来源 & 年份 | 论文链接 | 前往平台体验 |
| :--- | :--- | :--- | :--- |
| **Attention Is All You Need** <br> *Ashish Vaswani, et al.* | NeurIPS 2017| [📄 arXiv](https://arxiv.org/abs/1706.03762) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=e90aa38fdff9420e8902bc71909fa005&type=paper) |
| **Can We Get Rid of Handcrafted Feature Extractors?** <br> *Lei Su, et al.* | AAAI 2025| [📄 arXiv](https://arxiv.org/abs/2412.14598) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=97a182e56e904e92a0fe240f1f114709&type=paper) |
| **MOMENT: A Family of Open Time-series Foundation Models** <br> *Mononito Goswami, et al.* | ICML 2025| [📄 arXiv](https://arxiv.org/abs/2402.03885) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=05087484a3264a9c8b8a2c616e7cce0b&type=paper) |
| **Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data** <br> *Kashun Shum,et al.* | EMNLP 2023 | [📄 arXiv](https://arxiv.org/abs/2302.12822) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=c76b88e732cf41949b54515bdd319808&type=paper) |
| **Chronos: Learning the Language of Time Series** <br> *Abdul Fatir Ansari, et al.* | other 2024| [📄 arXiv](https://arxiv.org/pdf/2403.07815) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=6dd7daeec6584f61856876474b860e09&type=paper) |
| **Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis** <br> *Yu Yuan, et al.* | CVPR 2025| [📄 arXiv](https://arxiv.org/abs/2412.02168) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=2d412a08880f477ca5362af3ef8c14f2&type=paper) |
| **PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data** <br> *Shijie Huang, et al.* | ICCV 2025| [📄 arXiv](https://arxiv.org/abs/2502.14397) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=673e78d51fcc4bca89b636a52affa6b4&type=paper) |
| **Self-Instruct: Aligning Language Models with Self-Generated Instructions** <br> *Yizhong Wang, et al.* | ACL 2023| [📄 arXiv](https://arxiv.org/abs/2212.10560) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=2bbf2f4971f74c6e8def26879233f2fe&type=paper) |
| **RobustSAM: Segment Anything Robustly on Degraded Images** <br> *Wei-Ting Chen, et al.* | CVPR 2024| [📄 arXiv](https://arxiv.org/abs/2406.09627) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=7a23bf525a38476c952df14e72ecce23&type=paper) |
| **Side Adapter Network for Open-Vocabulary Semantic Segmentation** <br> *Mengde Xu, et al.* |CVPR 2023| [📄 arXiv](https://arxiv.org/pdf/2302.12242) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=5944c280114e41508d99e1fd85cbf78e&type=paper) |
| **Improving day-ahead Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context** <br> *Oussama Boussif, et al.* | NIPS 2023| [📄 arXiv](https://arxiv.org/abs/2306.01112) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=77ab6c82cc9444938c4bbbdd6709709a&type=paper) |
| **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting** <br> *Kashif Rasul, et al.* | other 2023| [📄 arXiv](https://arxiv.org/abs/2310.08278) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=a05e09588c4c475aac14354cae04986d&type=paper) |
| **CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos** <br> *Lei Nikita Karaev, et al.* | CVPR 2024| [📄 arXiv](https://arxiv.org/abs/2410.11831) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=269a8dc6d24a49b29d39de138b443a43&type=paper) |
| **Unified Training of Universal Time Series Forecasting Transformers** <br> *Gerald Woo, et al.* | ICML 2024| [📄 arXiv](https://arxiv.org/abs/2402.02592) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=af34d430edf14f56910c6cfc05c4ee89&type=paper) |
| **A decoder-only foundation model for time-series forecasting** <br> *Abhimanyu Das, et al.* | ICML 2024| [📄 arXiv](https://arxiv.org/abs/2310.10688) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=e78bad4ea95944af8fa0eeb98f69179f&type=paper) |
| **Timer: Generative Pre-trained Transformers Are Large Time Series Models** <br> *Yong Liu, et al.* | ICML 2024| [📄 arXiv](https://arxiv.org/abs/2402.02368) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=34d7c1e35c514d77b88762f18298e999&type=paper) |
| **LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant** <br> *Yikun Liu, et al.* | other 2024| [📄 arXiv](https://arxiv.org/abs/2412.01720) | [➡️ **立即体验**](https://www.lab4ai.cn/paper/detail?id=3c0069c96f60404b948ed30fd498fe7f&type=paper) |




---

### 🗺️ 社区复现路线图 (Community Roadmap)

我们已经筛选并整理了一份详细的待复现论文清单。这不仅是我们的工作计划，更是我们邀请您参与共建的蓝图。

<div align="center" style="margin-top: 20px;">
  <a href="https://lab4ai-hub.github.io/PaperHub/" style="display: inline-block; padding: 12px 24px; border: 2px solid #4B5563; color: #1F2937; text-decoration: none; font-weight: bold; border-radius: 6px;">
    查看待复现论文清单 (Roadmap)
  </a>
</div>

<br>

如果您对路线图中的项目感兴趣，或有新的论文推荐，请在我们的 [**Issue 列表**](https://github.com/Lab4AI-Hub/PaperHub/issues) 中发起讨论。


