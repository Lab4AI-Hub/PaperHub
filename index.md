| | | | | | | | | | |
|-|-|-|-|-|-|-|-|-|-|
| |论文详细信息| | | | | | | | |
|ID|论文名称|论文作者|论文链接|论文摘要|论文年份|来源标签 （会议期刊）|github链接|状态|案例类型|
|1|Attention is all you need|Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ﾅ「kasz Kaiser, Illia Polosukhin|https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf|The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.|2017|NIPS|https://github.com/jadore801120/attention-is-all-you-need-pytorch| |未复现|
|2|Improving language understanding by generative pre-training|Alec Radford，Karthik Narasimhan，Tim Salimans，Ilya Sutskever|https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf|Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model onadiverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).|2018|OpenAI| | |未复现|
|3|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova|https://aclanthology.org/N19-1423/?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC|We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).|2019|NAACL| | |未复现|
|4|Language models are unsupervised multitask learners|Alec Radford，Jeffrey Wu，Rewon Child，David Luan，Dario Amodei，Ilya Sutskever|https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf|Natural language processing tasks, such as ques tion answering, machine translation, reading com prehension, and summarization, are typically approached with supervised learning on task specific datasets. We demonstrate that language models begin to learn these tasks without any ex plicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the an swers generated by the language model reach 55 F1 on the CoQA dataset- matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and in creasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested lan guage modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain co herent paragraphs of text. These findings suggest a promising path towards building language pro cessing systems which learn to perform tasks from their naturally occurring demonstrations.|2019|OpenAI| | |未复现|
|5|Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer|Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu|https://www.jmlr.org/papers/v21/20-074.html|Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.|2020|JMLR|https://github.com/google-research/text-to-text-transfer-transformer| |未复现|
|6|From Local to Global: A GraphRAG Approach to Query-Focused Summarization|Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson|https://arxiv.org/abs/2404.16130|The use of retrieval-augmented generation (RAG) to retrieve relevantinformation from an external knowledge source enables large language models(LLMs) to answer questions over private and/or previously unseen documentcollections. However, RAG fails on global questions directed at an entire textcorpus, such as "What are the main themes in the dataset?", since this isinherently a query-focused summarization (QFS) task, rather than an explicitretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities oftext indexed by typical RAG systems. To combine the strengths of thesecontrasting methods, we propose GraphRAG, a graph-based approach to questionanswering over private text corpora that scales with both the generality ofuser questions and the quantity of source text. Our approach uses an LLM tobuild a graph index in two stages: first, to derive an entity knowledge graphfrom the source documents, then to pregenerate community summaries for allgroups of closely related entities. Given a question, each community summary isused to generate a partial response, before all partial responses are againsummarized in a final response to the user. For a class of global sensemakingquestions over datasets in the 1 million token range, we show that GraphRAGleads to substantial improvements over a conventional RAG baseline for both thecomprehensiveness and diversity of generated answers.|2024| |https://github.com/microsoft/graphrag| |未复现|
|7|Language Models are Few-Shot Learners|Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei|https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter|We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.|2020|NIPS|https://github.com/openai/gpt-3| |未复现|
|8|LoRA: Low-Rank Adaptation of Large Language Models|Edward J Hu,yelong shen,Phillip Wallis,Zeyuan Allen-Zhu,Yuanzhi Li,Shean Wang,Lu Wang,Weizhu Chen|https://openreview.net/forum?id=nZeVKeeFYf9|An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.|2022|ICLR|https://github.com/microsoft/LoRA| |未复现|
|9|Finetuned Language Models are Zero-Shot Learners|Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,Adams Wei Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V Le|https://openreview.net/forum?id=gEZrGCozdqR&ref=morioh.com&utm_source=morioh.com|This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.|2022|ICLR|https://github.com/google-research/FLAN| |未复现|
|10|LLaMA: Open and Efficient Foundation Language Models|Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample|https://arxiv.org/abs/2302.13971|We introduce LLaMA, a collection of foundation language models ranging from7B to 65B parameters. We train our models on trillions of tokens, and show thatit is possible to train state-of-the-art models using publicly availabledatasets exclusively, without resorting to proprietary and inaccessibledatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,and LLaMA-65B is competitive with the best models, Chinchilla-70B andPaLM-540B. We release all our models to the research community.|2023| |https://github.com/meta-llama/llama| |未复现|
|11|Self-Consistency Improves Chain of Thought Reasoning in Language Models|Xuezhi Wang,Jason Wei,Dale Schuurmans,Quoc V Le,Ed H. Chi,Sharan Narang,Aakanksha Chowdhery,Denny Zhou|https://openreview.net/forum?id=1PL1NIMMrw&utm_source=chatgpt.com|Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out all possible reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).|2023|ICLR|https://github.com/dj-sorry/self_consistency| |未复现|
|12|Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers|Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei|https://aclanthology.org/2023.findings-acl.247/|Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at https://aka.ms/icl.|2023|ACL|https://github.com/microsoft/LMOps/tree/main/understand_icl| |未复现|
|13|Toolformer: Language Models Can Teach Themselves to Use Tools|Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom|https://proceedings.neurips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html|Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.|2023|NIPS|https://github.com/lucidrains/toolformer-pytorch| |未复现|
|14|DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature|Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn|https://proceedings.mlr.press/v202/mitchell23a.html|The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM’s probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model’s log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT.|2023|PMLR|https://github.com/eric-mitchell/detect-gpt| |未复现|
|15|Recitation-augmented language models|Zhiqing Sun,Xuezhi Wang,Yi Tay,Yiming Yang,Denny Zhou|https://openreview.net/forum?id=-cqvvvb-NkI|We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on three pre-trained models (In-house LM, UL2, and OPT) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA).|2023|ICLR|https://github.com/Edward-Sun/RECITE| |未复现|
|16|Self-Instruct: Aligning Language Models with Self-Generated Instructions|Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi|https://aclanthology.org/2023.acl-long.754/|Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.|2023|ACL|https://github.com/yizhongw/self-instruct| |未复现|
|17|Automatic chain of thought prompting in large language models|Zhuosheng Zhang,Aston Zhang,Mu Li,Alex Smola|https://openreview.net/forum?id=5NTt8GFjUHkr|Large Language Models (LLMs) can carry out complex reasoning tasks by generating intermediate reasoning steps. These steps are triggered by what is called chain-of-thought (CoT) prompting, which comes in two flavors: one leverages a simple prompt like "Let’s think step by step" to facilitate step-by-step reasoning before answering a question (Zero-Shot-CoT). The other uses manual demonstrations, each composed of a question and a reasoning chain that leads to an answer (Manual-CoT). Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer. We show that such manual efforts may be eliminated by leveraging LLMs to generate the reasoning chains on its own. Since these generated chains often come with mistakes we propose a number of mitigation strategies. Our proposed Auto-CoT method automaticaly samples diverse questions and we perform post-processing quality control to generate usable reasoning chains from Zero-Shot-CoT. On ten public benchmark reasoning tasks, Auto-CoT performs on par with Manual-CoT without the need for human intervention.|2023|ICLR|https://github.com/amazon-science/auto-cot| |未复现|
|18|REALM: retrieval-augmented language model pre-training|Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang|https://dl.acm.org/doi/abs/10.5555/3524938.3525306|Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring everlarger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pretraining (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.|2020|ICML|https://github.com/google-research/language/tree/master/language/realm| |未复现|
|19|Language Is Not All You Need: Aligning Perception with Language Models|Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Nils Bjorck, Vishrav Chaudhary, Subhojit Som, XIA SONG, Furu Wei|https://proceedings.neurips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html|A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multi-modal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.|2023|NIPS|https://github.com/microsoft/unilm/tree/master/unilm-v1| |未复现|
|20|Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data|Kashun Shum, Shizhe Diao, Tong Zhang|https://aclanthology.org/2023.findings-emnlp.811/|Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, AutomateCoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machinegenerated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%).|2023|EMNLP|https://github.com/SHUMKASHUN/Automate-CoT| |未复现|
|21|Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching|Dingwen Zhang, Wenyuan Zeng, Guangyu Guo, Chaowei Fang, Lechao Cheng, Ming-Ming Cheng, Junwei Han|https://arxiv.org/abs/2112.09459|Current weakly supervised semantic segmentation (WSSS) frameworks usuallycontain the separated mask-refinement model and the main semantic region miningmodel. These approaches would contain redundant feature extraction backbonesand biased learning objectives, making them computational complex yetsub-optimal to addressing the WSSS task. To solve this problem, this paperestablishes a compact learning framework that embeds the classification andmask-refinement components into a unified deep model. With the shared featureextraction backbone, our model is able to facilitate knowledge sharing betweenthe two components while preserving a low computational complexity. Toencourage high-quality knowledge interaction, we propose a novel alternativeself-dual teaching (ASDT) mechanism. Unlike the conventional distillationstrategy, the knowledge of the two teacher branches in our model isalternatively distilled to the student branch by a Pulse Width Modulation(PWM), which generates PW wave-like selection signal to guide the knowledgedistillation process. In this way, the student branch can help prevent themodel from falling into local minimum solutions caused by the imperfectknowledge provided of either teacher branch. Comprehensive experiments on thePASCAL VOC 2012 and COCO-Stuff 10K demonstrate the effectiveness of theproposed alternative self-dual teaching mechanism as well as the newstate-of-the-art performance of our approach.|2025|TIP| | |未复现|
|22|Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning|Chun-Mei Feng, Kai Yu, Xinxing Xu, Salman Khan, Rick Siow Mong Goh, Wangmeng Zuo, Yong Liu|https://arxiv.org/abs/2506.10575|Benefited from image-text contrastive learning, pre-trained vision-languagemodels, e.g., CLIP, allow to direct leverage texts as images (TaI) forparameter-efficient fine-tuning (PEFT). While CLIP is capable of making imagefeatures to be similar to the corresponding text features, the modality gapremains a nontrivial issue and limits image recognition performance of TaI.Using multi-label image recognition (MLR) as an example, we present a novelmethod, called T2I-PAL to tackle the modality gap issue when using only textcaptions for PEFT. The core design of T2I-PAL is to leverage pre-trainedtext-to-image generation models to generate photo-realistic and diverse imagesfrom text captions, thereby reducing the modality gap. To further enhance MLR,T2I-PAL incorporates a class-wise heatmap and learnable prototypes. Thisaggregates local similarities, making the representation of local visualfeatures more robust and informative for multi-label recognition. For betterPEFT, we further combine both prompt tuning and adapter learning to enhanceclassification performance. T2I-PAL offers significant advantages: iteliminates the need for fully semantically annotated training images, therebyreducing the manual annotation workload, and it preserves the intrinsic mode ofthe CLIP model, allowing for seamless integration with any existing CLIPframework. Extensive experiments on multiple benchmarks, including MS-COCO,VOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performanceby 3.47% in average above the top-ranked state-of-the-art methods.|2024|TPAMI| | |未复现|
|23|Segment Concealed Objects with Incomplete Supervision|Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, Youwei Pang, Longxiang Tang, Chengyu Fang, Yulun Zhang, Linghe Kong, Xiu Li, Sina Farsiu|https://arxiv.org/pdf/2506.08955|Incompletely-Supervised Concealed Object Segmentation (ISCOS) involvessegmenting objects that seamlessly blend into their surrounding environments,utilizing incompletely annotated data, such as weak and semi-annotations, formodel training. This task remains highly challenging due to (1) the limitedsupervision provided by the incompletely annotated training data, and (2) thedifficulty of distinguishing concealed objects from the background, whicharises from the intrinsic similarities in concealed scenarios. In this paper,we introduce the first unified method for ISCOS to address these challenges. Totackle the issue of incomplete supervision, we propose a unified mean-teacherframework, SEE, that leverages the vision foundation model, ``\emph{SegmentAnything Model (SAM)}'', to generate pseudo-labels using coarse masks producedby the teacher model as prompts. To mitigate the effect of low-qualitysegmentation masks, we introduce a series of strategies for pseudo-labelgeneration, storage, and supervision. These strategies aim to produceinformative pseudo-labels, store the best pseudo-labels generated, and selectthe most reliable components to guide the student model, thereby ensuringrobust network training. Additionally, to tackle the issue of intrinsicsimilarity, we design a hybrid-granularity feature grouping module that groupsfeatures at different granularities and aggregates these results. By clusteringsimilar features, this module promotes segmentation coherence, facilitatingmore complete segmentation for both single-object and multiple-object images.We validate the effectiveness of our approach across multiple ISCOS tasks, andexperimental results demonstrate that our method achieves state-of-the-artperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancingthe performance of existing models.|2025|TPAMI|https://github.com/ChunmingHe/SEE| |未复现|
|24|Event-based Stereo Depth Estimation: A Survey|Suman Ghosh, Guillermo Gallego|https://arxiv.org/abs/2409.17680|Stereopsis has widespread appeal in robotics as it is the predominant way bywhich living beings perceive depth to navigate our 3D world. Event cameras arenovel bio-inspired sensors that detect per-pixel brightness changesasynchronously, with very high temporal resolution and high dynamic range,enabling machine perception in high-speed motion and broad illuminationconditions. The high temporal precision also benefits stereo matching, makingdisparity (depth) estimation a popular research area for event cameras eversince its inception. Over the last 30 years, the field has evolved rapidly,from low-latency, low-power circuit design to current deep learning (DL)approaches driven by the computer vision community. The bibliography is vastand difficult to navigate for non-experts due its highly interdisciplinarynature. Past surveys have addressed distinct aspects of this topic, in thecontext of applications, or focusing only on a specific class of techniques,but have overlooked stereo datasets. This survey provides a comprehensiveoverview, covering both instantaneous stereo and long-term methods suitable forsimultaneous localization and mapping (SLAM), along with theoretical andempirical comparisons. It is the first to extensively review DL methods as wellas stereo datasets, even providing practical suggestions for creating newbenchmarks to advance the field. The main advantages and challenges faced byevent-based stereo depth estimation are also discussed. Despite significantprogress, challenges remain in achieving optimal performance in not onlyaccuracy but also efficiency, a cornerstone of event-based computing. Weidentify several gaps and propose future research directions. We hope thissurvey inspires future research in this area, by serving as an accessible entrypoint for newcomers, as well as a practical guide for seasoned researchers inthe community.|2025|TPAMI| | |未复现|
|25|Efficient Low-Resolution Face Recognition via Bridge Distillation|Shiming Ge, Shengwei Zhao, Chenyu Li, Yu Zhang, Jia Li|https://arxiv.org/abs/2409.11786|Face recognition in the wild is now advancing towards light-weight models,fast inference speed and resolution-adapted capability. In this paper, wepropose a bridge distillation approach to turn a complex face model pretrainedon private high-resolution faces into a light-weight one for low-resolutionface recognition. In our approach, such a cross-dataset resolution-adaptedknowledge transfer problem is solved via two-step distillation. In the firststep, we conduct cross-dataset distillation to transfer the prior knowledgefrom private high-resolution faces to public high-resolution faces and generatecompact and discriminative features. In the second step, the resolution-adapteddistillation is conducted to further transfer the prior knowledge to syntheticlow-resolution faces via multi-task learning. By learning low-resolution facerepresentations and mimicking the adapted high-resolution knowledge, alight-weight student model can be constructed with high efficiency andpromising accuracy in recognizing low-resolution faces. Experimental resultsshow that the student model performs impressively in recognizing low-resolutionfaces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speedreaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobilephone, respectively.|2024|TIP| | |未复现|
|26|Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning|Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin|https://arxiv.org/abs/2403.05770|Vision-and-language navigation (VLN) asks an agent to follow a given languageinstruction to navigate through a real 3D environment. Despite significantadvances, conventional VLN agents are trained typically under disturbance-freeenvironments and may easily fail in real-world scenarios, since they areunaware of how to deal with various possible disturbances, such as suddenobstacles or human interruptions, which widely exist and may usually cause anunexpected route deviation. In this paper, we present a model-agnostic trainingparadigm, called Progressive Perturbation-aware Contrastive Learning (PROPER)to enhance the generalization ability of existing VLN agents, by requiring themto learn towards deviation-robust navigation. Specifically, a simple yeteffective path perturbation scheme is introduced to implement the routedeviation, with which the agent is required to still navigate successfullyfollowing the original instruction. Since directly enforcing the agent to learnperturbed trajectories may lead to inefficient training, a progressivelyperturbed trajectory augmentation strategy is designed, where the agent canself-adaptively learn to navigate under perturbation with the improvement ofits navigation performance for each specific trajectory. For encouraging theagent to well capture the difference brought by perturbation, aperturbation-aware contrastive learning mechanism is further developed bycontrasting perturbation-free trajectory encodings and perturbation-basedcounterparts. Extensive experiments on R2R show that PROPER can benefitmultiple VLN baselines in perturbation-free scenarios. We further collect theperturbed path data to construct an introspection subset based on the R2R,called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfyingrobustness of popular VLN agents and the capability of PROPER in improving thenavigation robustness.|2023|TPAMI|https://github.com/YicongHong/Recurrent-VLN-BERT| |未复现|
|27|Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning|Wei Tan, Lan Du, Wray Buntine|https://arxiv.org/abs/2312.10116|The effectiveness of active learning largely depends on the samplingefficiency of the acquisition function. Expected Loss Reduction (ELR) focuseson a Bayesian estimate of the reduction in classification error, and moregeneral costs fit in the same framework. We propose Bayesian Estimate of MeanProper Scores (BEMPS) to estimate the increase in strictly proper scores suchas log probability or negative mean square error within this framework. We alsoprove convergence results for this general class of costs. To facilitate betterexperimentation with the new acquisition functions, we develop a complementarybatch AL algorithm that encourages diversity in the vector of expected changesin scores for unlabeled data. To allow high-performance classifiers, we combinedeep ensembles, and dynamic validation set construction on pretrained models,and further speed up the ensemble process with the idea of Monte Carlo Dropout.Extensive experiments on both texts and images show that the use of mean squareerror and log probability with BEMPS yields robust acquisition functions andwell-calibrated classifiers, and consistently outperforms the others tested.The advantages of BEMPS over the others are further supported by a set ofqualitative analyses, where we visualise their sampling behaviour using datamaps and t-SNE plots.|2023|TPAMI| | |未复现|
|28|Paragraph-to-Image Generation with Information-Enriched Diffusion Model|Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang|https://arxiv.org/abs/2311.14284|Text-to-image (T2I) models have recently experienced rapid development,achieving astonishing performance in terms of fidelity and textual alignmentcapabilities. However, given a long paragraph (up to 512 words), thesegeneration models still struggle to achieve strong alignment and are unable togenerate images depicting complex scenes. In this paper, we introduce aninformation-enriched diffusion model for paragraph-to-image generation task,termed ParaDiffusion, which delves into the transference of the extensivesemantic comprehension capabilities of large language models to the task ofimage generation. At its core is using a large language model (e.g., Llama V2)to encode long-form text, followed by fine-tuning with LORA to alignthetext-image feature spaces in the generation task. To facilitate the training oflong-text semantic alignment, we also curated a high-quality paragraph-imagepair dataset, namely ParaImage. This dataset contains a small amount ofhigh-quality, meticulously annotated data, and a large-scale synthetic datasetwith long text descriptions being generated using a vision-language model.Experiments demonstrate that ParaDiffusion outperforms state-of-the-art models(SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45%human voting rate improvements for visual appeal and text faithfulness,respectively. The code and dataset will be released to foster communityresearch on long-text alignment.|2025|IJCV|https://github.com/weijiawu/ParaDiffusion| |未复现|
|29|Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory|Danpei Zhao, Bo Yuan, Zhenwei Shi|https://arxiv.org/abs/2309.15413|As a front-burner problem in incremental learning, class incremental semanticsegmentation (CISS) is plagued by catastrophic forgetting and semantic drift.Although recent methods have utilized knowledge distillation to transferknowledge from the old model, they are still unable to avoid pixel confusion,which results in severe misclassification after incremental steps due to thelack of annotations for past and future classes. Meanwhile data-replay-basedapproaches suffer from storage burdens and privacy concerns. In this paper, wepropose to address CISS without exemplar memory and resolve catastrophicforgetting as well as semantic drift synchronously. We present Inherit withDistillation and Evolve with Contrast (IDEC), which consists of a DenseKnowledge Distillation on all Aspects (DADA) manner and an AsymmetricRegion-wise Contrastive Learning (ARCL) module. Driven by the devised dynamicclass-specific pseudo-labelling strategy, DADA distils intermediate-layerfeatures and output-logits collaboratively with more emphasis onsemantic-invariant knowledge inheritance. ARCL implements region-wisecontrastive learning in the latent space to resolve semantic drift among knownclasses, current classes, and unknown classes. We demonstrate the effectivenessof our method on multiple CISS tasks by state-of-the-art performance, includingPascal VOC 2012, ADE20K and ISPRS datasets. Our method also shows superioranti-forgetting ability, particularly in multi-step CISS tasks.|2023|TPAMI| | |未复现|
|30|Dual Compensation Residual Networks for Class Imbalanced Learning|Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen|https://arxiv.org/abs/2308.13165|Learning generalizable representation and classifier for class-imbalanceddata is challenging for data-driven deep models. Most studies attempt tore-balance the data distribution, which is prone to overfitting on tail classesand underfitting on head classes. In this work, we propose Dual CompensationResidual Networks to better fit both tail and head classes. Firstly, we proposedual Feature Compensation Module (FCM) and Logit Compensation Module (LCM) toalleviate the overfitting issue. The design of these two modules is based onthe observation: an important factor causing overfitting is that there issevere feature drift between training and test data on tail classes. Indetails, the test features of a tail category tend to drift towards featurecloud of multiple similar head categories. So FCM estimates a multi-modefeature drift direction for each tail category and compensate for it.Furthermore, LCM translates the deterministic feature drift vector estimated byFCM along intra-class variations, so as to cover a larger effectivecompensation space, thereby better fitting the test features. Secondly, wepropose a Residual Balanced Multi-Proxies Classifier (RBMC) to alleviate theunder-fitting issue. Motivated by the observation that re-balancing strategyhinders the classifier from learning sufficient head knowledge and eventuallycauses underfitting, RBMC utilizes uniform learning with a residual path tofacilitate classifier learning. Comprehensive experiments on Long-tailed andClass-Incremental benchmarks validate the efficacy of our method.|2023|TPAMI| | |未复现|
|31|End-to-end Alternating Optimization for Real-World Blind Super Resolution|Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan|https://arxiv.org/abs/2308.08816|Blind Super-Resolution (SR) usually involves two sub-problems: 1) estimatingthe degradation of the given low-resolution (LR) image; 2) super-resolving theLR image to its high-resolution (HR) counterpart. Both problems are ill-poseddue to the information loss in the degrading process. Most previous methods tryto solve the two problems independently, but often fall into a dilemma: a goodsuper-resolved HR result requires an accurate degradation estimation, whichhowever, is difficult to be obtained without the help of original HRinformation. To address this issue, instead of considering these two problemsindependently, we adopt an alternating optimization algorithm, which canestimate the degradation and restore the SR image in a single model.Specifically, we design two convolutional neural modules, namely\textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SRimage based on the estimated degradation, and \textit{Estimator} estimates thedegradation with the help of the restored SR image. We alternate these twomodules repeatedly and unfold this process to form an end-to-end trainablenetwork. In this way, both \textit{Restorer} and \textit{Estimator} could getbenefited from the intermediate results of each other, and make eachsub-problem easier. Moreover, \textit{Restorer} and \textit{Estimator} areoptimized in an end-to-end manner, thus they could get more tolerant of theestimation deviations of each other and cooperate better to achieve more robustand accurate final results. Extensive experiments on both synthetic datasetsand real-world images show that the proposed method can largely outperformstate-of-the-art methods and produce more visually favorable results. The codesare rleased at \url{https://github.com/greatlog/RealDAN.git}.|2023|IJCV|https://github.com/greatlog/RealDAN| |未复现|
|32|YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection|Yuming Chen, Xinbin Yuan, Jiabao Wang, Ruiqi Wu, Xiang Li, Qibin Hou, Ming-Ming Cheng|https://arxiv.org/abs/2308.05480|We aim at providing the object detection community with an efficient andperformant object detector, termed YOLO-MS. The core design is based on aseries of investigations on how multi-branch features of the basic block andconvolutions with different kernel sizes affect the detection performance ofobjects at different scales. The outcome is a new strategy that cansignificantly enhance multi-scale feature representations of real-time objectdetectors. To verify the effectiveness of our work, we train our YOLO-MS on theMS COCO dataset from scratch without relying on any other large-scale datasets,like ImageNet or pre-trained weights. Without bells and whistles, our YOLO-MSoutperforms the recent state-of-the-art real-time object detectors, includingYOLO-v7, RTMDet, and YOLO-v8. Taking the XS version of YOLO-MS as an example,it can achieve an AP score of 42+% on MS COCO, which is about 2% higher thanRTMDet with the same model size. Furthermore, our work can also serve as aplug-and-play module for other YOLO models. Typically, our method significantlyadvances the APs, APl, and AP of YOLOv8-N from 18%+, 52%+, and 37%+ to 20%+,55%+, and 40%+, respectively, with even fewer parameters and MACs. Code andtrained models are publicly available athttps://github.com/FishAndWasabi/YOLO-MS. We also provide the Jittor version athttps://github.com/NK-JittorCV/nk-yolo.|2025|TPAMI|https://github.com/FishAndWasabi/YOLO-MS| |未复现|
|33|A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection|Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I. Webb, Irwin King, Shirui Pan|https://arxiv.org/abs/2307.03759|Time series are the primary data type used to record dynamic systemmeasurements and generated in great volume by both physical sensors and onlineprocesses (virtual sensors). Time series analytics is therefore crucial tounlocking the wealth of information implicit in available data. With the recentadvancements in graph neural networks (GNNs), there has been a surge inGNN-based approaches for time series analysis. These approaches can explicitlymodel inter-temporal and inter-variable relationships, which traditional andother deep neural network-based methods struggle to do. In this survey, weprovide a comprehensive review of graph neural networks for time seriesanalysis (GNN4TS), encompassing four fundamental dimensions: forecasting,classification, anomaly detection, and imputation. Our aim is to guidedesigners and practitioners to understand, build applications, and advanceresearch of GNN4TS. At first, we provide a comprehensive task-oriented taxonomyof GNN4TS. Then, we present and discuss representative research works andintroduce mainstream applications of GNN4TS. A comprehensive discussion ofpotential future research directions completes the survey. This survey, for thefirst time, brings together a vast array of knowledge on GNN-based time seriesresearch, highlighting foundations, practical applications, and opportunitiesof graph neural networks for time series analysis.|2024|TPAMI|https://github.com/KimMeen/Awesome-GNN4TS| |未复现|
|34|SplatFlow: Learning Multi-frame Optical Flow via Splatting|Bo Wang, Yifan Zhang, Jian Li, Yang Yu, Zhenping Sun, Li Liu, Dewen Hu|https://arxiv.org/abs/2306.08887|The occlusion problem remains a crucial challenge in optical flow estimation(OFE). Despite the recent significant progress brought about by deep learning,most existing deep learning OFE methods still struggle to handle occlusions; inparticular, those based on two frames cannot correctly handle occlusionsbecause occluded regions have no visual correspondences. However, there isstill hope in multi-frame settings, which can potentially mitigate theocclusion issue in OFE. Unfortunately, multi-frame OFE (MOFE) remainsunderexplored, and the limited studies on it are mainly specially designed forpyramid backbones or else obtain the aligned previous frame's features, such ascorrelation volume and optical flow, through time-consuming backward flowcalculation or non-differentiable forward warping transformation. This studyproposes an efficient MOFE framework named SplatFlow to address theseshortcomings. SplatFlow introduces the differentiable splatting transformationto align the previous frame's motion feature and designs a Final-to-Allembedding method to input the aligned motion feature into the current frame'sestimation, thus remodeling the existing two-frame backbones. The proposedSplatFlow is efficient yet more accurate, as it can handle occlusions properly.Extensive experimental evaluations show that SplatFlow substantiallyoutperforms all published methods on the KITTI2015 and Sintel benchmarks.Especially on the Sintel benchmark, SplatFlow achieves errors of 1.12 (cleanpass) and 2.07 (final pass), with surprisingly significant 19.4% and 16.2%error reductions, respectively, from the previous best results submitted. Thecode for SplatFlow is available at https://github.com/wwsource/SplatFlow.|2024|IJCV|https://github.com/wwsource/SplatFlow| |未复现|
|35|Towards Expressive Spectral-Temporal Graph Neural Networks for Time Series Forecasting|Ming Jin, Guangsi Shi, Yuan-Fang Li, Bo Xiong, Tian Zhou, Flora D. Salim, Liang Zhao, Lingfei Wu, Qingsong Wen, Shirui Pan|https://arxiv.org/abs/2305.06587|Time series forecasting has remained a focal point due to its vitalapplications in sectors such as energy management and transportation planning.Spectral-temporal graph neural network is a promising abstraction underlyingmost time series forecasting models that are based on graph neural networks(GNNs). However, more is needed to know about the underpinnings of this branchof methods. In this paper, we establish a theoretical framework that unravelsthe expressive power of spectral-temporal GNNs. Our results show that linearspectral-temporal GNNs are universal under mild assumptions, and theirexpressive power is bounded by our extended first-order Weisfeiler-Lemanalgorithm on discrete-time dynamic graphs. To make our findings useful inpractice on valid instantiations, we discuss related constraints in detail andoutline a theoretical blueprint for designing spatial and temporal modules inspectral domains. Building on these insights and to demonstrate how powerfulspectral-temporal GNNs are based on our framework, we propose a simpleinstantiation named Temporal Graph Gegenbauer Convolution (TGGC), whichsignificantly outperforms most existing models with only linear components andshows better model efficiency. Our findings pave the way for devising a broaderarray of provably expressive GNN-based models for time series.|2025|TPAMI| | |未复现|
|36|Efficient Halftoning via Deep Reinforcement Learning|Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Li Ding, Liang Chen, Kai Huang|https://arxiv.org/abs/2304.12152|Halftoning aims to reproduce a continuous-tone image with pixels whoseintensities are constrained to two discrete levels. This technique has beendeployed on every printer, and the majority of them adopt fast methods (e.g.,ordered dithering, error diffusion) that fail to render structural details,which determine halftone's quality. Other prior methods of pursuing visualpleasure by searching for the optimal halftone solution, on the contrary,suffer from their high computational cost. In this paper, we propose a fast andstructure-aware halftoning method via a data-driven approach. Specifically, weformulate halftoning as a reinforcement learning problem, in which each binarypixel's value is regarded as an action chosen by a virtual agent with a sharedfully convolutional neural network (CNN) policy. In the offline phase, aneffective gradient estimator is utilized to train the agents in producinghigh-quality halftones in one action step. Then, halftones can be generatedonline by one fast CNN inference. Besides, we propose a novel anisotropysuppressing loss function, which brings the desirable blue-noise property.Finally, we find that optimizing SSIM could result in holes in flat areas,which can be avoided by weighting the metric with the contone's contrast map.Experiments show that our framework can effectively train a light-weight CNN,which is 15x faster than previous structure-aware methods, to generateblue-noise halftones with satisfactory visual quality. We also present aprototype of deep multitoning to demonstrate the extensibility of our method.|2023|TIP| | |未复现|
|37|PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison|Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters|https://arxiv.org/abs/2211.16110|PAC-Bayes has recently re-emerged as an effective theory with which one canderive principled learning algorithms with tight performance guarantees.However, applications of PAC-Bayes to bandit problems are relatively rare,which is a great misfortune. Many decision-making problems in healthcare,finance and natural sciences can be modelled as bandit problems. In many ofthese applications, principled algorithms with strong performance guaranteeswould be very much appreciated. This survey provides an overview of PAC-Bayesbounds for bandit problems and an experimental comparison of these bounds. Onthe one hand, we found that PAC-Bayes bounds are a useful tool for designingoffline bandit algorithms with performance guarantees. In our experiments, aPAC-Bayesian offline contextual bandit algorithm was able to learn randomisedneural network polices with competitive expected reward and non-vacuousperformance guarantees. On the other hand, the PAC-Bayesian online banditalgorithms that we tested had loose cumulative regret bounds. We conclude bydiscussing some topics for future work on PAC-Bayesian bandit algorithms.|2023|TPAMI| | |未复现|
|38|Salient Object Detection via Dynamic Scale Routing|Zhenyu Wu, Shuai Li, Chenglizhao Chen, Hong Qin, Aimin Hao|https://arxiv.org/abs/2210.13821|Recent research advances in salient object detection (SOD) could largely beattributed to ever-stronger multi-scale feature representation empowered by thedeep learning technologies. The existing SOD deep models extract multi-scalefeatures via the off-the-shelf encoders and combine them smartly via variousdelicate decoders. However, the kernel sizes in this commonly-used thread areusually "fixed". In our new experiments, we have observed that kernels of smallsize are preferable in scenarios containing tiny salient objects. In contrast,large kernel sizes could perform better for images with large salient objects.Inspired by this observation, we advocate the "dynamic" scale routing (as abrand-new idea) in this paper. It will result in a generic plug-in that coulddirectly fit the existing feature backbone. This paper's key technicalinnovations are two-fold. First, instead of using the vanilla convolution withfixed kernel sizes for the encoder design, we propose the dynamic pyramidconvolution (DPConv), which dynamically selects the best-suited kernel sizesw.r.t. the given input. Second, we provide a self-adaptive bidirectionaldecoder design to accommodate the DPConv-based encoder best. The mostsignificant highlight is its capability of routing between feature scales andtheir dynamic collection, making the inference process scale-aware. As aresult, this paper continues to enhance the current SOTA performance. Both thecode and dataset are publicly available athttps://github.com/wuzhenyubuaa/DPNet.|2022|TIP|https://github.com/wuzhenyubuaa/DPNet| |未复现|
|39|Twin Contrastive Learning for Online Clustering|Yunfan Li, Mouxing Yang, Dezhong Peng, Taihao Li, Jiantao Huang, Xi Peng|https://arxiv.org/abs/2210.11680|This paper proposes to perform online clustering by conducting twincontrastive learning (TCL) at the instance and cluster level. Specifically, wefind that when the data is projected into a feature space with a dimensionalityof the target cluster number, the rows and columns of its feature matrixcorrespond to the instance and cluster representation, respectively. Based onthe observation, for a given dataset, the proposed TCL first constructspositive and negative pairs through data augmentations. Thereafter, in the rowand column space of the feature matrix, instance- and cluster-level contrastivelearning are respectively conducted by pulling together positive pairs whilepushing apart the negatives. To alleviate the influence of intrinsicfalse-negative pairs and rectify cluster assignments, we adopt aconfidence-based criterion to select pseudo-labels for boosting both theinstance- and cluster-level contrastive learning. As a result, the clusteringperformance is further improved. Besides the elegant idea of twin contrastivelearning, another advantage of TCL is that it could independently predict thecluster assignment for each instance, thus effortlessly fitting onlinescenarios. Extensive experiments on six widely-used image and text benchmarksdemonstrate the effectiveness of TCL. The code will be released on GitHub.|2022|IJCV| | |未复现|
|40|Kernel-Based Generalized Median Computation for Consensus Learning|Andreas Nienkötter, Xiaoyi Jiang|https://arxiv.org/abs/2209.10208|Computing a consensus object from a set of given objects is a core problem inmachine learning and pattern recognition. One popular approach is to formulateit as an optimization problem using the generalized median. Previous methodslike the Prototype and Distance-Preserving Embedding methods transform objectsinto a vector space, solve the generalized median problem in this space, andinversely transform back into the original space. Both of these methods havebeen successfully applied to a wide range of object domains, where thegeneralized median problem has inherent high computational complexity(typically $\mathcal{NP}$-hard) and therefore approximate solutions arerequired. Previously, explicit embedding methods were used in the computation,which often do not reflect the spatial relationship between objects exactly. Inthis work we introduce a kernel-based generalized median framework that isapplicable to both positive definite and indefinite kernels. This frameworkcomputes the relationship between objects and its generalized median in kernelspace, without the need of an explicit embedding. We show that the spatialrelationship between objects is more accurately represented in kernel spacethan in an explicit vector space using easy-to-compute kernels, and demonstratesuperior performance of generalized median computation on datasets of threedifferent domains. A software toolbox resulting from our work is made publiclyavailable to encourage other researchers to explore the generalized mediancomputation and applications.|2022|TPAMI| | |未复现|
|41|A Tale of HodgeRank and Spectral Method: Target Attack Against Rank Aggregation Is the Fixed Point of Adversarial Game|Ke Ma, Qianqian Xu, Jinshan Zeng, Guorong Li, Xiaochun Cao, Qingming Huang|https://arxiv.org/abs/2209.05742|Rank aggregation with pairwise comparisons has shown promising results inelections, sports competitions, recommendations, and information retrieval.However, little attention has been paid to the security issue of suchalgorithms, in contrast to numerous research work on the computational andstatistical characteristics. Driven by huge profits, the potential adversaryhas strong motivation and incentives to manipulate the ranking list. Meanwhile,the intrinsic vulnerability of the rank aggregation methods is not well studiedin the literature. To fully understand the possible risks, we focus on thepurposeful adversary who desires to designate the aggregated results bymodifying the pairwise data in this paper. From the perspective of thedynamical system, the attack behavior with a target ranking list is a fixedpoint belonging to the composition of the adversary and the victim. To performthe targeted attack, we formulate the interaction between the adversary and thevictim as a game-theoretic framework consisting of two continuous operatorswhile Nash equilibrium is established. Then two procedures against HodgeRankand RankCentrality are constructed to produce the modification of the originaldata. Furthermore, we prove that the victims will produce the target rankinglist once the adversary masters the complete information. It is noteworthy thatthe proposed methods allow the adversary only to hold incomplete information orimperfect feedback and perform the purposeful attack. The effectiveness of thesuggested target attack strategies is demonstrated by a series of toysimulations and several real-world data experiments. These experimental resultsshow that the proposed methods could achieve the attacker's goal in the sensethat the leading candidate of the perturbed ranking list is the designated oneby the adversary.|2022|TPAMI| | |未复现|
|42|Boosting Night-time Scene Parsing with Learnable Frequency|Zhifeng Xie, Sen Wang, Ke Xu, Zhizhong Zhang, Xin Tan, Yuan Xie, Lizhuang Ma|https://arxiv.org/abs/2208.14241|Night-Time Scene Parsing (NTSP) is essential to many vision applications,especially for autonomous driving. Most of the existing methods are proposedfor day-time scene parsing. They rely on modeling pixel intensity-based spatialcontextual cues under even illumination. Hence, these methods do not performwell in night-time scenes as such spatial contextual cues are buried in theover-/under-exposed regions in night-time scenes. In this paper, we firstconduct an image frequency-based statistical experiment to interpret theday-time and night-time scene discrepancies. We find that image frequencydistributions differ significantly between day-time and night-time scenes, andunderstanding such frequency distributions is critical to NTSP problem. Basedon this, we propose to exploit the image frequency distributions for night-timescene parsing. First, we propose a Learnable Frequency Encoder (LFE) to modelthe relationship between different frequency coefficients to measure allfrequency components dynamically. Second, we propose a Spatial Frequency Fusionmodule (SFF) that fuses both spatial and frequency information to guide theextraction of spatial context features. Extensive experiments show that ourmethod performs favorably against the state-of-the-art methods on theNightCity, NightCity+ and BDD100K-night datasets. In addition, we demonstratethat our method can be applied to existing day-time scene parsing methods andboost their performance on night-time scenes.|2023|TIP| | |未复现|
|43|SiamMask: A Framework for Fast Online Object Tracking and Segmentation|Weiming Hu, Qiang Wang, Li Zhang, Luca Bertinetto, Philip H. S. Torr|https://arxiv.org/abs/2207.02088|In this paper we introduce SiamMask, a framework to perform both visualobject tracking and video object segmentation, in real-time, with the samesimple method. We improve the offline training procedure of popularfully-convolutional Siamese approaches by augmenting their losses with a binarysegmentation task. Once the offline training is completed, SiamMask onlyrequires a single bounding box for initialization and can simultaneously carryout visual object tracking and segmentation at high frame-rates. Moreover, weshow that it is possible to extend the framework to handle multiple objecttracking and segmentation by simply re-using the multi-task model in a cascadedfashion. Experimental results show that our approach has high processingefficiency, at around 55 frames per second. It yields real-timestate-of-the-art results on visual-object tracking benchmarks, while at thesame time demonstrating competitive performance at a high speed for videoobject segmentation benchmarks.|2022|TPAMI| | |未复现|
|44|SERE: Exploring Feature Self-relation for Self-supervised Transformer|Zhong-Yu Li, Shanghua Gao, Ming-Ming Cheng|https://arxiv.org/abs/2206.05184|Learning representations with self-supervision for convolutional networks(CNN) has been validated to be effective for vision tasks. As an alternative toCNN, vision transformers (ViT) have strong representation ability with spatialself-attention and channel-level feedforward networks. Recent works reveal thatself-supervised learning helps unleash the great potential of ViT. Still, mostworks follow self-supervised strategies designed for CNN, e.g., instance-leveldiscrimination of samples, but they ignore the properties of ViT. We observethat relational modeling on spatial and channel dimensions distinguishes ViTfrom other networks. To enforce this property, we explore the featureSElf-RElation (SERE) for training self-supervised ViT. Specifically, instead ofconducting self-supervised learning solely on feature embeddings from multipleviews, we utilize the feature self-relations, i.e., spatial/channelself-relations, for self-supervised learning. Self-relation based learningfurther enhances the relation modeling ability of ViT, resulting in strongerrepresentations that stably improve performance on multiple downstream tasks.Our source code is publicly available at: https://github.com/MCG-NKU/SERE.|2023|TPAMI|https://github.com/MCG-NKU/SERE| |未复现|
|45|Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis|Maciej Besta, Torsten Hoefler|https://arxiv.org/abs/2205.09702|Graph neural networks (GNNs) are among the most powerful tools in deeplearning. They routinely solve complex problems on unstructured networks, suchas node classification, graph classification, or link prediction, with highaccuracy. However, both inference and training of GNNs are complex, and theyuniquely combine the features of irregular graph processing with dense andregular computations. This complexity makes it very challenging to execute GNNsefficiently on modern massively parallel architectures. To alleviate this, wefirst design a taxonomy of parallelism in GNNs, considering data and modelparallelism, and different forms of pipelining. Then, we use this taxonomy toinvestigate the amount of parallelism in numerous GNN models, GNN-drivenmachine learning tasks, software frameworks, or hardware accelerators. We usethe work-depth model, and we also assess communication volume andsynchronization. We specifically focus on the sparsity/density of theassociated tensors, in order to understand how to effectively apply techniquessuch as vectorization. We also formally analyze GNN pipelining, and wegeneralize the established Message-Passing class of GNN models to coverarbitrary pipeline depths, facilitating future optimizations. Finally, weinvestigate different forms of asynchronicity, navigating the path for futureasynchronous parallel GNN pipelines. The outcomes of our analysis aresynthesized in a set of insights that help to maximize GNN performance, and acomprehensive list of challenges and opportunities for further research intoefficient GNN computations. Our work will help to advance the design of futureGNNs.|2023|TPAMI| | |未复现|
|46|Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network|Dasong Li, Yi Zhang, Ka Lung Law, Xiaogang Wang, Hongwei Qin, Hongsheng Li|https://arxiv.org/abs/2205.04721|With the growing popularity of smartphones, capturing high-quality images isof vital importance to smartphones. The cameras of smartphones have smallapertures and small sensor cells, which lead to the noisy images in low lightenvironment. Denoising based on a burst of multiple frames generallyoutperforms single frame denoising but with the larger compututional cost. Inthis paper, we propose an efficient yet effective burst denoising system. Weadopt a three-stage design: noise prior integration, multi-frame alignment andmulti-frame denoising. First, we integrate noise prior by pre-processing rawsignals into a variance-stabilization space, which allows using a small-scalenetwork to achieve competitive performance. Second, we observe that it isessential to adopt an explicit alignment for burst denoising, but it is notnecessary to integrate a learning-based method to perform multi-framealignment. Instead, we resort to a conventional and efficient alignment methodand combine it with our multi-frame denoising network. At last, we propose adenoising strategy that processes multiple frames sequentially. Sequentialdenoising avoids filtering a large number of frames by decomposing multipleframes denoising into several efficient sub-network denoising. As for eachsub-network, we propose an efficient multi-frequency denoising network toremove noise of different frequencies. Our three-stage design is efficient andshows strong performance on burst denoising. Experiments on synthetic and realraw datasets demonstrate that our method outperforms state-of-the-art methods,with less computational cost. Furthermore, the low complexity and high-qualityperformance make deployment on smartphones possible.|2022|IJCV| | |未复现|
|47|Incomplete Gamma Kernels: Generalizing Locally Optimal Projection Operators|Patrick Stotko, Michael Weinmann, Reinhard Klein|https://arxiv.org/abs/2205.01087|We present incomplete gamma kernels, a generalization of Locally OptimalProjection (LOP) operators. In particular, we reveal the relation of theclassical localized $ L_1 $ estimator, used in the LOP operator for point clouddenoising, to the common Mean Shift framework via a novel kernel. Furthermore,we generalize this result to a whole family of kernels that are built upon theincomplete gamma function and each represents a localized $ L_p $ estimator. Byderiving various properties of the kernel family concerning distributional,Mean Shift induced, and other aspects such as strict positive definiteness, weobtain a deeper understanding of the operator's projection behavior. From thesetheoretical insights, we illustrate several applications ranging from animproved Weighted LOP (WLOP) density weighting scheme and a more accurateContinuous LOP (CLOP) kernel approximation to the definition of a novel set ofrobust loss functions. These incomplete gamma losses include the Gaussian andLOP loss as special cases and can be applied to various tasks including normalfiltering. Furthermore, we show that the novel kernels can be included aspriors into neural networks. We demonstrate the effects of each application ina range of quantitative and qualitative experiments that highlight the benefitsinduced by our modifications.|2024|TPAMI|https://github.com/stotko/incomplete-gamma-kernels| |未复现|
|48|From Slow Bidirectional to Fast Autoregressive Video Diffusion Models|Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang|https://arxiv.org/abs/2412.07772|Current video diffusion models achieve impressive generation quality butstruggle in interactive applications due to bidirectional attentiondependencies. The generation of a single frame requires the model to processthe entire sequence, including the future. We address this limitation byadapting a pretrained bidirectional diffusion transformer to an autoregressivetransformer that generates frames on-the-fly. To further reduce latency, weextend distribution matching distillation (DMD) to videos, distilling 50-stepdiffusion model into a 4-step generator. To enable stable and high-qualitydistillation, we introduce a student initialization scheme based on teacher'sODE trajectories, as well as an asymmetric distillation strategy thatsupervises a causal student model with a bidirectional teacher. This approacheffectively mitigates error accumulation in autoregressive generation, allowinglong-duration video synthesis despite training on short clips. Our modelachieves a total score of 84.27 on the VBench-Long benchmark, surpassing allprevious video generation models. It enables fast streaming generation ofhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Ourapproach also enables streaming video-to-video translation, image-to-video, anddynamic prompting in a zero-shot manner. We will release the code based on anopen-source model in the future.|2025|CVPR|https://github.com/tianweiy/CausVid| |未复现|
|49|FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention|Guangxuan Xiao, Tianwei Yin, William T. Freeman, Frédo Durand, Song Han|https://arxiv.org/abs/2305.10431|Diffusion models excel at text-to-image generation, especially insubject-driven generation for personalized images. However, existing methodsare inefficient due to the subject-specific fine-tuning, which iscomputationally intensive and hampers efficient deployment. Moreover, existingmethods struggle with multi-subject generation as they often blend featuresamong subjects. We present FastComposer which enables efficient, personalized,multi-subject text-to-image generation without fine-tuning. FastComposer usessubject embeddings extracted by an image encoder to augment the generic textconditioning in diffusion models, enabling personalized image generation basedon subject images and textual instructions with only forward passes. To addressthe identity blending problem in the multi-subject generation, FastComposerproposes cross-attention localization supervision during training, enforcingthe attention of reference subjects localized to the correct regions in thetarget images. Naively conditioning on subject embeddings results in subjectoverfitting. FastComposer proposes delayed subject conditioning in thedenoising step to maintain both identity and editability in subject-drivenimage generation. FastComposer generates images of multiple unseen individualswith different styles, actions, and contexts. It achieves300$\times$-2500$\times$ speedup compared to fine-tuning-based methods andrequires zero extra storage for new subjects. FastComposer paves the way forefficient, personalized, and high-quality multi-subject image creation. Code,model, and dataset are available athttps://github.com/mit-han-lab/fastcomposer.|2024|IJCV|https://github.com/mit-han-lab/fastcomposer| |未复现|
|50|ROGRAG: A Robustly Optimized GraphRAG Framework|Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong|https://arxiv.org/abs/2503.06474|Large language models (LLMs) commonly struggle with specialized or emergingtopics which are rarely seen in the training corpus. Graph-basedretrieval-augmented generation (GraphRAG) addresses this by structuring domainknowledge as a graph for dynamic retrieval. However, existing pipelines involvecomplex engineering workflows, making it difficult to isolate the impact ofindividual components. It is also challenging to evaluate the retrievaleffectiveness due to the overlap between the pretraining and evaluationdatasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAGframework. Specifically, we propose a multi-stage retrieval mechanism thatintegrates dual-level with logic form retrieval methods to improve retrievalrobustness without increasing computational cost. To further refine the system,we incorporate various result verification methods and adopt an incrementaldatabase construction approach. Through extensive ablation experiments, werigorously assess the effectiveness of each component. Our implementationincludes comparative experiments on SeedBench, where Qwen2.5-7B-Instructinitially underperformed. ROGRAG significantly improves the score from 60.0% to75.0% and outperforms mainstream methods. Experiments on domain-specificdatasets reveal that dual-level retrieval enhances fuzzy matching, while logicform retrieval improves structured reasoning, highlighting the importance ofmulti-stage retrieval.ROGRAG is released as an open-source resource andsupports installation with pip.|2025|ACL|https://github.com/tpoisonooo/ROGRAG| |未复现|
|51|Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images|Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Aysegul Dundar|https://arxiv.org/abs/2409.20530|3D GAN inversion aims to project a single image into the latent space of a 3DGenerative Adversarial Network (GAN), thereby achieving 3D geometryreconstruction. While there exist encoders that achieve good results in 3D GANinversion, they are predominantly built on EG3D, which specializes insynthesizing near-frontal views and is limiting in synthesizing comprehensive3D scenes from diverse viewpoints. In contrast to existing approaches, wepropose a novel framework built on PanoHead, which excels in synthesizingimages from a 360-degree perspective. To achieve realistic 3D modeling of theinput image, we introduce a dual encoder system tailored for high-fidelityreconstruction and realistic generation from different viewpoints. Accompanyingthis, we propose a stitching framework on the triplane domain to get the bestpredictions from both. To achieve seamless stitching, both encoders must outputconsistent results despite being specialized for different tasks. For thisreason, we carefully train these encoders using specialized losses, includingan adversarial loss based on our novel occlusion-aware triplane discriminator.Experiments reveal that our approach surpasses the existing encoder trainingmethods qualitatively and quantitatively. Please visit the project page:https://berkegokmen1.github.io/dual-enc-3d-gan-inv.|2024|NIPS|berkegokmen1/dual-enc-3d-gan-inversion| |未复现|
|52|Can We Leave Deepfake Data Behind in Training Deepfake Detector|Jikang Cheng, Zhiyuan Yan, Ying Zhang, Yuhao Luo, Zhongyuan Wang, Chen Li|https://arxiv.org/pdf/2408.17052|The generalization ability of deepfake detectors is vital for theirapplications in real-world scenarios. One effective solution to enhance thisability is to train the models with manually-blended data, which we termed"blendfake", encouraging models to learn generic forgery artifacts likeblending boundary. Interestingly, current SoTA methods utilize blendfakewithout incorporating any deepfake data in their training process. This islikely because previous empirical observations suggest that vanilla hybridtraining (VHT), which combines deepfake and blendfake data, results in inferiorperformance to methods using only blendfake data (so-called "1+1<2").Therefore, a critical question arises: Can we leave deepfake behind and relysolely on blendfake data to train an effective deepfake detector? Intuitively,as deepfakes also contain additional informative forgery clues (e.g., deepgenerative artifacts), excluding all deepfake data in training deepfakedetectors seems counter-intuitive. In this paper, we rethink the role ofblendfake in detecting deepfakes and formulate the process from "real toblendfake to deepfake" to be a progressive transition. Specifically, blendfakeand deepfake can be explicitly delineated as the oriented pivot anchors between"real-to-fake" transitions. The accumulation of forgery information should beoriented and progressively increasing during this transition process. To thisend, we propose an Oriented Progressive Regularizor (OPR) to establish theconstraints that compel the distribution of anchors to be discretely arranged.Furthermore, we introduce feature bridging to facilitate the smooth transitionbetween adjacent anchors. Extensive experiments confirm that our design allowsleveraging forgery information from both blendfake and deepfake effectively andcomprehensively.|2024|NIPS|https://github.com/beautyremain/ProDet| |未复现|
|53|VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset|Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu|https://arxiv.org/abs/2305.18500|Vision and text have been fully explored in contemporary video-textfoundational models, while other modalities such as audio and subtitles invideos have not received sufficient attention. In this paper, we resort toestablish connections between multi-modality video tracks, including Vision,Audio, and Subtitle, and Text by exploring an automatically generatedlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,we first collect 27 million open-domain video clips and separately train avision and an audio captioner to generate vision and audio captions. Then, weemploy an off-the-shelf Large Language Model (LLM) to integrate the generatedcaptions, together with subtitles and instructional prompts into omni-modalitycaptions. Based on the proposed VAST-27M dataset, we train an omni-modalityvideo-text foundational model named VAST, which can perceive and processvision, audio, and subtitle modalities from video, and better support varioustasks including vision-text, audio-text, and multi-modal video-text tasks(retrieval, captioning and QA). Extensive experiments have been conducted todemonstrate the effectiveness of our proposed VAST-27M corpus and VASTfoundation model. VAST achieves 22 new state-of-the-art results on variouscross-modality benchmarks. Code, model and dataset will be released athttps://github.com/TXH-mercury/VAST.|2023|NIPS|https://github.com/TXH-mercury/VAST| |未复现|
|54|MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors|Riku Murai, Eric Dexheimer, Andrew J. Davison|https://arxiv.org/abs/2412.12392|We present a real-time monocular dense SLAM system designed bottom-up fromMASt3R, a two-view 3D reconstruction and matching prior. Equipped with thisstrong prior, our system is robust on in-the-wild video sequences despitemaking no assumption on a fixed or parametric camera model beyond a uniquecamera centre. We introduce efficient methods for pointmap matching, cameratracking and local fusion, graph construction and loop closure, andsecond-order global optimisation. With known calibration, a simple modificationto the system achieves state-of-the-art performance across various benchmarks.Altogether, we propose a plug-and-play monocular SLAM system capable ofproducing globally-consistent poses and dense geometry while operating at 15FPS.|2025|CVPR|https://github.com/rmurai0610/MASt3R-SLAM| |未复现|
|55|MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM|Vladimir Yugay, Theo Gevers, Martin R. Oswald|https://arxiv.org/abs/2411.16785|Simultaneous localization and mapping (SLAM) systems with novel viewsynthesis capabilities are widely used in computer vision, with applications inaugmented reality, robotics, and autonomous driving. However, existingapproaches are limited to single-agent operation. Recent work has addressedthis problem using a distributed neural scene representation. Unfortunately,existing methods are slow, cannot accurately render real-world data, arerestricted to two agents, and have limited tracking accuracy. In contrast, wepropose a rigidly deformable 3D Gaussian-based scene representation thatdramatically speeds up the system. However, improving tracking accuracy andreconstructing a globally consistent map from multiple agents remainschallenging due to trajectory drift and discrepancies across agents'observations. Therefore, we propose new tracking and map-merging mechanisms andintegrate loop closure in the Gaussian-based SLAM pipeline. We evaluateMAGiC-SLAM on synthetic and real-world datasets and find it more accurate andfaster than the state of the art.|2025|CVPR|https://github.com/VladimirYugay/MAGiC-SLAM| |未复现|
|56|Murre: Multi-view Reconstruction via SfM-guided Monocular Depth Estimation|Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao|https://arxiv.org/abs/2503.14483|In this paper, we present a new method for multi-view geometricreconstruction. In recent years, large vision models have rapidly developed,performing excellently across various tasks and demonstrating remarkablegeneralization capabilities. Some works use large vision models for monoculardepth estimation, which have been applied to facilitate multi-viewreconstruction tasks in an indirect manner. Due to the ambiguity of themonocular depth estimation task, the estimated depth values are usually notaccurate enough, limiting their utility in aiding multi-view reconstruction. Wepropose to incorporate SfM information, a strong multi-view prior, into thedepth estimation process, thus enhancing the quality of depth prediction andenabling their direct application in multi-view geometric reconstruction.Experimental results on public real-world datasets show that our methodsignificantly improves the quality of depth estimation compared to previousmonocular depth estimation works. Additionally, we evaluate the reconstructionquality of our approach in various types of scenes including indoor,streetscape, and aerial views, surpassing state-of-the-art MVS methods. Thecode and supplementary materials are available athttps://zju3dv.github.io/murre/ .|2025|CVPR|https://github.com/zju3dv/Murre| |未复现|
|57|STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution|Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, Ying Tai|https://arxiv.org/abs/2501.02976|Image diffusion models have been adapted for real-world videosuper-resolution to tackle over-smoothing issues in GAN-based methods. However,these models struggle to maintain temporal consistency, as they are trained onstatic images, limiting their ability to capture temporal dynamics effectively.Integrating text-to-video (T2V) models into video super-resolution for improvedtemporal modeling is straightforward. However, two key challenges remain:artifacts introduced by complex degradations in real-world scenarios, andcompromised fidelity due to the strong generative capacity of powerful T2Vmodels (\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality ofrestored videos, we introduce\textbf{~\name}(\textbf{S}patial-\textbf{T}emporal \textbf{A}ugmentation with T2V models for\textbf{R}eal-world video super-resolution), a novel approach that leveragesT2V models for real-world video super-resolution, achieving realistic spatialdetails and robust temporal consistency. Specifically, we introduce a LocalInformation Enhancement Module (LIEM) before the global attention block toenrich local details and mitigate degradation artifacts. Moreover, we propose aDynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focuson different frequency components across diffusion steps. Extensive experimentsdemonstrate\textbf{~\name}~outperforms state-of-the-art methods on bothsynthetic and real-world datasets.|2025|ICCV|https://github.com/NJU-PCALab/STAR| |未复现|
|58|SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models|Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee|https://arxiv.org/abs/2403.09055|We introduce SemanticDraw, a new paradigm of interactive content creationwhere high-quality images are generated in near real-time from given multiplehand-drawn regions, each encoding prescribed semantic meaning. In order tomaximize the productivity of content creators and to fully realize theirartistic imagination, it requires both quick interactive interfaces andfine-grained regional controls in their tools. Despite astonishing generationquality from recent diffusion models, we find that existing approaches forregional controllability are very slow (52 seconds for $512 \times 512$ image)while not compatible with acceleration methods such as LCM, blocking their hugepotential in interactive content creation. From this observation, we build oursolution for interactive content creation in two steps: (1) we establishcompatibility between region-based controls and acceleration techniques fordiffusion models, maintaining high fidelity of multi-prompt image generationwith $\times 10$ reduced number of inference steps, (2) we increase thegeneration throughput with our new multi-prompt stream batch pipeline, enablinglow-latency generation from multiple, region-based text prompts on a single RTX2080 Ti GPU. Our proposed framework is generalizable to any existing diffusionmodels and acceleration schedulers, allowing sub-second (0.64 seconds) imagecontent creation application upon well-established image diffusion models. Ourproject page is: https://jaerinlee.com/research/semantic-draw|2025|CVPR|https://github.com/ironjr/semantic-draw| |未复现|
|59|HSMR: Reconstructing Humans with a Biomechanically Accurate Skeleton|Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos|https://arxiv.org/abs/2503.21751|In this paper, we introduce a method for reconstructing 3D humans from asingle image using a biomechanically accurate skeleton model. To achieve this,we train a transformer that takes an image as input and estimates theparameters of the model. Due to the lack of training data for this task, webuild a pipeline to produce pseudo ground truth model parameters for singleimages and implement a training procedure that iteratively refines these pseudolabels. Compared to state-of-the-art methods for 3D human mesh recovery, ourmodel achieves competitive performance on standard benchmarks, while itsignificantly outperforms them in settings with extreme 3D poses andviewpoints. Additionally, we show that previous reconstruction methodsfrequently violate joint angle limits, leading to unnatural rotations. Incontrast, our approach leverages the biomechanically plausible degrees offreedom making more realistic joint rotation estimates. We validate ourapproach across multiple human pose estimation benchmarks. We make the code,models and data available at: https://isshikihugh.github.io/HSMR/|2025|CVPR|https://github.com/IsshikiHugh/HSMR| |未复现|
|60|RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness|Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun|https://arxiv.org/abs/2405.17220|Traditional feedback learning for hallucination reduction relies onlabor-intensive manual labeling or expensive proprietary models. This leavesthe community without foundational knowledge about how to build high-qualityfeedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novelframework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximallyexplores open-source MLLMs from two perspectives, including high-qualityfeedback data generation for preference learning and self-feedback guidance forinference-time scaling. Extensive experiments on six benchmarks in bothautomatic and human evaluation show that RLAIF-V substantially enhances thetrustworthiness of models at both preference learning and inference time.RLAIF-V 7B reduces object hallucination by 80.7\% and overall hallucination by33.7\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential ofopen-source MLLMs, where the model can learn from feedback of itself to achievesuper GPT-4V trustworthiness.|2025|CVPR|https://github.com/RLHF-V/RLAIF-V| |未复现|
|61|DFormer：Rethinking RGBD Representation Learning for Semantic Segmentation|Bowen Yin, Xuying Zhang, Zhongyu Li, Li Liu, Ming-Ming Cheng, Qibin Hou|https://arxiv.org/abs/2309.09668|We present DFormer, a novel RGB-D pretraining framework to learn transferablerepresentations for RGB-D segmentation tasks. DFormer has two new keyinnovations: 1) Unlike previous works that encode RGB-D information with RGBpretrained backbone, we pretrain the backbone using image-depth pairs fromImageNet-1K, and hence the DFormer is endowed with the capacity to encode RGB-Drepresentations; 2) DFormer comprises a sequence of RGB-D blocks, which aretailored for encoding both RGB and depth information through a novel buildingblock design. DFormer avoids the mismatched encoding of the 3D geometryrelationships in depth maps by RGB pretrained backbones, which widely lies inexisting methods but has not been resolved. We finetune the pretrained DFormeron two popular RGB-D tasks, i.e., RGB-D semantic segmentation and RGB-D salientobject detection, with a lightweight decoder head. Experimental results showthat our DFormer achieves new state-of-the-art performance on these two taskswith less than half of the computational cost of the current best methods ontwo RGB-D semantic segmentation datasets and five RGB-D salient objectdetection datasets. Our code is available at:https://github.com/VCIP-RGBD/DFormer.|2025|CVPR|https://github.com/VCIP-RGBD/DFormer| |未复现|
|62|GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation|Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu|https://arxiv.org/abs/2406.06526|3D city generation with NeRF-based methods shows promising generation resultsbut is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) hasemerged as a highly efficient alternative for object-level 3D generation.However, adapting 3D-GS from finite-scale 3D objects and humans toinfinite-scale 3D cities is non-trivial. Unbounded 3D city generation entailssignificant storage overhead (out-of-memory issues), arising from the need toexpand points to billions, often demanding hundreds of Gigabytes of VRAM for acity scene spanning 10km^2. In this paper, we propose GaussianCity, agenerative Gaussian Splatting framework dedicated to efficiently synthesizingunbounded 3D cities with a single feed-forward pass. Our key insights aretwo-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as ahighly compact intermediate representation, ensuring that the growth in VRAMusage for unbounded scenes remains constant, thus enabling unbounded citygeneration. 2) Spatial-aware Gaussian Attribute Decoder: We presentspatial-aware BEV-Point decoder to produce 3D Gaussian attributes, whichleverages Point Serializer to integrate the structural and contextualcharacteristics of BEV points. Extensive experiments demonstrate thatGaussianCity achieves state-of-the-art results in both drone-view andstreet-view 3D city generation. Notably, compared to CityDreamer, GaussianCityexhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18FPS).|2025|CVPR|https://github.com/hzxie/GaussianCity| |未复现|
|63|PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos|Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li|https://arxiv.org/abs/2503.17973|Creating a physical digital twin of a real-world object has immense potentialin robotics, content creation, and XR. In this paper, we present PhysTwin, anovel framework that uses sparse videos of dynamic objects under interaction toproduce a photo- and physically realistic, real-time interactive virtualreplica. Our approach centers on two key components: (1) a physics-informedrepresentation that combines spring-mass models for realistic physicalsimulation, generative shape models for geometry, and Gaussian splats forrendering; and (2) a novel multi-stage, optimization-based inverse modelingframework that reconstructs complete geometry, infers dense physicalproperties, and replicates realistic appearance from videos. Our methodintegrates an inverse physics framework with visual perception cues, enablinghigh-fidelity reconstruction even from partial, occluded, and limitedviewpoints. PhysTwin supports modeling various deformable objects, includingropes, stuffed animals, cloth, and delivery packages. Experiments show thatPhysTwin outperforms competing methods in reconstruction, rendering, futureprediction, and simulation under novel interactions. We further demonstrate itsapplications in interactive real-time simulation and model-based robotic motionplanning.|2025|ICCV|https://github.com/Jianghanxiao/PhysTwin| |未复现|
|64|UniGoal: Towards Universal Zero-shot Goal-oriented Navigation|Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu|https://arxiv.org/abs/2503.10630|In this paper, we propose a general framework for universal zero-shotgoal-oriented navigation. Existing zero-shot methods build inference frameworkupon large language models (LLM) for specific tasks, which differs a lot inoverall pipeline and fails to generalize across different types of goal.Towards the aim of universal zero-shot navigation, we propose a uniform graphrepresentation to unify different goals, including object category, instanceimage and text description. We also convert the observation of agent into anonline maintained scene graph. With this consistent scene and goalrepresentation, we preserve most structural information compared with pure textand are able to leverage LLM for explicit graph-based reasoning. Specifically,we conduct graph matching between the scene graph and goal graph at each timeinstant and propose different strategies to generate long-term goal ofexploration according to different matching states. The agent first iterativelysearches subgraph of goal when zero-matched. With partial matching, the agentthen utilizes coordinate projection and anchor pair alignment to infer the goallocation. Finally scene graph correction and goal verification are applied forperfect matching. We also present a blacklist mechanism to enable robust switchbetween stages. Extensive experiments on several benchmarks show that ourUniGoal achieves state-of-the-art zero-shot performance on three studiednavigation tasks with a single model, even outperforming task-specificzero-shot methods and supervised universal methods.|2025|CVPR|https://github.com/bagh2178/UniGoal| |未复现|
|65|Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction|Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang|https://arxiv.org/abs/2412.04887|3D Gaussian Splatting has demonstrated notable success in large-scale scenereconstruction, but challenges persist due to high training memory consumptionand storage overhead. Hybrid representations that integrate implicit andexplicit features offer a way to mitigate these limitations. However, whenapplied in parallelized block-wise training, two critical issues arise sincereconstruction accuracy deteriorates due to reduced data diversity whentraining each block independently, and parallel training restricts the numberof divided blocks to the available number of GPUs. To address these issues, wepropose Momentum-GS, a novel approach that leverages momentum-basedself-distillation to promote consistency and accuracy across the blocks whiledecoupling the number of blocks from the physical GPU count. Our methodmaintains a teacher Gaussian decoder updated with momentum, ensuring a stablereference during training. This teacher provides each block with globalguidance in a self-distillation manner, promoting spatial consistency inreconstruction. To further ensure consistency across the blocks, we incorporateblock weighting, dynamically adjusting each block's weight according to itsreconstruction accuracy. Extensive experiments on large-scale scenes show thatour method consistently outperforms existing techniques, achieving a 12.8%improvement in LPIPS over CityGaussian with much fewer divided blocks andestablishing a new state of the art. Project page:https://jixuan-fan.github.io/Momentum-GS_Page/|2025|ICCV|https://github.com/Jixuan-Fan/Momentum-GS| |未复现|
|66|MINIMA: Modality Invariant Image Matching|Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai|https://arxiv.org/abs/2412.19412|Image matching for both cross-view and cross-modality plays a critical rolein multimodal perception. In practice, the modality gap caused by differentimaging systems/styles poses great challenges to the matching task. Existingworks try to extract invariant features for specific modalities and train onlimited datasets, showing poor generalization. In this paper, we presentMINIMA, a unified image matching framework for multiple cross-modal cases.Without pursuing fancy modules, our MINIMA aims to enhance universalperformance from the perspective of data scaling up. For such purpose, wepropose a simple yet effective data engine that can freely produce a largedataset containing multiple modalities, rich scenarios, and accurate matchinglabels. Specifically, we scale up the modalities from cheap but rich RGB-onlymatching data, by means of generative models. Under this setting, the matchinglabels and rich diversity of the RGB dataset are well inherited by thegenerated multimodal data. Benefiting from this, we construct MD-syn, a newcomprehensive dataset that fills the data gap for general multimodal imagematching. With MD-syn, we can directly train any advanced matching pipeline onrandomly selected modality pairs to obtain cross-modal ability. Extensiveexperiments on in-domain and zero-shot matching tasks, including $19$cross-modal cases, demonstrate that our MINIMA can significantly outperform thebaselines and even surpass modality-specific methods. The dataset and code areavailable at https://github.com/LSXI7/MINIMA.|2025|CVPR|https://github.com/LSXI7/MINIMA| |未复现|
|67|Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video|David Yifan Yao, Albert J. Zhai, Shenlong Wang|https://arxiv.org/abs/2503.21761|This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding.|2025|CVPR|https://github.com/Davidyao99/uni4d| |未复现|
|68|Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models|Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang|https://arxiv.org/abs/2501.18590|Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce DiffusionRenderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Experiments demonstrate that DiffusionRenderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input--including relighting, material editing, and realistic object insertion.|2025|CVPR|https://github.com/52CV/CVPR-2025-Papers| |未复现|
|69|Linear Programming Bounds on k-Uniform States|Yu Ning, Fei Shi, Tao Luo, Xiande Zhang|https://arxiv.org/abs/2503.02222|The existence of $k$-uniform states has been a widely studied problem due totheir applications in several quantum information tasks and their closerelation to combinatorial objects like Latin squares and orthogonal arrays.With the machinery of quantum enumerators and linear programming, we establishseveral improved non-existence results and bounds on $k$-uniform states.  1. First, for any fixed $l\geq 1$ and $q\geq 2$, we show that there exists aconstant $c$ such that $(\left\lfloor{n/2}\right\rfloor-l)$-uniform states in$(\mathbb{C}^q)^{\otimes n}$ do not exist when $n\geq cq^2+o(q^2)$. Theconstant $c$ equals $4$ when $l=1$ and $6$ when $l=2$, which generalizesScott's bound (2004) for $l=0$.  2. Second, when $n$ is sufficiently large, we show that there exists aconstant $\theta<1/2$ for each $q \le 9$, such that $k$-uniform states in$(\mathbb{C}^q)^{\otimes n}$ exist only when $k\leq \theta n$. In particular,this provides the first bound (to the best of our knowledge) of $k$ for $4\leqq\leq 9$ and confirms a conjecture posed by Shi et al. (2023) when $q=5$ in astronger form.  3. Finally, we improve the shadow bounds given by Shi et al. (2023) by aconstant for $q = 3,4,5$ and small $n$. When $q=4$, our results can update somebounds listed in the code tables maintained by Grassl (2007--2024).|2025|ICCV|https://github.com/Epona-World-Model/Epona| |未复现|
|70|LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation|Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, Ziwei Liu|https://arxiv.org/abs/2402.05054|3D content creation has achieved significant progress in terms of bothquality and speed. Although current feed-forward models can produce 3D objectsin seconds, their resolution is constrained by the intensive computationrequired during training. In this paper, we introduce Large Multi-View GaussianModel (LGM), a novel framework designed to generate high-resolution 3D modelsfrom text prompts or single-view images. Our key insights are two-fold: 1) 3DRepresentation: We propose multi-view Gaussian features as an efficient yetpowerful representation, which can then be fused together for differentiablerendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughputbackbone operating on multi-view images, which can be produced from text orsingle-view image input by leveraging multi-view diffusion models. Extensiveexperiments demonstrate the high fidelity and efficiency of our approach.Notably, we maintain the fast speed to generate 3D objects within 5 secondswhile boosting the training resolution to 512, thereby achievinghigh-resolution 3D content generation.|2024|ECCV|https://github.com/3DTopia/LGM| |未复现|
|71|VideoMamba: State Space Model for Efficient Video Understanding|Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao|https://arxiv.org/abs/2403.06977|Addressing the dual challenges of local redundancy and global dependencies invideo understanding, this work innovatively adapts the Mamba to the videodomain. The proposed VideoMamba overcomes the limitations of existing 3Dconvolution neural networks and video transformers. Its linear-complexityoperator enables efficient long-term modeling, which is crucial forhigh-resolution long video understanding. Extensive evaluations revealVideoMamba's four core abilities: (1) Scalability in the visual domain withoutextensive dataset pretraining, thanks to a novel self-distillation technique;(2) Sensitivity for recognizing short-term actions even with fine-grainedmotion differences; (3) Superiority in long-term video understanding,showcasing significant advancements over traditional feature-based models; and(4) Compatibility with other modalities, demonstrating robustness inmulti-modal contexts. Through these distinct advantages, VideoMamba sets a newbenchmark for video understanding, offering a scalable and efficient solutionfor comprehensive video understanding. All the code and models are available athttps://github.com/OpenGVLab/VideoMamba.|2024|ECCV|https://github.com/OpenGVLab/video-mamba-suite| |未复现|
|72|DriveLM: Driving with Graph Visual Question Answering|Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, Hongyang Li|https://arxiv.org/abs/2312.14150|We study how vision-language models (VLMs) trained on web-scale data can beintegrated into end-to-end driving systems to boost generalization and enableinteractivity with human users. While recent approaches adapt VLMs to drivingvia single-round visual question answering (VQA), human drivers reason aboutdecisions in multiple steps. Starting from the localization of key objects,humans estimate object interactions before taking actions. The key insight isthat with our proposed task, Graph VQA, where we model graph-structuredreasoning through perception, prediction and planning question-answer pairs, weobtain a suitable proxy task to mimic the human reasoning process. Weinstantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and proposea VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQAand end-to-end driving. The experiments demonstrate that Graph VQA provides asimple, principled framework for reasoning about a driving scene, andDriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agentbaseline performs end-to-end autonomous driving competitively in comparison tostate-of-the-art driving-specific architectures. Notably, its benefits arepronounced when it is evaluated zero-shot on unseen objects or sensorconfigurations. We hope this work can be the starting point to shed new lighton how to apply VLMs for autonomous driving. To facilitate future research, allcode, data, and models are available to the public.|2024|ECCV|https://github.com/OpenDriveLab/DriveLM| |未复现|
|73|GRiT: A Generative Region-to-text Transformer for Object Understanding|Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang|https://arxiv.org/abs/2212.00280|This paper presents a Generative RegIon-to-Text transformer, GRiT, for objectunderstanding. The spirit of GRiT is to formulate object understanding as<region, text> pairs, where region locates objects and text describes objects.For example, the text in object detection denotes class names while that indense captioning refers to descriptive sentences. Specifically, GRiT consistsof a visual encoder to extract image features, a foreground object extractor tolocalize objects, and a text decoder to generate open-set object descriptions.With the same model architecture, GRiT can understand objects via not onlysimple nouns, but also rich descriptive sentences including object attributesor actions. Experimentally, we apply GRiT to object detection and densecaptioning tasks. GRiT achieves 60.4 AP on COCO 2017 test-dev for objectdetection and 15.5 mAP on Visual Genome for dense captioning. Code is availableat https://github.com/JialianW/GRiT|2024|ECCV|https://github.com/JialianW/GRiT| |未复现|
|74|PointLLM: Empowering Large Language Models to Understand Point Clouds|Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin|https://arxiv.org/abs/2308.16911|The unprecedented advancements in Large Language Models (LLMs) have shown aprofound impact on natural language processing but are yet to fully embrace therealm of 3D understanding. This paper introduces PointLLM, a preliminary effortto fill this gap, enabling LLMs to understand point clouds and offering a newavenue beyond 2D visual data. PointLLM understands colored object point cloudswith human instructions and generates contextually appropriate responses,illustrating its grasp of point clouds and common sense. Specifically, itleverages a point cloud encoder with a powerful LLM to effectively fusegeometric, appearance, and linguistic information. We collect a novel datasetcomprising 660K simple and 70K complex point-text instruction pairs to enable atwo-stage training strategy: aligning latent spaces and subsequentlyinstruction-tuning the unified model. To rigorously evaluate the perceptual andgeneralization capabilities of PointLLM, we establish two benchmarks:Generative 3D Object Classification and 3D Object Captioning, assessed throughthree different methods, including human evaluation, GPT-4/ChatGPT evaluation,and traditional metrics. Experimental results reveal PointLLM's superiorperformance over existing 2D and 3D baselines, with a notable achievement inhuman-evaluated object captioning tasks where it surpasses human annotators inover 50% of the samples. Codes, datasets, and benchmarks are available athttps://github.com/OpenRobotLab/PointLLM .|2024|ECCV|https://github.com/OpenRobotLab/PointLLM| |未复现|
|75|nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding|Benjin Zhu, Zhe Wang, and Hongsheng Li|https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00730.pdf|Existing benchmarks for 3D semantic occupancy prediction in autonomous driving are limited by low resolution (up to [512×512×40] with 0.2m voxel size) and inaccurate annotations, hindering the unifica tion of 3D scene understanding through the occupancy representation. Moreover, previous methods can only generate occupancy predictions at 0.4m resolution or lower, requiring post-upsampling to reach their full resolution (0.2m). The root of these limitations lies in the sparsity, noise, and even errors present in the raw data. In this paper, we overcome these challenges by introducing nuCraft, a high-resolution and accurate semantic occupancy dataset derived from nuScenes. nuCraft offers an 8× increase in resolution ([1024 × 1024 × 80] with voxel size of 0.1m) and more precise semantic annotations compared to previous benchmarks. To address the high memory cost of high-resolution occupancy prediction, we propose VQ-Occ, a novel method that encodes occupancy data into a compact latent feature space using a VQ-VAE. This approach simplifies semantic occupancy prediction into feature simulation in the VQ latent space, making it easier and more memory-efficient. Our method enables direct generation of semantic occupancy fields at high resolution without post-upsampling, facilitating a more unified approach to 3D scene under standing. We validate the superior quality of nuCraft and the effective ness of VQ-Occ through extensive experiments, demonstrating significant advancements over existing benchmarks and methods.|2024|ECCV|/| |未复现|
|76|Adversarial Diffusion Distillation|Axel Sauer, Dominik Lorenz, Andreas Blattmann, Robin Rombach|https://arxiv.org/abs/2311.17042|We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models.|2024|ECCV|https://github.com/AMD-AIG-AIMA/AMD-Diffusion-Distillation| |未复现|
|77|Generative Image Dynamics|Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski|https://arxiv.org/abs/2309.07906|We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics such as trees, flowers, candles, and clothes swaying in the wind. We model this dense, long-term motion prior in the Fourier domain:given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an image-based rendering module, these trajectories can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to realistically interact with objects in real pictures by interpreting the spectral volumes as image-space modal bases, which approximate object dynamics.|2024|CVPR|https://generative-dynamics.github.io/|最佳论文|未复现|
|78|Rich Human Feedback for Text-to-Image Generation|Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai J Kohlhoff, Deepak Ramachandran, Vidhya Navalpakkam|https://arxiv.org/pdf/2312.10240|Recent Text-to-Image (T2I) generation models such as Stable Diffusion andImagen have made significant progress in generating high-resolution imagesbased on text descriptions. However, many generated images still suffer fromissues such as artifacts/implausibility, misalignment with text descriptions,and low aesthetic quality. Inspired by the success of Reinforcement Learningwith Human Feedback (RLHF) for large language models, prior works collectedhuman-provided scores as feedback on generated images and trained a rewardmodel to improve the T2I generation. In this paper, we enrich the feedbacksignal by (i) marking image regions that are implausible or misaligned with thetext, and (ii) annotating which words in the text prompt are misrepresented ormissing on the image. We collect such rich human feedback on 18K generatedimages (RichHF-18K) and train a multimodal transformer to predict the richfeedback automatically. We show that the predicted rich human feedback can beleveraged to improve image generation, for example, by selecting high-qualitytraining data to finetune and improve the generative models, or by creatingmasks with predicted heatmaps to inpaint the problematic regions. Notably, theimprovements generalize to models (Muse) beyond those used to generate theimages on which human feedback data were collected (Stable Diffusion variants).The RichHF-18K data set will be released in our GitHub repository:https://github.com/google-research/google-research/tree/master/richhf_18k.|2024|CVPR|https://github.com/youweiliang/RichHF|最佳论文|未复现|
|79|Mip-Splatting: Alias-free 3D Gaussian Splatting|Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger|https://arxiv.org/abs/2311.16493|Recently, 3D Gaussian Splatting has demonstrated impressive novel viewsynthesis results, reaching high fidelity and efficiency. However, strongartifacts can be observed when changing the sampling rate, \eg, by changingfocal length or camera distance. We find that the source for this phenomenoncan be attributed to the lack of 3D frequency constraints and the usage of a 2Ddilation filter. To address this problem, we introduce a 3D smoothing filterwhich constrains the size of the 3D Gaussian primitives based on the maximalsampling frequency induced by the input views, eliminating high-frequencyartifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mipfilter, which simulates a 2D box filter, effectively mitigates aliasing anddilation issues. Our evaluation, including scenarios such a training onsingle-scale images and testing on multiple scales, validates the effectivenessof our approach.|2024|CVPR|https://github.com/autonomousvision/mip-splatting|最佳学生论文|未复现|
|80|BioCLIP: A Vision Foundation Model for the Tree of Life|Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su|https://arxiv.org/abs/2311.18803|Images of the natural world, collected by a variety of cameras, from dronesto individual phones, are increasingly abundant sources of biologicalinformation. There is an explosion of computational methods and tools,particularly computer vision, for extracting biologically relevant informationfrom images for science and conservation. Yet most of these are bespokeapproaches designed for a specific task and are not easily adaptable orextendable to new questions, contexts, and datasets. A vision model for generalorganismal biology questions on images is of timely need. To approach this, wecurate and release TreeOfLife-10M, the largest and most diverse ML-readydataset of biology images. We then develop BioCLIP, a foundation model for thetree of life, leveraging the unique properties of biology captured byTreeOfLife-10M, namely the abundance and variety of images of plants, animals,and fungi, together with the availability of rich structured biologicalknowledge. We rigorously benchmark our approach on diverse fine-grained biologyclassification tasks and find that BioCLIP consistently and substantiallyoutperforms existing baselines (by 16% to 17% absolute). Intrinsic evaluationreveals that BioCLIP has learned a hierarchical representation conforming tothe tree of life, shedding light on its strong generalizability.|2024|CVPR|https://github.com/Imageomics/bioclip|最佳学生论文|未复现|
|81|4D Gaussian Splatting for Real-Time Dynamic Scene Rendering|Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang|https://arxiv.org/abs/2310.08528|Representing and rendering dynamic scenes has been an important butchallenging task. Especially, to accurately model complex motions, highefficiency is usually hard to guarantee. To achieve real-time dynamic scenerendering while also enjoying high training and storage efficiency, we propose4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenesrather than applying 3D-GS for each individual frame. In 4D-GS, a novelexplicit representation containing both 3D Gaussians and 4D neural voxels isproposed. A decomposed neural voxel encoding algorithm inspired by HexPlane isproposed to efficiently build Gaussian features from 4D neural voxels and thena lightweight MLP is applied to predict Gaussian deformations at noveltimestamps. Our 4D-GS method achieves real-time rendering under highresolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU whilemaintaining comparable or better quality than previous state-of-the-artmethods. More demos and code are available athttps://guanjunwu.github.io/4dgs/.|2024|CVPR|https://github.com/hustvl/4DGaussians| |未复现|
|82|Depth Anything: Unleashing The Power of Large-Scale Unlabeled Data|Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao|https://arxiv.org/abs/2401.10891|This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. |2024|CVPR|https://github.com/LiheYoung/Depth-Anything| |未复现|
|83|LISA: Reasoning Segmentation Via Large Language Model|Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia|https://arxiv.org/abs/2308.00692|Although perception systems have made remarkable advancements in recentyears, they still rely on explicit human instruction or pre-defined categoriesto identify the target objects before executing visual recognition tasks. Suchsystems cannot actively reason and comprehend implicit user intention. In thiswork, we propose a new segmentation task -- reasoning segmentation. The task isdesigned to output a segmentation mask given a complex and implicit query text.Furthermore, we establish a benchmark comprising over one thousandimage-instruction-mask data samples, incorporating intricate reasoning andworld knowledge for evaluation purposes. Finally, we present LISA: largeLanguage Instructed Segmentation Assistant, which inherits the languagegeneration capabilities of multimodal Large Language Models (LLMs) while alsopossessing the ability to produce segmentation masks. We expand the originalvocabulary with a <SEG> token and propose the embedding-as-mask paradigm tounlock the segmentation capability. Remarkably, LISA can handle cases involvingcomplex reasoning and world knowledge. Also, it demonstrates robust zero-shotcapability when trained exclusively on reasoning-free datasets. In addition,fine-tuning the model with merely 239 reasoning segmentation data samplesresults in further performance enhancement. Both quantitative and qualitativeexperiments show our method effectively unlocks new reasoning segmentationcapabilities for multimodal LLMs. |2024|CVPR|https://github.com/dvlab-research/LISA| |未复现|
|84|InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks|Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai|https://arxiv.org/abs/2312.14238|The exponential growth of large language models (LLMs) has opened up numerouspossibilities for multimodal AGI systems. However, the progress in vision andvision-language foundation models, which are also critical elements ofmulti-modal AGI, has not kept pace with LLMs. In this work, we design alarge-scale vision-language foundation model (InternVL), which scales up thevision foundation model to 6 billion parameters and progressively aligns itwith the LLM, using web-scale image-text data from various sources. This modelcan be broadly applied to and achieve state-of-the-art performance on 32generic visual-linguistic benchmarks including visual perception tasks such asimage-level or pixel-level recognition, vision-language tasks such as zero-shotimage/video classification, zero-shot image/video-text retrieval, and link withLLMs to create multi-modal dialogue systems. It has powerful visualcapabilities and can be a good alternative to the ViT-22B. We hope that ourresearch could contribute to the development of multi-modal large models. |2024|CVPR|https://github.com/OpenGVLab/InternVL| |未复现|
|85|MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark|Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen|https://arxiv.org/abs/2311.16502|We introduce MMMU: a new benchmark designed to evaluate multimodal models onmassive multi-discipline tasks demanding college-level subject knowledge anddeliberate reasoning. MMMU includes 11.5K meticulously collected multimodalquestions from college exams, quizzes, and textbooks, covering six coredisciplines: Art & Design, Business, Science, Health & Medicine, Humanities &Social Science, and Tech & Engineering. These questions span 30 subjects and183 subfields, comprising 30 highly heterogeneous image types, such as charts,diagrams, maps, tables, music sheets, and chemical structures. Unlike existingbenchmarks, MMMU focuses on advanced perception and reasoning withdomain-specific knowledge, challenging models to perform tasks akin to thosefaced by experts. The evaluation of 14 open-source LMMs as well as theproprietary GPT-4V(ision) and Gemini highlights the substantial challengesposed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieveaccuracies of 56% and 59% respectively, indicating significant room forimprovement. We believe MMMU will stimulate the community to buildnext-generation multimodal foundation models towards expert artificial generalintelligence.|2024|CVPR|https://github.com/MMMU-Benchmark/MMMU| |未复现|
|86|EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything|Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra|https://arxiv.org/abs/2312.00863|Segment Anything Model (SAM) has emerged as a powerful tool for numerousvision applications. A key component that drives the impressive performance forzero-shot transfer and high versatility is a super large Transformer modeltrained on the extensive high-quality SA-1B dataset. While beneficial, the hugecomputation cost of SAM model has limited its applications to wider real-worldapplications. To address this limitation, we propose EfficientSAMs,light-weight SAM models that exhibits decent performance with largely reducedcomplexity. Our idea is based on leveraging masked image pretraining, SAMI,which learns to reconstruct features from SAM image encoder for effectivevisual representation learning. Further, we take SAMI-pretrained light-weightimage encoders and mask decoder to build EfficientSAMs, and finetune the modelson SA-1B for segment anything task. We perform evaluations on multiple visiontasks including image classification, object detection, instance segmentation,and semantic object detection, and find that our proposed pretraining method,SAMI, consistently outperforms other masked image pretraining methods. Onsegment anything task such as zero-shot instance segmentation, ourEfficientSAMs with SAMI-pretrained lightweight image encoders perform favorablywith a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.|2024|CVPR|https://github.com/yformer/EfficientSAM| |未复现|
|87|Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)|Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee|https://arxiv.org/abs/2310.03744|Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible.|2024|CVPR|https://github.com/LLaVA-VL/LLaVA-NeXT| |未复现|
|88|DemoFusion: Democratising High-Resolution Image Generation With No $$$|Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma|https://arxiv.org/abs/2311.16973|High-resolution image generation with Generative Artificial Intelligence(GenAI) has immense potential but, due to the enormous capital investmentrequired for training, it is increasingly centralised to a few largecorporations, and hidden behind paywalls. This paper aims to democratisehigh-resolution GenAI by advancing the frontier of high-resolution generationwhile remaining accessible to a broad audience. We demonstrate that existingLatent Diffusion Models (LDMs) possess untapped potential for higher-resolutionimage generation. Our novel DemoFusion framework seamlessly extends open-sourceGenAI models, employing Progressive Upscaling, Skip Residual, and DilatedSampling mechanisms to achieve higher-resolution image generation. Theprogressive nature of DemoFusion requires more passes, but the intermediateresults can serve as "previews", facilitating rapid prompt iteration.|2024|CVPR|https://github.com/PRIS-CV/DemoFusion| |未复现|
|89|ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models|Lukas Höllein, Aljaž Božič, Norman Müller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhöfer, Matthias Nießner|https://arxiv.org/abs/2403.01807|3D asset generation is getting massive amounts of attention, inspired by the recent success of text-guided 2D content creation. Existing text-to-3D methods use pretrained text-to-image diffusion models in an optimization problem or fine-tune them on synthetic data, which often results in non-photorealistic 3D objects without backgrounds. In this paper, we present a method that leverages pretrained text-to-image models as a prior, and learn to generate multi-view images in a single denoising process from real-world data. Concretely, we propose to integrate 3D volume-rendering and cross-frame-attention layers into each block of the existing U-Net network of the text-to-image model. Moreover, we design an autoregressive generation that renders more 3D-consistent images at any viewpoint. We train our model on real-world datasets of objects and showcase its capabilities to generate instances with a variety of high-quality shapes and textures in authentic surroundings. Compared to the existing methods, the results generated by our method are consistent, and have favorable visual quality (-30% FID, -37% KID).|2024|CVPR|https://github.com/facebookresearch/ViewDiff| |未复现|
|90|OmniGlue: Generalizable Feature Matching with Foundation Model Guidance|Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andre Araujo|https://arxiv.org/abs/2405.12979|The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of  datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of  with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by  this http URL and model can be found at this https URL|2024|CVPR|https://github.com/google-research/omniglue| |未复现|
|91|DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks|Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, Lianwen Jin|https://arxiv.org/abs/2405.04408|Document image restoration is a crucial aspect of Document AI systems, as the quality of document images significantly influences the overall performance. Prevailing methods address distinct restoration tasks independently, leading to intricate systems and the incapability to harness the potential synergies of multi-task learning. To overcome this challenge, we propose DocRes, a generalist model that unifies five document image restoration tasks including dewarping, deshadowing, appearance enhancement, deblurring, and binarization. To instruct DocRes to perform various restoration tasks, we propose a novel visual prompt approach called Dynamic Task-Specific Prompt (DTSPrompt). The DTSPrompt for different tasks comprises distinct prior features, which are additional characteristics extracted from the input image. Beyond its role as a cue for task-specific execution, DTSPrompt can also serve as supplementary information to enhance the model's performance. Moreover, DTSPrompt is more flexible than prior visual prompt approaches as it can be seamlessly applied and adapted to inputs with high and variable resolutions. Experimental results demonstrate that DocRes achieves competitive or superior performance compared to existing state-of-the-art task-specific models. This underscores the potential of DocRes across a broader spectrum of document image restoration tasks. |2024|CVPR|https://github.com/ZZZHANG-jx/DocRes| |未复现|
|92|MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training|Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel|https://arxiv.org/abs/2311.17049|Contrastive pretraining of image-text foundation models, such as CLIP,demonstrated excellent zero-shot performance and improved robustness on a widerange of downstream tasks. However, these models utilize largetransformer-based encoders with significant memory and latency overhead whichpose challenges for deployment on mobile devices. In this work, we introduceMobileCLIP -- a new family of efficient image-text models optimized for runtimeperformance along with a novel and efficient training approach, namelymulti-modal reinforced training. The proposed training approach leveragesknowledge transfer from an image captioning model and an ensemble of strongCLIP encoders to improve the accuracy of efficient models. Our approach avoidstrain-time compute overhead by storing the additional knowledge in a reinforceddataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff forzero-shot classification and retrieval tasks on several datasets. OurMobileCLIP-S2 variant is 2.3$\times$ faster while more accurate compared toprevious best CLIP model based on ViT-B/16. We further demonstrate theeffectiveness of our multi-modal reinforced training by training a CLIP modelbased on ViT-B/16 image backbone and achieving +2.9% average performanceimprovement on 38 evaluation benchmarks compared to the previous best.Moreover, we show that the proposed approach achieves 10×-1000× improved learning efficiency when compared with non-reinforced CLIP training.|2024|CVPR|https://github.com/apple/ml-mobileclip| |未复现|
|93|Describing Differences in Image Sets with Natural Language|Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy|https://arxiv.org/abs/2312.02974|How do two sets of images differ? Discerning set-level differences is crucialfor understanding model behaviors and analyzing datasets, yet manually siftingthrough thousands of images is impractical. To aid in this discovery process,we explore the task of automatically describing the differences between two$\textbf{sets}$ of images, which we term Set Difference Captioning. This tasktakes in image sets $D_A$ and $D_B$, and outputs a description that is moreoften true on $D_A$ than $D_B$. We outline a two-stage approach that firstproposes candidate difference descriptions from image sets and then re-ranksthe candidates by checking how well they can differentiate the two sets. Weintroduce VisDiff, which first captions the images and prompts a language modelto propose candidate descriptions, then re-ranks these descriptions using CLIP.To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired imagesets with ground truth difference descriptions. We apply VisDiff to variousdomains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparingclassification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizingmodel failure modes (supervised ResNet), characterizing differences betweengenerative models (e.g., StableDiffusionV1 and V2), and discovering what makesimages memorable. Using VisDiff, we are able to find interesting and previouslyunknown differences in datasets and models, demonstrating its utility inrevealing nuanced insights.|2024|CVPR|https://github.com/Understanding-Visual-Datasets/VisDiff| |未复现|
|94|XFeat: Accelerated Features for Lightweight Image Matching|Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, Erickson R. Nascimento|https://arxiv.org/abs/2404.19174|We introduce a lightweight and accurate architecture for resource-efficientvisual correspondence. Our method, dubbed XFeat (Accelerated Features),revisits fundamental design choices in convolutional neural networks fordetecting, extracting, and matching local features. Our new model satisfies acritical need for fast and robust algorithms suitable to resource-limiteddevices. In particular, accurate image matching requires sufficiently largeimage resolutions - for this reason, we keep the resolution as large aspossible while limiting the number of channels in the network. Besides, ourmodel is designed to offer the choice of matching at the sparse or semi-denselevels, each of which may be more suitable for different downstreamapplications, such as visual navigation and augmented reality. Our model is thefirst to offer semi-dense matching efficiently, leveraging a novel matchrefinement module that relies on coarse local descriptors. XFeat is versatileand hardware-independent, surpassing current deep learning-based local featuresin speed (up to 5x faster) with comparable or better accuracy, proven in poseestimation and visual localization. We showcase it running in real-time on aninexpensive laptop CPU without specialized hardware optimizations. |2024|CVPR|https://github.com/verlab/accelerated_features| |未复现|
|95|pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction|David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann|https://arxiv.org/abs/2312.12337|We introduce pixelSplat, a feed-forward model that learns to reconstruct 3Dradiance fields parameterized by 3D Gaussian primitives from pairs of images.Our model features real-time and memory-efficient rendering for scalabletraining as well as fast 3D reconstruction at inference time. To overcome localminima inherent to sparse and locally supported representations, we predict adense probability distribution over 3D and sample Gaussian means from thatprobability distribution. We make this sampling operation differentiable via areparameterization trick, allowing us to back-propagate gradients through theGaussian splatting representation. We benchmark our method on wide-baselinenovel view synthesis on the real-world RealEstate10k and ACID datasets, wherewe outperform state-of-the-art light field transformers and acceleraterendering by 2.5 orders of magnitude while reconstructing an interpretable andeditable 3D radiance field.|2024|CVPR| | |未复现|
|96|GPT4Point: A Unified Framework for Point-Language Understanding and Generation|Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao|https://arxiv.org/abs/2312.02980|Multimodal Large Language Models (MLLMs) have excelled in 2D image-textcomprehension and image generation, but their understanding of the 3D world isnotably deficient, limiting progress in 3D language understanding andgeneration. To solve this problem, we introduce GPT4Point, an innovativegroundbreaking point-language multimodal model designed specifically forunified 3D object understanding and generation within the MLLM framework.GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-textreference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Pointis equipped with advanced capabilities for controllable 3D generation, it canget high-quality results through a low-quality point-text feature maintainingthe geometric shapes and colors. To support the expansive needs of 3Dobject-text pairs, we develop Pyramid-XL, a point-language dataset annotationengine. It constructs a large-scale database over 1M objects of varied textgranularity levels from the Objaverse-XL dataset, essential for trainingGPT4Point. A comprehensive benchmark has been proposed to evaluate 3Dpoint-language understanding capabilities. In extensive evaluations, GPT4Pointhas demonstrated superior performance in understanding and generation.|2024|CVPR| | |未复现|
|97|Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks|Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan|https://arxiv.org/abs/2311.06242|We introduce Florence-2, a novel vision foundation model with a unified,prompt-based representation for a variety of computer vision andvision-language tasks. While existing large vision models excel in transferlearning, they struggle to perform a diversity of tasks with simpleinstructions, a capability that implies handling the complexity of variousspatial hierarchy and semantic granularity. Florence-2 was designed to taketext-prompt as task instructions and generate desirable results in text forms,whether it be captioning, object detection, grounding or segmentation. Thismulti-task learning setup demands large-scale, high-quality annotated data. Tothis end, we co-developed FLD-5B that consists of 5.4 billion comprehensivevisual annotations on 126 million images, using an iterative strategy ofautomated image annotation and model refinement. We adopted asequence-to-sequence structure to train Florence-2 to perform versatile andcomprehensive vision tasks. Extensive evaluations on numerous tasksdemonstrated Florence-2 to be a strong vision foundation model contender withunprecedented zero-shot and fine-tuning capabilities.|2024|CVPR| | |未复现|
|98|Identity-Preserving Text-to-Video Generation by Frequency Decomposition|Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, Li Yuan|https://arxiv.org/abs/2411.17440|Identity-preserving text-to-video (IPT2V) generation aims to createhigh-fidelity videos with consistent human identity. It is an important task invideo generation but remains an open problem for generative models. This paperpushes the technical frontier of IPT2V in two directions that have not beenresolved in literature: (1) A tuning-free pipeline without tedious case-by-casefinetuning, and (2) A frequency-aware heuristic identity-preserving DiT-basedcontrol scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2Vmodel to keep human identity consistent in the generated video. Inspired byprior findings in frequency analysis of diffusion transformers, it employsidentity-control signals in the frequency domain, where facial features can bedecomposed into low-frequency global features and high-frequency intrinsicfeatures. First, from a low-frequency perspective, we introduce a global facialextractor, which encodes reference images and facial key points into a latentspace, generating features enriched with low-frequency information. Thesefeatures are then integrated into shallow layers of the network to alleviatetraining challenges associated with DiT. Second, from a high-frequencyperspective, we design a local facial extractor to capture high-frequencydetails and inject them into transformer blocks, enhancing the model's abilityto preserve fine-grained features. We propose a hierarchical training strategyto leverage frequency information for identity preservation, transforming avanilla pre-trained video generation model into an IPT2V model. Extensiveexperiments demonstrate that our frequency-aware heuristic scheme provides anoptimal control solution for DiT-based models. Thanks to this scheme, ourConsisID generates high-quality, identity-preserving videos, making stridestowards more effective IPT2V. Code: https://github.com/PKU-YuanGroup/ConsisID.|2025|CVPR|https://github.com/PKU-YuanGroup/ConsisID| |未复现|
|99|Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models|Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Yuan-Fang Li, Cunjian Chen, Yu Qiao|https://arxiv.org/abs/2407.15642|Diffusion models have achieved great progress in image animation due topowerful generative capabilities. However, maintaining spatio-temporalconsistency with detailed information from the input static image over time(e.g., style, background, and object of the input static image) and ensuringsmoothness in animated video narratives guided by textual prompts still remainschallenging. In this paper, we introduce Cinemo, a novel image animationapproach towards achieving better motion controllability, as well as strongertemporal consistency and smoothness. In general, we propose three effectivestrategies at the training and inference stages of Cinemo to accomplish ourgoal. At the training stage, Cinemo focuses on learning the distribution ofmotion residuals, rather than directly predicting subsequent via a motiondiffusion model. Additionally, a structural similarity index-based strategy isproposed to enable Cinemo to have better controllability of motion intensity.At the inference stage, a noise refinement technique based on discrete cosinetransformation is introduced to mitigate sudden motion changes. Such threestrategies enable Cinemo to produce highly consistent, smooth, andmotion-controllable results. Compared to previous methods, Cinemo offerssimpler and more precise user controllability. Extensive experiments againstseveral state-of-the-art methods, including both commercial tools and researchapproaches, across multiple metrics, demonstrate the effectiveness andsuperiority of our proposed approach.|2025|CVPR|https://github.com/maxin-cn/Cinemo| |未复现|
|100|X-Dyna: Expressive Dynamic Human Image Animation|Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, Mohammad Soleymani|https://arxiv.org/abs/2501.10021|We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline foranimating a single human image using facial expressions and body movementsderived from a driving video, that generates realistic, context-aware dynamicsfor both the subject and the surrounding environment. Building on priorapproaches centered on human pose control, X-Dyna addresses key shortcomingscausing the loss of dynamic details, enhancing the lifelike qualities of humanvideo animations. At the core of our approach is the Dynamics-Adapter, alightweight module that effectively integrates reference appearance contextinto the spatial attentions of the diffusion backbone while preserving thecapacity of motion modules in synthesizing fluid and intricate dynamic details.Beyond body pose control, we connect a local control module with our model tocapture identity-disentangled facial expressions, facilitating accurateexpression transfer for enhanced realism in animated scenes. Together, thesecomponents form a unified framework capable of learning physical human motionand natural scene dynamics from a diverse blend of human and scene videos.Comprehensive qualitative and quantitative evaluations demonstrate that X-Dynaoutperforms state-of-the-art methods, creating highly lifelike and expressiveanimations. |2025|CVPR|https://github.com/bytedance/X-Dyna| |未复现|
|101|PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation|Qiyao Xue, Xiangyu Yin, Boyuan Yang, Wei Gao|https://arxiv.org/pdf/2412.00596|Text-to-video (T2V) generation has been recently enabled by transformer-baseddiffusion models, but current T2V models lack capabilities in adhering to thereal-world common knowledge and physical rules, due to their limitedunderstanding of physical realism and deficiency in temporal modeling. Existingsolutions are either data-driven or require extra model inputs, but cannot begeneralizable to out-of-distribution domains. In this paper, we present PhyT2V,a new data-independent T2V technique that expands the current T2V model'scapability of video generation to out-of-distribution domains, by enablingchain-of-thought and step-back reasoning in T2V prompting. Our experiments showthat PhyT2V improves existing T2V models' adherence to real-world physicalrules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.|2025|CVPR|https://github.com/pittisl/PhyT2V| |未复现|
|102|Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model|Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan|https://arxiv.org/abs/2411.19108|As a fundamental backbone for video generation, diffusion models arechallenged by low inference speed due to the sequential nature of denoising.Previous methods speed up the models by caching and reusing model outputs atuniformly selected timesteps. However, such a strategy neglects the fact thatdifferences among model outputs are not uniform across timesteps, which hindersselecting the appropriate model outputs to cache, leading to a poor balancebetween inference efficiency and visual quality. In this study, we introduceTimestep Embedding Aware Cache (TeaCache), a training-free caching approachthat estimates and leverages the fluctuating differences among model outputsacross timesteps. Rather than directly using the time-consuming model outputs,TeaCache focuses on model inputs, which have a strong correlation with themodeloutputs while incurring negligible computational cost. TeaCache firstmodulates the noisy inputs using the timestep embeddings to ensure theirdifferences better approximating those of model outputs. TeaCache thenintroduces a rescaling strategy to refine the estimated differences andutilizes them to indicate output caching. Experiments show that TeaCacheachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%Vbench score) degradation of visual quality.|2025|CVPR|https://github.com/ali-vilab/TeaCache| |未复现|
|103|AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion|Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, Jing Liu|https://arxiv.org/abs/2503.07418|The task of video generation requires synthesizing visually realistic andtemporally coherent video frames. Existing methods primarily use asynchronousauto-regressive models or synchronous diffusion models to address thischallenge. However, asynchronous auto-regressive models often suffer frominconsistencies between training and inference, leading to issues such as erroraccumulation, while synchronous diffusion models are limited by their relianceon rigid sequence length. To address these issues, we introduce Auto-RegressiveDiffusion (AR-Diffusion), a novel model that combines the strengths ofauto-regressive and diffusion models for flexible, asynchronous videogeneration. Specifically, our approach leverages diffusion to gradually corruptvideo frames in both training and inference, reducing the discrepancy betweenthese phases. Inspired by auto-regressive generation, we incorporate anon-decreasing constraint on the corruption timesteps of individual frames,ensuring that earlier frames remain clearer than subsequent ones. This setup,together with temporal causal attention, enables flexible generation of videoswith varying lengths while preserving temporal coherence. In addition, wedesign two specialized timestep schedulers: the FoPP scheduler for balancedtimestep sampling during training, and the AD scheduler for flexible timestepdifferences during inference, supporting both synchronous and asynchronousgeneration. Extensive experiments demonstrate the superiority of our proposedmethod, which achieves competitive and state-of-the-art results across fourchallenging benchmarks.|2025|CVPR|https://github.com/iva-mzsun/AR-Diffusion| |未复现|
|104|Number it: Temporal Grounding Videos like Flipping Manga|Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, Xu Yang|https://arxiv.org/abs/2411.10332|Video Large Language Models (Vid-LLMs) have made remarkable advancements incomprehending video content for QA dialogue. However, they struggle to extendthis visual understanding to tasks requiring precise temporal localization,known as Video Temporal Grounding (VTG). To address this gap, we introduceNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visualcomprehension with temporal grounding by adding unique numerical identifiers toeach video frame. Treating a video as a sequence of numbered frame images,NumPro transforms VTG into an intuitive process: flipping through manga panelsin sequence. This allows Vid-LLMs to "read" event timelines, accurately linkingvisual content with corresponding temporal information. Our experimentsdemonstrate that NumPro significantly boosts VTG performance of top-tierVid-LLMs without additional computational cost. Furthermore, fine-tuning on aNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassingprevious top-performing methods by up to 6.9\% in mIoU for moment retrieval and8.5\% in mAP for highlight detection. |2025|CVPR|https://github.com/yongliang-wu/NumPro| |未复现|
|105|Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing|Hanhui Wang, Yihua Zhang, Ruizheng Bai, Yue Zhao, Sijia Liu, Zhengzhong Tu|https://arxiv.org/abs/2411.16832|Recent advancements in diffusion models have made generative image editingmore accessible, enabling creative edits but raising ethical concerns,particularly regarding malicious edits to human portraits that threaten privacyand identity security. Existing protection methods primarily rely onadversarial perturbations to nullify edits but often fail against diverseediting requests. We propose FaceLock, a novel approach to portrait protectionthat optimizes adversarial perturbations to destroy or significantly alterbiometric information, rendering edited outputs biometrically unrecognizable.FaceLock integrates facial recognition and visual perception into perturbationoptimization to provide robust protection against various editing attempts. Wealso highlight flaws in commonly used evaluation metrics and reveal how theycan be manipulated, emphasizing the need for reliable assessments ofprotection. Experiments show FaceLock outperforms baselines in defendingagainst malicious edits and is robust against purification techniques. Ablationstudies confirm its stability and broad applicability across diffusion-basedediting algorithms. Our work advances biometric defense and sets the foundationfor privacy-preserving practices in image editing.|2025|CVPR|https://github.com/taco-group/FaceLock| |未复现|
|106|h-Edit: Effective and Flexible Diffusion-Based Editing via Doob’s h-Transform|Toan Nguyen, Kien Do, Duc Kieu, Thin Nguyen|https://arxiv.org/abs/2503.02187|We introduce a theoretical framework for diffusion-based image editing byformulating it as a reverse-time bridge modeling problem. This approachmodifies the backward process of a pretrained diffusion model to construct abridge that converges to an implicit distribution associated with the editingtarget at time 0. Building on this framework, we propose h-Edit, a novelediting method that utilizes Doob's h-transform and Langevin Monte Carlo todecompose the update of an intermediate edited sample into two components: a"reconstruction" term and an "editing" term. This decomposition providesflexibility, allowing the reconstruction term to be computed via existinginversion techniques and enabling the combination of multiple editing terms tohandle complex editing tasks. To our knowledge, h-Edit is the firsttraining-free method capable of performing simultaneous text-guided andreward-model-based editing. Extensive experiments, both quantitative andqualitative, show that h-Edit outperforms state-of-the-art baselines in termsof editing effectiveness and faithfulness. |2025|CVPR|https://github.com/nktoan/h-edit| |未复现|
|107|OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels|Meng Lou, Yizhou Yu|https://arxiv.org/abs/2502.20087|Top-down attention plays a crucial role in the human vision system, whereinthe brain initially obtains a rough overview of a scene to discover salientcues (i.e., overview first), followed by a more careful finer-grainedexamination (i.e., look closely next). However, modern ConvNets remain confinedto a pyramid structure that successively downsamples the feature map forreceptive field expansion, neglecting this crucial biomimetic principle. Wepresent OverLoCK, the first pure ConvNet backbone architecture that explicitlyincorporates a top-down attention mechanism. Unlike pyramid backbone networks,our design features a branched architecture with three synergisticsub-networks: 1) a Base-Net that encodes low/mid-level features; 2) alightweight Overview-Net that generates dynamic top-down attention throughcoarse global context modeling (i.e., overview first); and 3) a robustFocus-Net that performs finer-grained perception guided by top-down attention(i.e., look closely next). To fully unleash the power of top-down attention, wefurther propose a novel context-mixing dynamic convolution (ContMix) thateffectively models long-range dependencies while preserving inherent localinductive biases even when the input resolution increases, addressing criticallimitations in existing convolutions. Our OverLoCK exhibits a notableperformance improvement over existing methods. For instance, OverLoCK-Tachieves a Top-1 accuracy of 84.2%, significantly surpassing ConvNeXt-B whileusing only around one-third of the FLOPs/parameters. On object detection, ourOverLoCK-S clearly surpasses MogaNet-B by 1% in AP^b. On semantic segmentation,our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7% in mIoU. |2025|CVPR|https://github.com/LMMMEng/OverLoCK|oral|未复现|
|108|Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space|Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan|https://arxiv.org/pdf/2503.09419|Latent Diffusion Models (LDMs) are known to have an unstable generationprocess, where even small perturbations or shifts in the input noise can leadto significantly different outputs. This hinders their applicability inapplications requiring consistent results. In this work, we redesign LDMs toenhance consistency by making them shift-equivariant. While introducinganti-aliasing operations can partially improve shift-equivariance, significantaliasing and inconsistency persist due to the unique challenges in LDMs,including 1) aliasing amplification during VAE training and multiple U-Netinferences, and 2) self-attention modules that inherently lackshift-equivariance. To address these issues, we redesign the attention modulesto be shift-equivariant and propose an equivariance loss that effectivelysuppresses the frequency bandwidth of the features in the continuous domain.The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and isalso robust to irregular warping. Extensive experiments demonstrate that AF-LDMproduces significantly more consistent results than vanilla LDM across variousapplications, including video editing and image-to-image translation. |2025|CVPR|https://github.com/SingleZombie/AFLDM|oral|未复现|
|109|3D Student Splatting and Scooping|Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang|https://arxiv.org/abs/2503.10148|Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novelview synthesis, and has spiked a new wave of research in neural rendering andrelated applications. As 3DGS is becoming a foundational component of manymodels, any improvement on 3DGS itself can bring huge benefits. To this end, weaim to improve the fundamental paradigm and formulation of 3DGS. We argue thatas an unnormalized mixture model, it needs to be neither Gaussians norsplatting. We subsequently propose a new mixture model consisting of flexibleStudent's t distributions, with both positive (splatting) and negative(scooping) densities. We name our model Student Splatting and Scooping, or SSS.When providing better expressivity, SSS also poses new challenges in learning.Therefore, we also propose a new principled sampling approach for optimization.Through exhaustive evaluation and comparison, across multiple datasets,settings, and metrics, we demonstrate that SSS outperforms existing methods interms of quality and parameter efficiency, e.g. achieving matching or betterquality with similar numbers of components, and obtaining comparable resultswhile reducing the component number by as much as 82%.|2025|CVPR|https://github.com/realcrane/3D-student-splating-and-scooping|oral|未复现|
|110|CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models|Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell|https://arxiv.org/pdf/2412.12093|Reconstructing photorealistic and dynamic portrait avatars from images isessential to many applications including advertising, visual effects, andvirtual reality. Depending on the application, avatar reconstruction involvesdifferent capture setups and constraints $-$ for example, visual effectsstudios use camera arrays to capture hundreds of reference images, whilecontent creators may seek to animate a single portrait image downloaded fromthe internet. As such, there is a large and heterogeneous ecosystem of methodsfor avatar reconstruction. Techniques based on multi-view stereo or neuralrendering achieve the highest quality results, but require hundreds ofreference images. Recent generative models produce convincing avatars from asingle reference image, but visual fidelity yet lags behind multi-viewtechniques. Here, we present CAP4D: an approach that uses a morphablemulti-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portraitavatars from any number of reference images (i.e., one to 100) and animate andrender them in real time. Our approach demonstrates state-of-the-artperformance for single-, few-, and multi-image 4D portrait avatarreconstruction, and takes steps to bridge the gap in visual fidelity betweensingle-image and multi-view reconstruction techniques.|2025|CVPR|https://github.com/felixtaubner/cap4d/|oral|未复现|
|111|Multi-view Reconstruction via SfM-guided Monocular Depth Estimation|Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao|https://arxiv.org/pdf/2503.14483|In this paper, we present a new method for multi-view geometricreconstruction. In recent years, large vision models have rapidly developed,performing excellently across various tasks and demonstrating remarkablegeneralization capabilities. Some works use large vision models for monoculardepth estimation, which have been applied to facilitate multi-viewreconstruction tasks in an indirect manner. Due to the ambiguity of themonocular depth estimation task, the estimated depth values are usually notaccurate enough, limiting their utility in aiding multi-view reconstruction. Wepropose to incorporate SfM information, a strong multi-view prior, into thedepth estimation process, thus enhancing the quality of depth prediction andenabling their direct application in multi-view geometric reconstruction.Experimental results on public real-world datasets show that our methodsignificantly improves the quality of depth estimation compared to previousmonocular depth estimation works. Additionally, we evaluate the reconstructionquality of our approach in various types of scenes including indoor,streetscape, and aerial views, surpassing state-of-the-art MVS methods. |2025|CVPR|https://github.com/zju3dv/Murre|oral|未复现|
|112|Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models|Zhejun Zhang, Peter Karkus, Maximilian Igl, Wenhao Ding, Yuxiao Chen, Boris Ivanovic, Marco Pavone|https://arxiv.org/pdf/2412.05334|Traffic simulation aims to learn a policy for traffic agents that, whenunrolled in closed-loop, faithfully recovers the joint distribution oftrajectories observed in the real world. Inspired by large language models,tokenized multi-agent policies have recently become the state-of-the-art intraffic simulation. However, they are typically trained through open-loopbehavior cloning, and thus suffer from covariate shift when executed inclosed-loop during simulation. In this work, we present Closest Among Top-K(CAT-K) rollouts, a simple yet effective closed-loop fine-tuning strategy tomitigate covariate shift. CAT-K fine-tuning only requires existing trajectorydata, without reinforcement learning or generative adversarial imitation.Concretely, CAT-K fine-tuning enables a small 7M-parameter tokenized trafficsimulation policy to outperform a 102M-parameter model from the same modelfamily, achieving the top spot on the Waymo Sim Agent Challenge leaderboard atthe time of submission. |2025|CVPR|https://github.com/NVlabs/catk|oral|未复现|
|113|CustAny: Customizing Anything from A Single Example|Lingjie Kong, Kai Wu, Xiaobin Hu, Wenhui Han, Jinlong Peng, Chengming Xu, Donghao Luo, Mengtian Li, Jiangning Zhang, Chengjie Wang, Yanwei Fu|https://arxiv.org/pdf/2406.11643v4|Recent advances in diffusion-based text-to-image models have simplifiedcreating high-fidelity images, but preserving the identity (ID) of specificelements, like a personal dog, is still challenging. Object customization,using reference images and textual descriptions, is key to addressing thisissue. Current object customization methods are either object-specific,requiring extensive fine-tuning, or object-agnostic, offering zero-shotcustomization but limited to specialized domains. The primary issue ofpromoting zero-shot object customization from specific domains to the generaldomain is to establish a large-scale general ID dataset for model pre-training,which is time-consuming and labor-intensive. In this paper, we propose a novelpipeline to construct a large dataset of general objects and build theMulti-Category ID-Consistent (MC-IDC) dataset, featuring 315k text-imagesamples across 10k categories. With the help of MC-IDC, we introduceCustomizing Anything (CustAny), a zero-shot framework that maintains IDfidelity and supports flexible text editing for general objects. CustAnyfeatures three key components: a general ID extraction module, a dual-level IDinjection module, and an ID-aware decoupling module, allowing it to customizeany object from a single reference image and text prompt. Experimentsdemonstrate that CustAny outperforms existing methods in both general objectcustomization and specialized domains like human customization and virtualtry-on. Our contributions include a large-scale dataset, the CustAny frameworkand novel ID processing to advance this field. |2025|CVPR|https://github.com/LingjieKong-fdu/CustAny|oral|未复现|
|114|VGGT:Visual Geometry Grounded Transformer|Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny|https://arxiv.org/pdf/2503.11651|We present VGGT, a feed-forward neural network that directly infers all key3D attributes of a scene, including camera parameters, point maps, depth maps,and 3D point tracks, from one, a few, or hundreds of its views. This approachis a step forward in 3D computer vision, where models have typically beenconstrained to and specialized for single tasks. It is also simple andefficient, reconstructing images in under one second, and still outperformingalternatives that require post-processing with visual geometry optimizationtechniques. The network achieves state-of-the-art results in multiple 3D tasks,including camera parameter estimation, multi-view depth estimation, dense pointcloud reconstruction, and 3D point tracking. We also show that using pretrainedVGGT as a feature backbone significantly enhances downstream tasks, such asnon-rigid point tracking and feed-forward novel view synthesis. |2025|CVPR|https://github.com/facebookresearch/vggt|oral,Award Candidate|未复现|
|115|Navigation World Models|Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun|https://arxiv.org/pdf/2412.03572|Navigation is a fundamental skill of agents with visual-motor capabilities.We introduce a Navigation World Model (NWM), a controllable video generationmodel that predicts future visual observations based on past observations andnavigation actions. To capture complex environment dynamics, NWM employs aConditional Diffusion Transformer (CDiT), trained on a diverse collection ofegocentric videos of both human and robotic agents, and scaled up to 1 billionparameters. In familiar environments, NWM can plan navigation trajectories bysimulating them and evaluating whether they achieve the desired goal. Unlikesupervised navigation policies with fixed behavior, NWM can dynamicallyincorporate constraints during planning. Experiments demonstrate itseffectiveness in planning trajectories from scratch or by ranking trajectoriessampled from an external policy. Furthermore, NWM leverages its learned visualpriors to imagine trajectories in unfamiliar environments from a single inputimage, making it a flexible and powerful tool for next-generation navigationsystems.|2025|CVPR|https://github.com/facebookresearch/nwm/|oral|未复现|
|116|MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos|Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, Noah Snavely|https://arxiv.org/pdf/2412.04463|We present a system that allows for accurate, fast, and robust estimation ofcamera parameters and depth maps from casual monocular videos of dynamicscenes. Most conventional structure from motion and monocular SLAM techniquesassume input videos that feature predominantly static scenes with large amountsof parallax. Such methods tend to produce erroneous estimates in the absence ofthese conditions. Recent neural network-based approaches attempt to overcomethese challenges; however, such methods are either computationally expensive orbrittle when run on dynamic videos with uncontrolled camera motion or unknownfield of view. We demonstrate the surprising effectiveness of a deep visualSLAM framework: with careful modifications to its training and inferenceschemes, this system can scale to real-world videos of complex dynamic sceneswith unconstrained camera paths, including videos with little camera parallax.Extensive experiments on both synthetic and real videos demonstrate that oursystem is significantly more accurate and robust at camera pose and depthestimation when compared with prior and concurrent work, with faster orcomparable running times. |2025|CVPR|https://github.com/mega-sam/mega-sam|oral|未复现|
|117|FoundationStereo: Zero-Shot Stereo Matching|Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield|https://arxiv.org/pdf/2501.09898|Tremendous progress has been made in deep stereo matching to excel onbenchmark datasets through per-domain fine-tuning. However, achieving strongzero-shot generalization - a hallmark of foundation models in other computervision tasks - remains challenging for stereo matching. We introduceFoundationStereo, a foundation model for stereo depth estimation designed toachieve strong zero-shot generalization. To this end, we first construct alarge-scale (1M stereo pairs) synthetic training dataset featuring largediversity and high photorealism, followed by an automatic self-curationpipeline to remove ambiguous samples. We then design a number of networkarchitecture components to enhance scalability, including a side-tuning featurebackbone that adapts rich monocular priors from vision foundation models tomitigate the sim-to-real gap, and long-range context reasoning for effectivecost volume filtering. Together, these components lead to strong robustness andaccuracy across domains, establishing a new standard in zero-shot stereo depthestimation. |2025|CVPR|https://github.com/NVlabs/FoundationStereo/|oral,Award Candidate|未复现|
|118|The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition|Otto Brookes, Maksim Kukushkin, Majid Mirmehdi, Colleen Stephens, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus Zuberbühler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar Kühl, Tilo Burghardt|https://arxiv.org/pdf/2502.21201|Computer vision analysis of camera trap video footage is essential forwildlife conservation, as captured behaviours offer some of the earliestindicators of changes in population health. Recently, several high-impactanimal behaviour datasets and methods have been introduced to encourage theiruse; however, the role of behaviour-correlated background information and itssignificant effect on out-of-distribution generalisation remain unexplored. Inresponse, we present the PanAf-FGBG dataset, featuring 20 hours of wildchimpanzee behaviours, recorded at over 350 individual camera locations.Uniquely, it pairs every video with a chimpanzee (referred to as a foregroundvideo) with a corresponding background video (with no chimpanzee) from the samecamera location. We present two views of the dataset: one with overlappingcamera locations and one with disjoint locations. This setup enables, for thefirst time, direct evaluation of in-distribution and out-of-distributionconditions, and for the impact of backgrounds on behaviour recognition modelsto be quantified. All clips come with rich behavioural annotations and metadataincluding unique camera IDs and detailed textual scene descriptions.Additionally, we establish several baselines and present a highly effectivelatent-space normalisation technique that boosts out-of-distributionperformance by +5.42% mAP for convolutional and +3.75% mAP fortransformer-based models. Finally, we provide an in-depth analysis on the roleof backgrounds in out-of-distribution behaviour recognition, including the sofar unexplored impact of background durations (i.e., the count of backgroundframes within foreground videos).|2025|CVPR|https://obrookes.github.io/panaf-fgbg.github.io/|oral,Award Candidate|未复现|
|119|Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing|Bingliang Zhang, Wenda Chu, Julius Berner, Chenlin Meng, Anima Anandkumar, Yang Song|https://arxiv.org/pdf/2407.01521|Diffusion models have recently achieved success in solving Bayesian inverseproblems with learned data priors. Current methods build on top of thediffusion sampling process, where each denoising step makes small modificationsto samples from the previous step. However, this process struggles to correcterrors from earlier sampling steps, leading to worse performance in complicatednonlinear inverse problems, such as phase retrieval. To address this challenge,we propose a new method called Decoupled Annealing Posterior Sampling (DAPS)that relies on a novel noise annealing process. Specifically, we decoupleconsecutive steps in a diffusion sampling trajectory, allowing them to varyconsiderably from one another while ensuring their time-marginals anneal to thetrue posterior as we reduce noise levels. This approach enables the explorationof a larger solution space, improving the success rate for accuratereconstructions. We demonstrate that DAPS significantly improves sample qualityand stability across multiple image restoration tasks, particularly incomplicated nonlinear inverse problems.|2025|CVPR|https://github.com/zhangbingliang2019/DAPS|oral|未复现|
|120|MV-DUSt3R+: Single-StageSceneReconstruction fromSparseViewsIn2Seconds|Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan|https://arxiv.org/pdf/2412.06974|Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3Rno longer require camera calibration and camera pose estimation. However, theyonly process a pair of views at a time to infer pixel-aligned pointmaps. Whendealing with more than two views, a combinatorial number of error pronepairwise reconstructions are usually followed by an expensive globaloptimization, which often fails to rectify the pairwise reconstruction errors.To handle more views, reduce errors, and improve inference time, we propose thefast single-stage feed-forward network MV-DUSt3R. At its core are multi-viewdecoder blocks which exchange information across any number of views whileconsidering one reference view. To make our method robust to reference viewselection, we further propose MV-DUSt3R+, which employs cross-reference-viewblocks to fuse information across different reference view choices. To furtherenable novel view synthesis, we extend both by adding and jointly trainingGaussian splatting heads. Experiments on multi-view stereo reconstruction,multi-view pose estimation, and novel view synthesis confirm that our methodsimprove significantly upon prior art. Code will be released.|2025|CVPR|https://github.com/facebookresearch/mvdust3r|oral|未复现|
|121|DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models|Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling|https://arxiv.org/pdf/2503.01774|Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3Dreconstruction and novel-view synthesis task. However, achieving photorealisticrendering from extreme novel viewpoints remains challenging, as artifactspersist across representations. In this work, we introduce Difix3D+, a novelpipeline designed to enhance 3D reconstruction and novel-view synthesis throughsingle-step diffusion models. At the core of our approach is Difix, asingle-step image diffusion model trained to enhance and remove artifacts inrendered novel views caused by underconstrained regions of the 3Drepresentation. Difix serves two critical roles in our pipeline. First, it isused during the reconstruction phase to clean up pseudo-training views that arerendered from the reconstruction and then distilled back into 3D. This greatlyenhances underconstrained regions and improves the overall 3D representationquality. More importantly, Difix also acts as a neural enhancer duringinference, effectively removing residual artifacts arising from imperfect 3Dsupervision and the limited capacity of current reconstruction models. Difix3D+is a general solution, a single model compatible with both NeRF and 3DGSrepresentations, and it achieves an average 2× improvement in FID scoreover baselines while maintaining 3D consistency.|2025|CVPR|https://github.com/nv-tlabs/Difix3D|oral,Award Candidate|未复现|
|122|DIFFUSIONRENDERER: Neural Inverse and Forward Rendering with Video Diffusion Models|Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang|https://arxiv.org/pdf/2501.18590|Understanding and modeling lighting effects are fundamental tasks in computervision and graphics. Classic physically-based rendering (PBR) accuratelysimulates the light transport, but relies on precise scenerepresentations--explicit 3D geometry, high-quality material properties, andlighting conditions--that are often impractical to obtain in real-worldscenarios. Therefore, we introduce DiffusionRenderer, a neural approach thataddresses the dual problem of inverse and forward rendering within a holisticframework. Leveraging powerful video diffusion model priors, the inverserendering model accurately estimates G-buffers from real-world videos,providing an interface for image editing tasks, and training data for therendering model. Conversely, our rendering model generates photorealisticimages from G-buffers without explicit light transport simulation. Experimentsdemonstrate that DiffusionRenderer effectively approximates inverse andforwards rendering, consistently outperforming the state-of-the-art. Our modelenables practical applications from a single video input--including relighting,material editing, and realistic object insertion.|2025|CVPR|https://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/|oral|未复现|
|123|OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation|Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao, Kaipeng Zhang|https://arxiv.org/pdf/2411.18499|Multimodal Large Language Models (MLLMs) have made significant strides invisual understanding and generation tasks. However, generating interleavedimage-text content remains a challenge, which requires integrated multimodalunderstanding and generation abilities. While the progress in unified modelsoffers new solutions, existing benchmarks are insufficient for evaluating thesemethods due to limitations in data size and diversity. To bridge this gap, weintroduce OpenING, a comprehensive benchmark comprising 5,400 high-qualityhuman-annotated instances across 56 real-world tasks. OpenING covers diversedaily scenarios such as travel guide, design, and brainstorming, offering arobust platform for challenging interleaved generation methods. In addition, wepresent IntJudge, a judge model for evaluating open-ended multimodal generationmethods. Trained with a novel data pipeline, our IntJudge achieves an agreementrate of 82.42% with human judgments, outperforming GPT-based evaluators by11.34%. Extensive experiments on OpenING reveal that current interleavedgeneration methods still have substantial room for improvement. Key findings oninterleaved image-text generation are further presented to guide thedevelopment of next-generation models.|2025|CVPR|https://github.com/LanceZPF/OpenING|oral|未复现|
|124|RandAR: Decoder-only Autoregressive Visual Generation in Random Orders|Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang|https://arxiv.org/pdf/2412.01827|We introduce RandAR, a decoder-only visual autoregressive (AR) model capableof generating images in arbitrary token orders. Unlike previous decoder-only ARmodels that rely on a predefined generation order, RandAR removes thisinductive bias, unlocking new capabilities in decoder-only generation. Ouressential design enables random order by inserting a "position instructiontoken" before each image token to be predicted, representing the spatiallocation of the next image token. Trained on randomly permuted token sequences-- a more challenging task than fixed-order generation, RandAR achievescomparable performance to its conventional raster-order counterpart. Moreimportantly, decoder-only transformers trained from random orders acquire newcapabilities. For the efficiency bottleneck of AR models, RandAR adoptsparallel decoding with KV-Cache at inference time, enjoying 2.5x accelerationwithout sacrificing generation quality. Additionally, RandAR supportsinpainting, outpainting and resolution extrapolation in a zero-shot manner. Wehope RandAR inspires new directions for decoder-only visual generation modelsand broadens their applications across diverse scenarios. |2025|CVPR|https://github.com/ziqipang/RandAR|oral|未复现|
|125|AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea|Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, Yueting Zhuang|https://arxiv.org/pdf/2411.15738|Instruction-based image editing aims to modify specific image elements withnatural language instructions. However, current models in this domain oftenstruggle to accurately execute complex user instructions, as they are trainedon low-quality data with limited editing types. We present AnyEdit, acomprehensive multi-modal instruction editing dataset, comprising 2.5 millionhigh-quality editing pairs spanning over 20 editing types and five domains. Weensure the diversity and quality of the AnyEdit collection through threeaspects: initial data diversity, adaptive editing process, and automatedselection of editing results. Using the dataset, we further train a novelAnyEdit Stable Diffusion with task-aware routing and learnable task embeddingfor unified image editing. Comprehensive experiments on three benchmarkdatasets show that AnyEdit consistently boosts the performance ofdiffusion-based editing models. This presents prospects for developinginstruction-driven image editing models that support human creativity.|2025|CVPR|https://github.com/DCDmllm/AnyEdit|oral|未复现|
|126|VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection|Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, Si Liu|https://arxiv.org/pdf/2411.14794|The advancement of Large Vision Language Models (LVLMs) has significantlyimproved multimodal understanding, yet challenges remain in video reasoningtasks due to the scarcity of high-quality, large-scale datasets. Existing videoquestion-answering (VideoQA) datasets often rely on costly manual annotationswith insufficient granularity or automatic construction methods with redundantframe-by-frame analysis, limiting their scalability and effectiveness forcomplex reasoning. To address these challenges, we introduce VideoEspresso, anovel dataset that features VideoQA pairs preserving essential spatial detailsand temporal coherence, along with multimodal annotations of intermediatereasoning steps. Our construction pipeline employs a semantic-aware method toreduce redundancy, followed by generating QA pairs using GPT-4o. We furtherdevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,guiding GPT-4o in extracting logical relationships from QA pairs and videocontent. To exploit the potential of high-quality VideoQA pairs, we propose aHybrid LVLMs Collaboration framework, featuring a Frame Selector and atwo-stage instruction fine-tuned reasoning LVLM. This framework adaptivelyselects core frames and performs CoT reasoning using multimodal evidence.Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, ourmethod outperforms existing baselines on most tasks, demonstrating superiorvideo reasoning capabilities.|2025|CVPR|https://github.com/hshjerry/VideoEspresso|oral|未复现|
|127|SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images|Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang|https://arxiv.org/pdf/2410.01768|Remote sensing image plays an irreplaceable role in fields such asagriculture, water resources, military, and disaster relief. Pixel-levelinterpretation is a critical aspect of remote sensing image applications;however, a prevalent limitation remains the need for extensive manualannotation. For this, we try to introduce open-vocabulary semantic segmentation(OVSS) into the remote sensing context. However, due to the sensitivity ofremote sensing images to low-resolution features, distorted target shapes andill-fitting boundaries are exhibited in the prediction mask. To tackle thisissue, we propose a simple and general upsampler, SimFeatUp, to restore lostspatial information in deep features in a training-free style. Further, basedon the observation of the abnormal response of local patch tokens to [CLS]token in CLIP, we propose to execute a straightforward subtraction operation toalleviate the global bias in patch tokens. Extensive experiments are conductedon 17 remote sensing datasets spanning semantic segmentation, buildingextraction, road detection, and flood detection tasks. Our method achieves anaverage of 5.8%, 8.2%, 4.0%, and 15.3% improvement over state-of-the-artmethods on 4 tasks. |2025|CVPR|https://github.com/likyoo/SegEarth-OV|oral|未复现|
|128|Minority-Focused Text-to-Image Generation via Prompt Optimization|Soobin Um, Jong Chul Ye|https://arxiv.org/pdf/2410.07838|We investigate the generation of minority samples using pretrainedtext-to-image (T2I) latent diffusion models. Minority instances, in the contextof T2I generation, can be defined as ones living on low-density regions oftext-conditional data distributions. They are valuable for various applicationsof modern T2I generators, such as data augmentation and creative AI.Unfortunately, existing pretrained T2I diffusion models primarily focus onhigh-density regions, largely due to the influence of guided samplers (likeCFG) that are essential for high-quality generation. To address this, wepresent a novel framework to counter the high-density-focus of T2I diffusionmodels. Specifically, we first develop an online prompt optimization frameworkthat encourages emergence of desired properties during inference whilepreserving semantic contents of user-provided prompts. We subsequently tailorthis generic prompt optimizer into a specialized solver that promotesgeneration of minority features by incorporating a carefully-crafted likelihoodobjective. Extensive experiments conducted across various types of T2I modelsdemonstrate that our approach significantly enhances the capability to producehigh-quality minority instances compared to existing samplers.|2025|CVPR|https://github.com/soobin-um/MinorityPrompt|oral|未复现|
|129|Autoregressive Distillation of Diffusion Transformers|Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar Schönfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu|https://arxiv.org/pdf/2504.11295|Diffusion models with transformer architectures have demonstrated promisingcapabilities in generating high-fidelity images and scalability for highresolution. However, iterative sampling process required for synthesis is veryresource-intensive. A line of work has focused on distilling solutions toprobability flow ODEs into few-step student models. Nevertheless, existingmethods have been limited by their reliance on the most recent denoised samplesas input, rendering them susceptible to exposure bias. To address thislimitation, we propose AutoRegressive Distillation (ARD), a novel approach thatleverages the historical trajectory of the ODE to predict future steps. ARDoffers two key benefits: 1) it mitigates exposure bias by utilizing a predictedhistorical trajectory that is less susceptible to accumulated errors, and 2) itleverages the previous history of the ODE trajectory as a more effective sourceof coarse-grained information. ARD modifies the teacher transformerarchitecture by adding token-wise time embedding to mark each input from thetrajectory history and employs a block-wise causal attention mask for training.Furthermore, incorporating historical inputs only in lower transformer layersenhances performance and efficiency. We validate the effectiveness of ARD in aclass-conditioned generation on ImageNet and T2I synthesis. Our model achievesa $5\times$ reduction in FID degradation compared to the baseline methods whilerequiring only 1.1\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available1024p text-to-image distilled models in prompt adherence score with a minimaldrop in FID compared to the teacher. |2025|CVPR|https://github.com/alsdudrla10/ARD|oral|未复现|
|130|Molmo and PixMo:Open Weights and Open Data for State-of-the-Art Vision-Language Mo|Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi|https://arxiv.org/pdf/2409.17146|Today's most advanced vision-language models (VLMs) remain proprietary. Thestrongest open-weight models rely heavily on synthetic data from proprietaryVLMs to achieve good performance, effectively distilling these closed VLMs intoopen ones. As a result, the community has been missing foundational knowledgeabout how to build performant VLMs from scratch. We present Molmo, a new familyof VLMs that are state-of-the-art in their class of openness. Our keycontribution is a collection of new datasets called PixMo, including a datasetof highly detailed image captions for pre-training, a free-form image Q&Adataset for fine-tuning, and an innovative 2D pointing dataset, all collectedwithout the use of external VLMs. The success of our approach relies on carefulmodeling choices, a well-tuned training pipeline, and, most critically, thequality of our newly collected datasets. Our best-in-class 72B model not onlyoutperforms others in the class of open weight and data models, but alsooutperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks andon a large human evaluation. Our model weights, new datasets, and source codeare available at https://molmo.allenai.org/blog.|2025|CVPR|https://github.com/allenai/molmo|oral|未复现|
|131|Continuous 3D Perception Model with Persistent State|Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa|https://arxiv.org/pdf/2501.12387|We present a unified framework capable of solving a broad range of 3D tasks.Our approach features a stateful recurrent model that continuously updates itsstate representation with each new observation. Given a stream of images, thisevolving state can be used to generate metric-scale pointmaps (per-pixel 3Dpoints) for each new input in an online fashion. These pointmaps reside withina common coordinate system, and can be accumulated into a coherent, dense scenereconstruction that updates as new images arrive. Our model, called CUT3R(Continuous Updating Transformer for 3D Reconstruction), captures rich priorsof real-world scenes: not only can it predict accurate pointmaps from imageobservations, but it can also infer unseen regions of the scene by probing atvirtual, unobserved views. Our method is simple yet highly flexible, naturallyaccepting varying lengths of images that may be either video streams orunordered photo collections, containing both static and dynamic content. Weevaluate our method on various 3D/4D tasks and demonstrate competitive orstate-of-the-art performance in each. |2025|CVPR|https://github.com/CUT3R/CUT3R|oral|未复现|
|132|Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content|Zicheng Zhang, Tengchuan Kou, Shushi Wang, Chunyi Li, Wei Sun, Wei Wang, Xiaoyu Li, Zongyu Wang, Xuezhi Cao, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai|https://arxiv.org/pdf/2503.02357|Evaluating text-to-vision content hinges on two crucial aspects: visualquality and alignment. While significant progress has been made in developingobjective models to assess these dimensions, the performance of such modelsheavily relies on the scale and quality of human annotations. According toScaling Law, increasing the number of human-labeled instances follows apredictable pattern that enhances the performance of evaluation models.Therefore, we introduce a comprehensive dataset designed to Evaluate Visualquality and Alignment Level for text-to-vision content (Q-EVAL-100K), featuringthe largest collection of human-labeled Mean Opinion Scores (MOS) for thementioned two aspects. The Q-EVAL-100K dataset encompasses both text-to-imageand text-to-video models, with 960K human annotations specifically focused onvisual quality and alignment for 100K instances (60K images and 40K videos).Leveraging this dataset with context prompt, we propose Q-Eval-Score, a unifiedmodel capable of evaluating both visual quality and alignment with specialimprovements for handling long-text prompt alignment. Experimental resultsindicate that the proposed Q-Eval-Score achieves superior performance on bothvisual quality and alignment, with strong generalization capabilities acrossother benchmarks. These findings highlight the significant value of theQ-EVAL-100K dataset.|2025|CVPR|https://github.com/zzc-1998/Q-Eval|oral|未复现|
|133|Video-XL:Extra-Long Vision Language Model for Hour-Scale Video Understanding|Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao|https://arxiv.org/pdf/2409.14485|Long video understanding poses a significant challenge for currentMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrainedby their limited context lengths and the substantial costs while processinglong videos. Although several existing methods attempt to reduce visual tokens,their strategies encounter severe bottleneck, restricting MLLMs' ability toperceive fine-grained visual details. In this work, we propose Video-XL, anovel approach that leverages MLLMs' inherent key-value (KV) sparsificationcapacity to condense the visual input. Specifically, we introduce a new specialtoken, the Visual Summarization Token (VST), for each interval of the video,which summarizes the visual information within the interval as its associatedKV. The VST module is trained by instruction fine-tuning, where two optimizingstrategies are offered. 1.Curriculum learning, where VST learns to make small(easy) and large compression (hard) progressively. 2. Composite data curation,which integrates single-image, multi-image, and synthetic data to overcome thescarcity of long-video instruction data. The compression quality is furtherimproved by dynamic compression, which customizes compression granularity basedon the information density of different video intervals. Video-XL'seffectiveness is verified from three aspects. First, it achieves a superiorlong-video understanding capability, outperforming state-of-the-art models ofcomparable sizes across multiple popular benchmarks. Second, it effectivelypreserves video information, with minimal compression loss even at 16xcompression ratio. Third, it realizes outstanding cost-effectiveness, enablinghigh-quality processing of thousands of frames on a single A100 GPU.|2025|CVPR|https://github.com/VectorSpaceLab/Video-XL|oral|未复现|
|134|Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise|Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu|https://arxiv.org/pdf/2501.08331|Generative modeling aims to transform random noise into structured outputs.In this work, we enhance video diffusion models by allowing motion control viastructured latent noise sampling. This is achieved by just a change in data: wepre-process training videos to yield structured noise. Consequently, our methodis agnostic to diffusion model design, requiring no changes to modelarchitectures or training pipelines. Specifically, we propose a novel noisewarping algorithm, fast enough to run in real time, that replaces randomtemporal Gaussianity with correlated warped noise derived from optical flowfields, while preserving the spatial Gaussianity. The efficiency of ouralgorithm enables us to fine-tune modern video diffusion base models usingwarped noise with minimal overhead, and provide a one-stop solution for a widerange of user-friendly motion control: local object motion control, globalcamera movement control, and motion transfer. The harmonization betweentemporal coherence and spatial Gaussianity in our warped noise leads toeffective motion control while maintaining per-frame pixel quality. Extensiveexperiments and user studies demonstrate the advantages of our method, makingit a robust and scalable approach for controlling motion in video diffusionmodels. Video results are available on our webpage:https://eyeline-research.github.io/Go-with-the-Flow. |2025|CVPR|https://github.com/Eyeline-Research/Go-with-the-Flow|oral|未复现|
|135|CleanDIFT: Diffusion Features without Noise|Nick Stracke, Stefan Andreas Baumann, Kolja Bauer, Frank Fundel, Björn Ommer|https://arxiv.org/pdf/2412.03439|Internal features from large-scale pre-trained diffusion models have recentlybeen established as powerful semantic descriptors for a wide range ofdownstream tasks. Works that use these features generally need to add noise toimages before passing them through the model to obtain the semantic features,as the models do not offer the most useful features when given images withlittle to no noise. We show that this noise has a critical impact on theusefulness of these features that cannot be remedied by ensembling withdifferent random noises. We address this issue by introducing a lightweight,unsupervised fine-tuning method that enables diffusion backbones to providehigh-quality, noise-free semantic features. We show that these features readilyoutperform previous diffusion features by a wide margin in a wide variety ofextraction setups and downstream tasks, offering better performance than evenensemble-based methods at a fraction of the cost.|2025|CVPR|https://github.com/CompVis/cleandift|oral|未复现|
|136|CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner|Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long|https://arxiv.org/pdf/2405.14979|We present a novel generative 3D modeling system, coined CraftsMan, which cangenerate high-fidelity 3D geometries with highly varied shapes, regular meshtopologies, and detailed surfaces, and, notably, allows for refining thegeometry in an interactive manner. Despite the significant advancements in 3Dgeneration, existing methods still struggle with lengthy optimizationprocesses, irregular mesh topologies, noisy surfaces, and difficulties inaccommodating user edits, consequently impeding their widespread adoption andimplementation in 3D modeling software. Our work is inspired by the craftsman,who usually roughs out the holistic figure of the work first and elaborates thesurface details subsequently. Specifically, we employ a 3D native diffusionmodel, which operates on latent space learned from latent set-based 3Drepresentations, to generate coarse geometries with regular mesh topology inseconds. In particular, this process takes as input a text prompt or areference image and leverages a powerful multi-view (MV) diffusion model togenerate multiple views of the coarse geometry, which are fed into ourMV-conditioned 3D diffusion model for generating the 3D geometry, significantlyimproving robustness and generalizability. Following that, a normal-basedgeometry refiner is used to significantly enhance the surface details. Thisrefinement can be performed automatically, or interactively with user-suppliededits. Extensive experiments demonstrate that our method achieves high efficacyin producing superior-quality 3D assets compared to existing methods. |2025|CVPR|https://github.com/wyysf-98/CraftsMan3D|oral|未复现|
|137|DreamRelation: Bridging Customization and Relation Generation|Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li|https://arxiv.org/pdf/2410.23280|Customized image generation is essential for creating personalized contentbased on user prompts, allowing large-scale text-to-image diffusion models tomore effectively meet individual needs. However, existing models often neglectthe relationships between customized objects in generated images. In contrast,this work addresses this gap by focusing on relation-aware customized imagegeneration, which seeks to preserve the identities from image prompts whilemaintaining the relationship specified in text prompts. Specifically, weintroduce DreamRelation, a framework that disentangles identity and relationlearning using a carefully curated dataset. Our training data consists ofrelation-specific images, independent object images containing identityinformation, and text prompts to guide relation generation. Then, we proposetwo key modules to tackle the two main challenges: generating accurate andnatural relationships, especially when significant pose adjustments arerequired, and avoiding object confusion in cases of overlap. First, weintroduce a keypoint matching loss that effectively guides the model inadjusting object poses closely tied to their relationships. Second, weincorporate local features of the image prompts to better distinguish betweenobjects, preventing confusion in overlapping cases. Extensive results on ourproposed benchmarks demonstrate the superiority of DreamRelation in generatingprecise relations while preserving object identities across a diverse set ofobjects and relationships.|2025|CVPR|https://github.com/Shi-qingyu/DreamRelation|oral|未复现|
|138|Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens|Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, Hanwang Zhang|https://arxiv.org/pdf/2504.14666|Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unifyvisual comprehension and generation by combining LLM and diffusion models, thestate-of-the-art in each task, respectively. Existing approaches rely onspatial visual tokens, where image patches are encoded and arranged accordingto a spatial order (e.g., raster scan). However, we show that spatial tokenslack the recursive structure inherent to languages, hence form an impossiblelanguage for LLM to master. In this paper, we build a proper visual language byleveraging diffusion timesteps to learn discrete, recursive visual tokens. Ourproposed tokens recursively compensate for the progressive attribute loss innoisy images as timesteps increase, enabling the diffusion model to reconstructthe original image at any timestep. This approach allows us to effectivelyintegrate the strengths of LLMs in autoregressive reasoning and diffusionmodels in precise image generation, achieving seamless multimodal comprehensionand generation within a unified framework. Extensive experiments show that weachieve superior performance for multimodal comprehension and generationsimultaneously compared with other MLLMs. |2025|CVPR|https://github.com/selftok-team/SelftokTokenizer/|oral|未复现|
|139|3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion|Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, Liang Pan, Dahua Lin, Ziwei Liu|https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html|The increasing demand for high-quality 3D assets across various industries necessitates efficient and automated 3D content creation. Despite recent advancements in 3D generative models, existing methods still face challenges with optimization speed, geometric fidelity, and the lack of assets for physically based rendering (PBR). In this paper, we introduce 3DTopia-XL, a scalable native 3D generative model designed to overcome these limitations. 3DTopia-XL leverages a novel primitive-based 3D representation, PrimX, which encodes detailed shape, albedo, and material field into a compact tensorial format, facilitating the modeling of high-resolution geometry with PBR assets. On top of the novel representation, we propose a generative framework based on Diffusion Transformer (DiT), which comprises 1) Primitive Patch Compression, 2) and Latent Primitive Diffusion. 3DTopia-XL learns to generate high-quality 3D assets from textual or visual inputs. Extensive qualitative and quantitative experiments are conducted to demonstrate that 3DTopia-XL significantly outperforms existing methods in generating high-quality 3D assets with fine-grained textures and materials, efficiently bridging the quality gap between generative models and real-world applications.|2025|CVPR|https://github.com/3DTopia/3DTopia-XL|highlight|未复现|
|140|AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities|Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu|https://arxiv.org/abs/2412.14123|Geospatial models must adapt to the diversity of Earth observation data interms of resolutions, scales, and modalities. However, existing approachesexpect fixed input configurations, which limits their practical applicability.We propose AnySat, a multimodal model based on joint embedding predictivearchitecture (JEPA) and scale-adaptive spatial encoders, allowing us to train asingle model on highly heterogeneous data in a self-supervised manner. Todemonstrate the advantages of this unified approach, we compile GeoPlex, acollection of 5 multimodal datasets with varying characteristics and $11$distinct sensors. We then train a single powerful model on these diversedatasets simultaneously. Once fine-tuned or probed, we reach state-of-the-artresults on the test sets of GeoPlex and for 6 external datasets across variousenvironment monitoring tasks: land cover mapping, tree species identification,crop type classification, change detection, climate type classification, andsegmentation of flood, burn scar, and deforestation. |2025|CVPR|https://github.com/gastruc/AnySat|highlight|未复现|
|141|Cross-modal Causal Relation Alignment for Video Question Grounding|Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin|https://arxiv.org/abs/2503.07635|Video question grounding (VideoQG) requires models to answer the questionsand simultaneously infer the relevant video segments to support the answers.However, existing VideoQG methods usually suffer from spurious cross-modalcorrelations, leading to a failure to identify the dominant visual scenes thatalign with the intended question. Moreover, vision-language models exhibitunfaithful generalization performance and lack robustness on challengingdownstream tasks such as VideoQG. In this work, we propose a novel VideoQGframework named Cross-modal Causal Relation Alignment (CRA), to eliminatespurious correlations and improve the causal consistency betweenquestion-answering and video temporal grounding. Our CRA involves threeessential components: i) Gaussian Smoothing Grounding (GSG) module forestimating the time interval via cross-modal attention, which is de-noised byan adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances theperformance of weakly supervised VideoQG by leveraging bidirectionalcontrastive learning between estimated video segments and QA features, iii)Explicit Causal Intervention (ECI) module for multimodal deconfounding, whichinvolves front-door intervention for vision and back-door intervention forlanguage. Extensive experiments on two VideoQG datasets demonstrate thesuperiority of our CRA in discovering visually grounded content and achievingrobust question reasoning. |2025|CVPR|https://github.com/WissingChen/CRA-GQA|highlight|未复现|
|142|DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos|Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan|https://arxiv.org/abs/2409.02095|Estimating video depth in open-world scenarios is challenging due to thediversity of videos in appearance, content motion, camera movement, and length.We present DepthCrafter, an innovative method for generating temporallyconsistent long depth sequences with intricate details for open-world videos,without requiring any supplementary information such as camera poses or opticalflow. The generalization ability to open-world videos is achieved by trainingthe video-to-depth model from a pre-trained image-to-video diffusion model,through our meticulously designed three-stage training strategy. Our trainingapproach enables the model to generate depth sequences with variable lengths atone time, up to 110 frames, and harvest both precise depth details and richcontent diversity from realistic and synthetic datasets. We also propose aninference strategy that can process extremely long videos through segment-wiseestimation and seamless stitching. Comprehensive evaluations on multipledatasets reveal that DepthCrafter achieves state-of-the-art performance inopen-world video depth estimation under zero-shot settings. Furthermore,DepthCrafter facilitates various downstream applications, including depth-basedvisual effects and conditional video generation.|2025|CVPR|https://github.com/Tencent/DepthCrafter|highlight|未复现|
|143|DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery|Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang|https://arxiv.org/abs/2503.16964|Drones have become essential tools for reconstructing wild scenes due totheir outstanding maneuverability. Recent advances in radiance field methodshave achieved remarkable rendering quality, providing a new avenue for 3Dreconstruction from drone imagery. However, dynamic distractors in wildenvironments challenge the static scene assumption in radiance fields, whilelimited view constraints hinder the accurate capture of underlying scenegeometry. To address these challenges, we introduce DroneSplat, a novelframework designed for robust 3D reconstruction from in-the-wild drone imagery.Our method adaptively adjusts masking thresholds by integrating local-globalsegmentation heuristics with statistical approaches, enabling preciseidentification and elimination of dynamic distractors in static scenes. Weenhance 3D Gaussian Splatting with multi-view stereo predictions and avoxel-guided optimization strategy, supporting high-quality rendering underlimited view constraints. For comprehensive evaluation, we provide adrone-captured 3D reconstruction dataset encompassing both dynamic and staticscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGSand NeRF baselines in handling in-the-wild drone imagery.|2025|CVPR|https://github.com/BITyia/DroneSplat|highlight|未复现|
|144|Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility|Yidi Li, Jun Xiao, Zhengda Lu, Yiqun Wang, Haiyong Jiang|https://arxiv.org/abs/2505.21377|This work presents a novel text-to-vector graphics generation approach,Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detailoptimization, and view-dependent occlusion awareness. Our approach is adual-branch optimization framework, consisting of an auxiliary 3D GaussianSplatting optimization branch and a 3D vector graphics optimization branch. Theintroduced 3DGS branch can bridge the domain gaps between text prompts andvector graphics with more consistent guidance. Moreover, 3DGS allows forprogressive detail control by scheduling classifier-free guidance, facilitatingguiding vector graphics with coarse shapes at the initial stages and finerdetails at later stages. We also improve the view-dependent occlusions bydevising a visibility-awareness rendering module. Extensive results on 3Dsketches and 3D iconographies, demonstrate the superiority of the method ondifferent abstraction levels of details, cross-view consistency, andocclusion-aware stroke culling.|2025|CVPR|https://github.com/chenxinl/Dream3DVG|highlight|未复现|
|145|Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think|Jie Tian, Xiaoye Qu, Zhenyi Lu, Wei Wei, Sichen Liu, Yu Cheng|https://arxiv.org/abs/2503.00948|Image-to-Video (I2V) generation aims to synthesize a video clip according toa given image and condition (e.g., text). The key challenge of this task liesin simultaneously generating natural motions while preserving the originalappearance of the images. However, current I2V diffusion models (I2V-DMs) oftenproduce videos with limited motion degrees or exhibit uncontrollable motionthat conflicts with the textual condition. To address these limitations, wepropose a novel Extrapolating and Decoupling framework, which introduces modelmerging techniques to the I2V domain for the first time. Specifically, ourframework consists of three separate stages: (1) Starting with a base I2V-DM,we explicitly inject the textual condition into the temporal module using alightweight, learnable adapter and fine-tune the integrated model to improvemotion controllability. (2) We introduce a training-free extrapolation strategyto amplify the dynamic range of the motion, effectively reversing thefine-tuning process to enhance the motion degree significantly. (3) With theabove two-stage models excelling in motion controllability and degree, wedecouple the relevant parameters associated with each type of motion abilityand inject them into the base I2V-DM. Since the I2V-DM handles different levelsof motion controllability and dynamics at various denoising time steps, weadjust the motion-aware parameters accordingly over time. Extensive qualitativeand quantitative experiments have been conducted to demonstrate the superiorityof our framework over existing methods.|2025|CVPR|https://github.com/Chuge0335/EDG|highlight|未复现|
|146|ETAP: Event-based Tracking of Any Point|Friedhelm Hamann, Daniel Gehrig, Filbert Febryanto, Kostas Daniilidis, Guillermo Gallego|https://arxiv.org/abs/2412.00133|Tracking any point (TAP) recently shifted the motion estimation paradigm fromfocusing on individual salient points with local templates to trackingarbitrary points with global image contexts. However, while research has mostlyfocused on driving the accuracy of models in nominal settings, addressingscenarios with difficult lighting conditions and high-speed motions remains outof reach due to the limitations of the sensor. This work addresses thischallenge with the first event camera-based TAP method. It leverages the hightemporal resolution and high dynamic range of event cameras for robusthigh-speed tracking, and the global contexts in TAP methods to handleasynchronous and sparse event measurements. We further extend the TAP frameworkto handle event feature variations induced by motion -- thereby addressing anopen challenge in purely event-based tracking -- with a novel feature-alignmentloss which ensures the learning of motion-robust features. Our method istrained with data from a new data generation pipeline and systematicallyablated across all design decisions. Our method shows strong cross-datasetgeneralization and performs 136% better on the average Jaccard metric than thebaselines. Moreover, on an established feature tracking benchmark, it achievesa 20% improvement over the previous best event-only method and even surpassesthe previous best events-and-frames method by 4.1%. |2025|CVPR|https://github.com/tub-rip/ETAP|highlight|未复现|
|147|Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality|Liyan Chen, Gregory P. Meyer, Zaiwei Zhang, Eric M. Wolff, Paul Vernaza|https://arxiv.org/abs/2412.16481|Recent efforts recognize the power of scale in 3D learning (e.g. PTv3) andattention mechanisms (e.g. FlashAttention). However, current point cloudbackbones fail to holistically unify geometric locality, attention mechanisms,and GPU architectures in one view. In this paper, we introduce Flash3DTransformer, which aligns geometric locality and GPU tiling through aprincipled locality mechanism based on Perfect Spatial Hashing (PSH). Thecommon alignment with GPU tiling naturally fuses our PSH locality mechanismwith FlashAttention at negligible extra cost. This mechanism affords flexibledesign choices throughout the backbone that result in superior downstream taskresults. Flash3D outperforms state-of-the-art PTv3 results on benchmarkdatasets, delivering a 2.25x speed increase and 2.4x memory efficiency boost.This efficiency enables scaling to wider attention scopes and larger modelswithout additional overhead. Such scaling allows Flash3D to achieve even highertask accuracies than PTv3 under the same compute budget.|2025|CVPR|https://github.com/liyanc/Flash3DTransformer|highlight|未复现|
|148|Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution|Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh|https://arxiv.org/abs/2412.15213|Diffusion models, and their generalization, flow matching, have had aremarkable impact on the field of media generation. Here, the conventionalapproach is to learn the complex mapping from a simple source distribution ofGaussian noise to the target media distribution. For cross-modal tasks such astext-to-image generation, this same mapping from noise to image is learntwhilst including a conditioning mechanism in the model. One key and thus farrelatively unexplored feature of flow matching is that, unlike Diffusionmodels, they are not constrained for the source distribution to be noise.Hence, in this paper, we propose a paradigm shift, and ask the question ofwhether we can instead train flow matching models to learn a direct mappingfrom the distribution of one modality to the distribution of another, thusobviating the need for both the noise distribution and conditioning mechanism.We present a general and simple framework, CrossFlow, for cross-modal flowmatching. We show the importance of applying Variational Encoders to the inputdata, and introduce a method to enable Classifier-free guidance. Surprisingly,for text-to-image, CrossFlow with a vanilla transformer without cross attentionslightly outperforms standard flow matching, and we show that it scales betterwith training steps and model size, while also allowing for interesting latentarithmetic which results in semantically meaningful edits in the output space.To demonstrate the generalizability of our approach, we also show thatCrossFlow is on par with or outperforms the state-of-the-art for variouscross-modal / intra-modal mapping tasks, viz. image captioning, depthestimation, and image super-resolution. We hope this paper contributes toaccelerating progress in cross-modal media generation.|2025|CVPR|https://github.com/qihao067/CrossFlow|highlight|未复现|
|149|Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis|Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, Stanley Chan|https://arxiv.org/abs/2412.02168|Image generation today can produce somewhat realistic images from textprompts. However, if one asks the generator to synthesize a specific camerasetting such as creating different fields of view using a 24mm lens versus a70mm lens, the generator will not be able to interpret and generatescene-consistent images. This limitation not only hinders the adoption ofgenerative tools in professional photography but also highlights the broaderchallenge of aligning data-driven models with real-world physical settings. Inthis paper, we introduce Generative Photography, a framework that allowscontrolling camera intrinsic settings during content generation. The coreinnovation of this work are the concepts of Dimensionality Lifting andDifferential Camera Intrinsics Learning, enabling smooth and consistenttransitions across different camera settings. Experimental results show thatour method produces significantly more scene-consistent photorealistic imagesthan state-of-the-art models such as Stable Diffusion 3 and FLUX. |2025|CVPR|https://github.com/pandayuanyu/generative-photography|highlight|未复现|
|150|Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation|Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin|https://arxiv.org/abs/2412.15211|Reconstructing the geometry and appearance of objects from photographs takenin different environments is difficult as the illumination and therefore theobject appearance vary across captured images. This is particularly challengingfor more specular objects whose appearance strongly depends on the viewingdirection. Some prior approaches model appearance variation across images usinga per-image embedding vector, while others use physically-based rendering torecover the materials and per-image illumination. Such approaches fail atfaithfully recovering view-dependent appearance given significant variation ininput illumination and tend to produce mostly diffuse results. We present anapproach that reconstructs objects from images taken under differentilluminations by first relighting the images under a single referenceillumination with a multiview relighting diffusion model and thenreconstructing the object's geometry and appearance with a radiance fieldarchitecture that is robust to the small remaining inconsistencies among therelit images. We validate our proposed approach on both synthetic and realdatasets and demonstrate that it greatly outperforms existing techniques atreconstructing high-fidelity appearance from images taken under extremeillumination variation. Moreover, our approach is particularly effective atrecovering view-dependent "shiny" appearance which cannot be reconstructed byprior methods.|2025|CVPR|https://relight-to-reconstruct.github.io/|highlight|未复现|
|151|Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction|Seungtae Nam, Xiangyu Sun, Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park|https://arxiv.org/abs/2412.06234|Generalized feed-forward Gaussian models have achieved significant progressin sparse-view 3D reconstruction by leveraging prior knowledge from largemulti-view datasets. However, these models often struggle to representhigh-frequency details due to the limited number of Gaussians. While thedensification strategy used in per-scene 3D Gaussian splatting (3D-GS)optimization can be adapted to the feed-forward models, it may not be ideallysuited for generalized scenarios. In this paper, we propose GenerativeDensification, an efficient and generalizable method to densify Gaussiansgenerated by feed-forward models. Unlike the 3D-GS densification strategy,which iteratively splits and clones raw Gaussian parameters, our methodup-samples feature representations from the feed-forward models and generatestheir corresponding fine Gaussians in a single forward pass, leveraging theembedded prior knowledge for enhanced generalization. Experimental results onboth object-level and scene-level reconstruction tasks demonstrate that ourmethod outperforms state-of-the-art approaches with comparable or smaller modelsizes, achieving notable improvements in representing fine details.|2025|CVPR|https://github.com/stnamjef/GenerativeDensification|highlight|未复现|
|152|GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control|Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao|https://arxiv.org/abs/2503.03751|We present GEN3C, a generative video model with precise Camera Control andtemporal 3D Consistency. Prior video models already generate realistic videos,but they tend to leverage little 3D information, leading to inconsistencies,such as objects popping in and out of existence. Camera control, if implementedat all, is imprecise, because camera parameters are mere inputs to the neuralnetwork which must then infer how the video depends on the camera. In contrast,GEN3C is guided by a 3D cache: point clouds obtained by predicting thepixel-wise depth of seed images or previously generated frames. When generatingthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache withthe new camera trajectory provided by the user. Crucially, this means thatGEN3C neither has to remember what it previously generated nor does it have toinfer the image structure from the camera pose. The model, instead, can focusall its generative power on previously unobserved regions, as well as advancingthe scene state to the next frame. Our results demonstrate more precise cameracontrol than prior work, as well as state-of-the-art results in sparse-viewnovel view synthesis, even in challenging settings such as driving scenes andmonocular dynamic video. Results are best viewed in videos. |2025|CVPR|https://github.com/nv-tlabs/GEN3C|highlight|未复现|
|153|Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity|Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, Nong Sang|https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html|How can we enable models to comprehend video anomalies occurring over varying temporal scales and contexts?Traditional Video Anomaly Understanding (VAU) methods focus on frame-level anomaly prediction, often missing the interpretability of complex and diverse real-world anomalies. Recent multimodal approaches leverage visual and textual data but lack hierarchical annotations that capture both short-term and long-term anomalies.To address this challenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical video anomaly understanding across any granularity. We develop a semi-automated annotation engine that efficiently scales high-quality annotations by combining manual video segmentation with recursive free-text annotation using large language models (LLMs). This results in over 70,000 multi-granular annotations organized at clip-level, event-level, and video-level segments.For efficient anomaly detection in long videos, we propose the Anomaly-focused Temporal Sampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to adaptively select frames based on anomaly scores, ensuring that the multimodal LLM concentrates on anomaly-rich regions, which significantly enhances both efficiency and accuracy.Extensive experiments demonstrate that our hierarchical instruction data markedly improves anomaly comprehension. The integrated ATS and visual-language model outperform traditional methods in processing long videos.Our benchmark and model will be publicly available.|2025|CVPR|https://github.com/pipixin321/HolmesVAU|highlight|未复现|
|154|HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation|Mehdi Zayene, Jannik Endres, Albias Havolli, Charles Corbière, Salim Cherkaoui, Alexandre Kontouli, Alexandre Alahi|https://arxiv.org/abs/2411.18335|Despite progress in stereo depth estimation, omnidirectional imaging remainsunderexplored, mainly due to the lack of appropriate data. We introduceHelvipad, a real-world dataset for omnidirectional stereo depth estimation,featuring 40K video frames from video sequences across diverse environments,including crowded indoor and outdoor scenes with various lighting conditions.Collected using two 360{\deg} cameras in a top-bottom setup and a LiDAR sensor,the dataset includes accurate depth and disparity labels by projecting 3D pointclouds onto equirectangular images. Additionally, we provide an augmentedtraining set with an increased label density by using depth completion. Webenchmark leading stereo depth estimation models for both standard andomnidirectional images. The results show that while recent stereo methodsperform decently, a challenge persists in accurately estimating depth inomnidirectional imaging. To address this, we introduce necessary adaptations tostereo models, leading to improved performance.|2025|CVPR|https://github.com/vita-epfl/Helvipad|highlight|未复现|
|155|ImViD: Immersive Volumetric Videos for Enhanced VR Engagement|Zhengxian Yang, Shi Pan, Shengqi Wang, Haoxiang Wang, Li Lin, Guanjun Li, Zhengqi Wen, Borong Lin, Jianhua Tao, Tao Yu|https://arxiv.org/abs/2503.14359|User engagement is greatly enhanced by fully immersive multi-modalexperiences that combine visual and auditory stimuli. Consequently, the nextfrontier in VR/AR technologies lies in immersive volumetric videos withcomplete scene capture, large 6-DoF interaction space, multi-modal feedback,and high resolution & frame-rate contents. To stimulate the reconstruction ofimmersive volumetric videos, we introduce ImViD, a multi-view, multi-modaldataset featuring complete space-oriented data capture and variousindoor/outdoor scenarios. Our capture rig supports multi-view video-audiocapture while on the move, a capability absent in existing datasets,significantly enhancing the completeness, flexibility, and efficiency of datacapture.  The captured multi-view videos (with synchronized audios) are in 5Kresolution at 60FPS, lasting from 1-5 minutes, and include richforeground-background elements, and complex dynamics. We benchmark existingmethods using our dataset and establish a base pipeline for constructingimmersive volumetric videos from multi-view audiovisual inputs for 6-DoFmulti-modal immersive VR experiences. The benchmark and the reconstruction andinteraction results demonstrate the effectiveness of our dataset and baselinemethod, which we believe will stimulate future research on immersive volumetricvideo production.|2025|CVPR|https://github.com/Metaverse-AI-Lab-THU/ImViD|highlight|未复现|
|156|Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning|Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu|https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html|Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks.|2025|CVPR|https://github.com/hanxunyu/Inst3D-LMM|highlight|未复现|
|157|Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene|Shengqiong Wu, Hao Fei, Jingkang Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, Tat-seng Chua|https://arxiv.org/abs/2503.15019|The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-everrepresentation for comprehensively modeling the dynamic 4D visual real world.Unfortunately, current pioneering 4D-PSG research can primarily suffer fromdata scarcity issues severely, as well as the resulting out-of-vocabularyproblems; also, the pipeline nature of the benchmark generation method can leadto suboptimal performance. To address these challenges, this paper investigatesa novel framework for 4D-PSG generation that leverages rich 2D visual sceneannotations to enhance 4D scene learning. First, we introduce a 4D LargeLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-endgeneration of 4D-PSG. A chained SG inference mechanism is further designed toexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensiveobject and relation labels iteratively. Most importantly, we propose a 2D-to-4Dvisual scene transfer learning framework, where a spatial-temporal scenetranscending strategy effectively transfers dimension-invariant features fromabundant 2D SG annotations to 4D scenes, effectively compensating for datascarcity in 4D-PSG. Extensive experiments on the benchmark data demonstratethat we strikingly outperform baseline models by a large margin, highlightingthe effectiveness of our method.|2025|CVPR|https://github.com/ChocoWu/PSG-4D-LLM|highlight|未复现|
|158|Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation|Xin Zhang, Robby T. Tan|https://arxiv.org/abs/2504.03193|Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gainedtraction in Domain Generalized Semantic Segmentation (DGSS) due to their stronggeneralization capabilities. However, existing DGSS methods often relyexclusively on either VFMs or VLMs, overlooking their complementary strengths.VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,CLIP) provide robust text alignment but struggle with coarse granularity.Despite their complementary strengths, effectively integrating VFMs and VLMswith attention mechanisms is challenging, as the increased patch tokenscomplicate long-sequence modeling. To address this, we propose MFuser, a novelMamba-based fusion framework that efficiently combines the strengths of VFMsand VLMs while maintaining linear scalability in sequence length. MFuserconsists of two key components: MVFuser, which acts as a co-adapter to jointlyfine-tune the two models by capturing both sequential and spatial dynamics; andMTEnhancer, a hybrid attention-Mamba module that refines text embeddings byincorporating image priors. Our approach achieves precise feature locality andstrong text alignment without incurring significant computational overhead.Extensive experiments demonstrate that MFuser significantly outperformsstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and71.87 mIoU on real-to-real benchmarks. The code is available athttps://github.com/devinxzhang/MFuser.|2025|CVPR|https://github.com/devinxzhang/MFuser|highlight|未复现|
|159|MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps|Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer Sumbül, Alexander Mathis, Devis Tuia|https://arxiv.org/abs/2503.18223|Monitoring wildlife is essential for ecology and ethology, especially inlight of the increasing human impact on ecosystems. Camera traps have emergedas habitat-centric sensors enabling the study of wildlife populations at scalewith minimal disturbance. However, the lack of annotated video datasets limitsthe development of powerful video understanding models needed to process thevast amount of fieldwork data collected. To advance research in wild animalbehavior monitoring we present MammAlps, a multimodal and multi-view dataset ofwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.MammAlps contains over 14 hours of video with audio, 2D segmentation maps and8.5 hours of individual tracks densely labeled for species and behavior. Basedon 6135 single animal clips, we propose the first hierarchical and multimodalanimal behavior recognition benchmark using audio, video and reference scenesegmentation maps as inputs. Furthermore, we also propose a secondecology-oriented benchmark aiming at identifying activities, species, number ofindividuals and meteorological conditions from 397 multi-view and long-termecological events, including false positive triggers. We advocate that bothtasks are complementary and contribute to bridging the gap between machinelearning and ecology. Code and data are available at:https://github.com/eceo-epfl/MammAlps|2025|CVPR|https://github.com/eceo-epfl/MammAlps|highlight|未复现|
|160|Matrix3D: Large Photogrammetry Model All-in-One|Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, Shiwei Li|https://arxiv.org/abs/2502.07685|We present Matrix3D, a unified model that performs several photogrammetrysubtasks, including pose estimation, depth prediction, and novel view synthesisusing just the same model. Matrix3D utilizes a multi-modal diffusiontransformer (DiT) to integrate transformations across several modalities, suchas images, camera parameters, and depth maps. The key to Matrix3D's large-scalemulti-modal training lies in the incorporation of a mask learning strategy.This enables full-modality model training even with partially complete data,such as bi-modality data of image-pose and image-depth pairs, thussignificantly increases the pool of available training data. Matrix3Ddemonstrates state-of-the-art performance in pose estimation and novel viewsynthesis tasks. Additionally, it offers fine-grained control throughmulti-round interactions, making it an innovative tool for 3D content creation.Project page: https://nju-3dv.github.io/projects/matrix3d.|2025|CVPR|https://github.com/apple/ml-matrix3d|highlight|未复现|
|161|MITracker: Multi-View Integration for Visual Object Tracking|Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang|https://arxiv.org/abs/2502.20111|Multi-view object tracking (MVOT) offers promising solutions to challengessuch as occlusion and target loss, which are common in traditional single-viewtracking. However, progress has been limited by the lack of comprehensivemulti-view datasets and effective cross-view integration methods. To overcomethese limitations, we compiled a Multi-View object Tracking (MVTrack) datasetof 234K high-quality annotated frames featuring 27 distinct objects acrossvarious scenes. In conjunction with this dataset, we introduce a novel MVOTmethod, Multi-View Integration Tracker (MITracker), to efficiently integratemulti-view object features and provide stable tracking outcomes. MITracker cantrack any object in video frames of arbitrary length from arbitrary viewpoints.The key advancements of our method over traditional single-view approaches comefrom two aspects: (1) MITracker transforms 2D image features into a 3D featurevolume and compresses it into a bird's eye view (BEV) plane, facilitatinginter-view information fusion; (2) we propose an attention mechanism thatleverages geometric information from fused 3D feature volume to refine thetracking results at each view. MITracker outperforms existing methods on theMVTrack and GMTD datasets, achieving state-of-the-art performance. The code andthe new dataset will be available athttps://mii-laboratory.github.io/MITracker/.|2025|CVPR|https://github.com/XuM007/MITracker|highlight|未复现|
|162|Open-Canopy: Towards Very High Resolution Forest Monitoring|Fajwel Fogel, Yohann Perron, Nikola Besic, Laurent Saint-André, Agnès Pellissier-Tanon, Martin Schwartz, Thomas Boudras, Ibrahim Fayad, Alexandre d'Aspremont, Loic Landrieu, Philippe Ciais|https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html|Estimating canopy height and its changes at meter resolution from satellite imagery is a significant challenge in computer vision with critical environmental applications. However, the lack of open-access datasets at this resolution hinders the reproducibility and evaluation of models. We introduce Open-Canopy, the first open-access, country-scale benchmark for very high-resolution (1.5 m) canopy height estimation, covering over 87,000 km2 across France with 1.5 m resolution satellite imagery and aerial LiDAR data. Additionally, we present Open-Canopy-, a benchmark for canopy height change detection between images from different years at tree level--a challenging task for current computer vision models. We evaluate state-of-the-art architectures on these benchmarks, highlighting significant challenges and opportunities for improvement. Our datasets and code are publicly available at https://github.com/fajwel/Open-Canopy.|2025|CVPR|https://github.com/fajwel/Open-Canopy|highlight|未复现|
|163|OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation|Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu|https://arxiv.org/abs/2412.00115|Recent advancements in visual generation technologies have markedly increasedthe scale and availability of video datasets, which are crucial for trainingeffective video generation models. However, a significant lack of high-quality,human-centric video datasets presents a challenge to progress in this field. Tobridge this gap, we introduce OpenHumanVid, a large-scale and high-qualityhuman-centric video dataset characterized by precise and detailed captions thatencompass both human appearance and motion states, along with supplementaryhuman motion conditions, including skeleton sequences and speech audio. Tovalidate the efficacy of this dataset and the associated training strategies,we propose an extension of existing classical diffusion transformerarchitectures and conduct further pretraining of our models on the proposeddataset. Our findings yield two critical insights: First, the incorporation ofa large-scale, high-quality dataset substantially enhances evaluation metricsfor generated human videos while preserving performance in general videogeneration tasks. Second, the effective alignment of text with humanappearance, human motion, and facial motion is essential for producinghigh-quality video outputs. Based on these insights and correspondingmethodologies, the straightforward extended network trained on the proposeddataset demonstrates an obvious improvement in the generation of human-centricvideos. Project page https://fudan-generative-vision.github.io/OpenHumanVid|2025|CVPR|https://github.com/fudan-generative-vision/OpenHumanVid|highlight|未复现|
|164|OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit|Benquan Wang, Ruyi An, Jin-Kyu So, Sergei Kurdiumov, Eng Aik Chan, Giorgio Adamo, Yuhan Peng, Yewen Li, Bo An|https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html|Optical imaging capable of resolving nanoscale features would revolutionize scientific research and engineering applications across biomedicine, smart manufacturing, and semiconductor quality control. However, due to the physical phenomenon of diffraction, the optical resolution is limited to approximately half the wavelength of light, which impedes the observation of subwavelength objects such as the native state coronavirus, typically smaller than 200 nm. Fortunately, deep learning methods have shown remarkable potential in uncovering underlying patterns within data, promising to overcome the diffraction limit by revealing the mapping pattern between diffraction images and their corresponding ground truth object images. However, the absence of suitable datasets has hindered progress in this field--collecting high-quality optical data of subwavelength objects is highly difficult as these objects are inherently invisible under conventional microscopy, making it impossible to perform standard visual calibration and drift correction. Therefore, we provide the first general optical imaging dataset based on the "building block" concept for challenging the diffraction limit. Drawing an analogy to modular construction principles, we construct a comprehensive optical imaging dataset comprising subwavelength fundamental elements, i.e., small square units that can be assembled into larger and more complex objects. We then frame the task as an image-to-image translation task and evaluate various vision methods. Experimental results validate our "building block" concept, demonstrating that models trained on basic square units can effectively generalize to realistic, more complex unseen objects. Most importantly, by highlighting this underexplored AI-for-science area and its potential, we aspire to advance optical science by fostering collaboration with the vision and machine learning communities.|2025|CVPR|https://github.com/Deep-See/OpticalNet|highlight|未复现|
|165|Optimizing for the Shortest Path in Denoising Diffusion Model|Ping Chen, Xingpeng Zhang, Zhaoxiang Liu, Huan Hu, Xiang Liu, Kai Wang, Min Wang, Yanlin Qian, Shiguo Lian|https://arxiv.org/abs/2503.03265|In this research, we propose a novel denoising diffusion model based onshortest-path modeling that optimizes residual propagation to enhance bothdenoising efficiency and quality. Drawing on Denoising Diffusion ImplicitModels (DDIM) and insights from graph theory, our model, termed the ShortestPath Diffusion Model (ShortDF), treats the denoising process as a shortest-pathproblem aimed at minimizing reconstruction error. By optimizing the initialresiduals, we improve the efficiency of the reverse diffusion process and thequality of the generated samples. Extensive experiments on multiple standardbenchmarks demonstrate that ShortDF significantly reduces diffusion time (orsteps) while enhancing the visual fidelity of generated samples compared toprior arts. This work, we suppose, paves the way for interactivediffusion-based applications and establishes a foundation for rapid datageneration. Code is available at https://github.com/UnicomAI/ShortDF.|2025|CVPR|https://github.com/UnicomAI/ShortDF|highlight|未复现|
|166|Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval|Yuanmin Tang, Xiaoting Qin, Jue Zhang, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Ling, Saravan Rajmohan, Dongmei Zhang, Qi Wu|https://arxiv.org/abs/2412.11077|Composed Image Retrieval (CIR) aims to retrieve target images that closelyresemble a reference image while integrating user-specified textualmodifications, thereby capturing user intent more precisely. Existingtraining-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process:they first generate a caption for the reference image and then use LargeLanguage Models for reasoning to obtain a target description. However, thesemethods suffer from missing critical visual details and limited reasoningcapabilities, leading to suboptimal retrieval performance. To address thesechallenges, we propose a novel, training-free one-stage method, One-StageReflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employsMultimodal Large Language Models to retain essential visual information in asingle-stage reasoning process, eliminating the information loss seen intwo-stage methods. Our Reflective Chain-of-Thought framework further improvesinterpretative accuracy by aligning manipulation intent with contextual cuesfrom reference images. OSrCIR achieves performance gains of 1.80% to 6.44% overexisting training-free methods across multiple tasks, setting newstate-of-the-art results in ZS-CIR and enhancing its utility in vision-languageapplications. Our code will be available athttps://github.com/Pter61/osrcir2024/.|2025|CVPR|https://github.com/Pter61/osrcir|highlight|未复现|
|167|SmartCLIP: Modular Vision-language Alignment with Identification Guarantees|Shaoan Xie, Lingjing Lingjing, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang|https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html|Contrastive Language-Image Pre-training (CLIP) \citep radford2021learning  has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning. However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard. On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts -- ultimately limiting its generalization on certain downstream tasks involving short prompts. In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only preserve cross-modal semantic information in its entirety but also disentangle visual representations to capture fine-grained textual concepts.  Building on this foundation, we introduce \ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory. The code is available at https://github.com/Mid-Push/SmartCLIP.|2025|CVPR|https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html|highlight|未复现|
|168|Structured 3D Latents for Scalable and Versatile 3D Generation|Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, Jiaolong Yang|https://arxiv.org/abs/2412.01506|We introduce a novel 3D generation method for versatile and high-quality 3Dasset creation. The cornerstone is a unified Structured LATent (SLAT)representation which allows decoding to different output formats, such asRadiance Fields, 3D Gaussians, and meshes. This is achieved by integrating asparsely-populated 3D grid with dense multiview visual features extracted froma powerful vision foundation model, comprehensively capturing both structural(geometry) and textural (appearance) information while maintaining flexibilityduring decoding. We employ rectified flow transformers tailored for SLAT as our3D generation models and train models with up to 2 billion parameters on alarge 3D asset dataset of 500K diverse objects. Our model generateshigh-quality results with text or image conditions, significantly surpassingexisting methods, including recent ones at similar scales. We showcase flexibleoutput format selection and local 3D editing capabilities which were notoffered by previous models. Code, model, and data will be released.|2025|CVPR|https://github.com/microsoft/TRELLIS|highlight|未复现|
|169|StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer|Ruojun Xu, Weijie Xi, Xiaodi Wang, Yongbo Mao, Zach Cheng|https://arxiv.org/abs/2501.11319|Training-free diffusion-based methods have achieved remarkable success instyle transfer, eliminating the need for extensive training or fine-tuning.However, due to the lack of targeted training for style information extractionand constraints on the content image layout, training-free methods often sufferfrom layout changes of original content and content leakage from style images.Through a series of experiments, we discovered that an effective startpoint inthe sampling stage significantly enhances the style transfer process. Based onthis discovery, we propose StyleSSP, which focuses on obtaining a betterstartpoint to address layout changes of original content and content leakagefrom style image. StyleSSP comprises two key components: (1) FrequencyManipulation: To improve content preservation, we reduce the low-frequencycomponents of the DDIM latent, allowing the sampling stage to pay moreattention to the layout of content images; and (2) Negative Guidance viaInversion: To mitigate the content leakage from style image, we employ negativeguidance in the inversion stage to ensure that the startpoint of the samplingstage is distanced from the content of style image. Experiments show thatStyleSSP surpasses previous training-free style transfer baselines,particularly in preserving original content and minimizing the content leakagefrom style image. Project page: https://github.com/bytedance/StyleSSP.|2025|CVPR|https://github.com/bytedance/StyleSSP|highlight|未复现|
|170|Towards Autonomous Micromobility through Scalable Urban Simulation|Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou|https://arxiv.org/abs/2505.00690|Micromobility, which utilizes lightweight mobile machines moving in urbanpublic spaces, such as delivery robots and mobility scooters, emerges as apromising alternative to vehicular mobility. Current micromobility dependsmostly on human manual operation (in-person or remote control), which raisessafety and efficiency concerns when navigating busy urban environments full ofunpredictable obstacles and pedestrians. Assisting humans with AI agents inmaneuvering micromobility devices presents a viable solution for enhancingsafety and efficiency. In this work, we present a scalable urban simulationsolution to advance autonomous micromobility. First, we build URBAN-SIM - ahigh-performance robot learning platform for large-scale training of embodiedagents in interactive urban scenes. URBAN-SIM contains three critical modules:Hierarchical Urban Generation pipeline, Interactive Dynamics Generationstrategy, and Asynchronous Scene Sampling scheme, to improve the diversity,realism, and efficiency of robot learning in simulation. Then, we proposeURBAN-BENCH - a suite of essential tasks and benchmarks to gauge variouscapabilities of the AI agents in achieving autonomous micromobility.URBAN-BENCH includes eight tasks based on three core skills of the agents:Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robotswith heterogeneous embodiments, such as the wheeled and legged robots, acrossthese tasks. Experiments on diverse terrains and urban structures reveal eachrobot's strengths and limitations.|2025|CVPR|https://github.com/metadriverse/urban-sim|highlight|未复现|
|171|UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models|Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao|https://arxiv.org/abs/2412.11441|Recent studies show that diffusion models (DMs) are vulnerable to backdoorattacks. Existing backdoor attacks impose unconcealed triggers (e.g., a graybox and eyeglasses) that contain evident patterns, rendering remarkable attackeffects yet easy detection upon human inspection and defensive algorithms.While it is possible to improve stealthiness by reducing the strength of thebackdoor, doing so can significantly compromise its generality andeffectiveness. In this paper, we propose UIBDiffusion, the universalimperceptible backdoor attack for diffusion models, which allows us to achievesuperior attack and generation performance while evading state-of-the-artdefenses. We propose a novel trigger generation approach based on universaladversarial perturbations (UAPs) and reveal that such perturbations, which areinitially devised for fooling pre-trained discriminative models, can be adaptedas potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion onmultiple types of DMs with different kinds of samplers across various datasetsand targets. Experimental results demonstrate that UIBDiffusion brings threeadvantages: 1) Universality, the imperceptible trigger is universal (i.e.,image and model agnostic) where a single trigger is effective to any images andall diffusion models with different samplers; 2) Utility, it achievescomparable generation quality (e.g., FID) and even better attack success rate(i.e., ASR) at low poison rates compared to the prior works; and 3)Undetectability, UIBDiffusion is plausible to human perception and can bypassElijah and TERD, the SOTA defenses against backdoors for DMs. We will releaseour backdoor triggers and code.|2025|CVPR|https://github.com/TheLaoLab/UIBDiffusion|highlight|未复现|
|172|UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning|Weiqi Yan, Lvhai Chen, Huaijia Kou, Shengchuan Zhang, Yan Zhang, Liujuan Cao|https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html|Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it doesn't need to rely on extensive pixel-level labels. Existing UCOD methods typically generate pseudo-labels using fixed strategies and train 1 x1 convolutional layers as a simple decoder, leading to low performance compared to fully-supervised methods. We emphasize two drawbacks in these approaches: 1). The model is prone to fitting incorrect knowledge due to the pseudo-label containing substantial noise. 2). The simple decoder fails to capture and learn the semantic features of camouflaged objects, especially for small-sized objects, due to the low-resolution pseudo-labels and severe confusion between foreground and background pixels. To this end, we propose a UCOD method with a teacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL, which contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial (DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines pseudo-labels generated by fixed strategies and the teacher model to prevent the model from overfitting incorrect knowledge while preserving the ability for self-correction; the DBA decoder takes adversarial learning of different segmentation objectives, guides the model to overcome the foreground-background confusion of camouflaged objects, and the Look-Twice mechanism mimics the human tendency to zoom in on camouflaged objects and performs secondary refinement on small-sized objects. Extensive experiments show that our method demonstrates outstanding performance, even surpassing some existing fully supervised methods. Our code will be released soon.|2025|CVPR|https://github.com/Heartfirey/UCOD-DPL|highlight|未复现|
|173|Video Depth Anything: Consistent Depth Estimation for Super-Long Videos|Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang|https://arxiv.org/abs/2501.12375|Depth Anything has achieved remarkable success in monocular depth estimationwith strong generalization ability. However, it suffers from temporalinconsistency in videos, hindering its practical applications. Various methodshave been proposed to alleviate this issue by leveraging video generationmodels or introducing priors from optical flow and camera poses. Nonetheless,these methods are only applicable to short videos (< 10 seconds) and require atrade-off between quality and computational efficiency. We propose Video DepthAnything for high-quality, consistent depth estimation in super-long videos(over several minutes) without sacrificing efficiency. We base our model onDepth Anything V2 and replace its head with an efficient spatial-temporal head.We design a straightforward yet effective temporal consistency loss byconstraining the temporal depth gradient, eliminating the need for additionalgeometric priors. The model is trained on a joint dataset of video depth andunlabeled images, similar to Depth Anything V2. Moreover, a novelkey-frame-based strategy is developed for long video inference. Experimentsshow that our model can be applied to arbitrarily long videos withoutcompromising quality, consistency, or generalization ability. Comprehensiveevaluations on multiple video benchmarks demonstrate that our approach sets anew state-of-the-art in zero-shot video depth estimation. We offer models ofdifferent scales to support a range of scenarios, with our smallest modelcapable of real-time performance at 30 FPS.|2025|CVPR|https://github.com/DepthAnything/Video-Depth-Anything|highlight|未复现|
|174|World-consistent Video Diffusion with Explicit 3D Modeling|Qihang Zhang, Shuangfei Zhai, Miguel Ángel Bautista Martin, Kevin Miao, Alexander Toshev, Joshua Susskind, Jiatao Gu|https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html|Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.|2025|CVPR|https://zqh0253.github.io/wvd/|highlight|未复现|
|175|Your ViT is Secretly an Image Segmentation Model|Tommie Kerssies, Niccolò Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, Daan de Geus|https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html|Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.|2025|CVPR|https://github.com/tue-mps/EoMT|highlight|未复现|
|176|WonderWorld: Interactive 3D Scene Generation from a Single Image|Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, Jiajun Wu|https://arxiv.org/abs/2406.09394|We present WonderWorld, a novel framework for interactive 3D scene generationthat enables users to interactively specify scene contents and layout and seethe created scenes in low latency. The major challenge lies in achieving fastgeneration of 3D scenes. Existing scene generation approaches fall short ofspeed as they often require (1) progressively generating many views and depthmaps, and (2) time-consuming optimization of the scene geometryrepresentations. We introduce the Fast Layered Gaussian Surfels (FLAGS) as ourscene representation and an algorithm to generate it from a single view. Ourapproach does not need multiple views, and it leverages a geometry-basedinitialization that significantly reduces optimization time. Another challengeis generating coherent geometry that allows all scenes to be connected. Weintroduce the guided depth diffusion that allows partial conditioning of depthestimation. WonderWorld generates connected and diverse 3D scenes in less than10 seconds on a single A6000 GPU, enabling real-time user interaction andexploration. We demonstrate the potential of WonderWorld for user-drivencontent creation and exploration in virtual environments. We release full codeand software for reproducibility. Project website:https://kovenyu.com/WonderWorld/.|2025|CVPR|https://github.com/KovenYu/WonderWorld|highlight|未复现|
|177|Relightable Gaussian Codec Avatars|Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam|https://arxiv.org/pdf/2312.03704|The fidelity of relighting is bounded by both geometry and appearancerepresentations. For geometry, both mesh and volumetric approaches havedifficulty modeling intricate structures like 3D hair geometry. For appearance,existing relighting models are limited in fidelity and often too slow to renderin real-time with high-resolution continuous environments. In this work, wepresent Relightable Gaussian Codec Avatars, a method to build high-fidelityrelightable head avatars that can be animated to generate novel expressions.Our geometry model based on 3D Gaussians can capture 3D-consistentsub-millimeter details such as hair strands and pores on dynamic facesequences. To support diverse materials of human heads such as the eyes, skin,and hair in a unified manner, we present a novel relightable appearance modelbased on learnable radiance transfer. Together with global illumination-awarespherical harmonics for the diffuse components, we achieve real-time relightingwith all-frequency reflections using spherical Gaussians. This appearance modelcan be efficiently relit under both point light and continuous illumination. Wefurther improve the fidelity of eye reflections and enable explicit gazecontrol by introducing relightable explicit eye models. Our method outperformsexisting approaches without compromising real-time performance. We alsodemonstrate real-time relighting of avatars on a tethered consumer VR headset,showcasing the efficiency and fidelity of our avatars.|2024|CVPR|https://github.com/facebookresearch/goliath|oral|未复现|
|178|Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following|Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, Jingren Zhou|https://arxiv.org/pdf/2311.17002|Existing text-to-image (T2I) diffusion models usually struggle ininterpreting complex prompts, especially those with quantity, object-attributebinding, and multi-subject descriptions. In this work, we introduce a semanticpanel as the middleware in decoding texts to images, supporting the generatorto better follow instructions. The panel is obtained through arranging thevisual concepts parsed from the input text by the aid of large language models,and then injected into the denoising network as a detailed control signal tocomplement the text condition. To facilitate text-to-panel learning, we come upwith a carefully designed semantic formatting protocol, accompanied by afully-automatic data preparation pipeline. Thanks to such a design, ourapproach, which we call Ranni, manages to enhance a pre-trained T2I generatorregarding its textual controllability. More importantly, the introduction ofthe generative middleware brings a more convenient form of interaction (i.e.,directly adjusting the elements in the panel or using language instructions)and further allows users to finely customize their generation, based on whichwe develop a practical system and showcase its potential in continuousgeneration and chatting-based editing. Our project page is athttps://ranni-t2i.github.io/Ranni.|2024|CVPR|https://github.com/ali-vilab/Ranni|oral|未复现|
|179|Rethinking Inductive Biases for Surface Normal Estimation|Gwangbin Bae, Andrew J. Davison|https://arxiv.org/pdf/2403.00712|Despite the growing demand for accurate surface normal estimation models,existing methods use general-purpose dense prediction models, adopting the sameinductive biases as other tasks. In this paper, we discuss the inductive biasesneeded for surface normal estimation and propose to (1) utilize the per-pixelray direction and (2) encode the relationship between neighboring surfacenormals by learning their relative rotation. The proposed method can generatecrisp - yet, piecewise smooth - predictions for challenging in-the-wild imagesof arbitrary resolution and aspect ratio. Compared to a recent ViT-basedstate-of-the-art model, our method shows a stronger generalization ability,despite being trained on an orders of magnitude smaller dataset. The code isavailable at https://github.com/baegwangbin/DSINE.|2024|CVPR|https://github.com/baegwangbin/DSINE|oral|未复现|
|180|PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness|Anh-Quan Cao, Angela Dai, Raoul de Charette|https://arxiv.org/abs/2312.02158|We propose the task of Panoptic Scene Completion (PSC) which extends therecently popular Semantic Scene Completion (SSC) task with instance-levelinformation to produce a richer understanding of the 3D scene. Our PSC proposalutilizes a hybrid mask-based technique on the non-empty voxels from sparsemulti-scale completions. Whereas the SSC literature overlooks uncertainty whichis critical for robotics applications, we instead propose an efficientensembling to estimate both voxel-wise and instance-wise uncertainties alongPSC. This is achieved by building on a multi-input multi-output (MIMO)strategy, while improving performance and yielding better uncertainty forlittle additional compute. Additionally, we introduce a technique to aggregatepermutation-invariant mask predictions. Our experiments demonstrate that ourmethod surpasses all baselines in both Panoptic Scene Completion anduncertainty estimation on three large-scale autonomous driving datasets. Ourcode and data are available at https://astra-vision.github.io/PaSCo .|2024|CVPR|https://github.com/astra-vision/PaSCo|oral|未复现|
|181|Transcriptomics-guided Slide Representation Learning in Computational Pathology|Guillaume Jaume, Lukas Oldenburg, Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Thomas Peeters, Andrew H. Song, Faisal Mahmood|https://arxiv.org/pdf/2405.11618|Self-supervised learning (SSL) has been successful in building patchembeddings of small histology images (e.g., 224x224 pixels), but scaling thesemodels to learn slide embeddings from the entirety of giga-pixel whole-slideimages (WSIs) remains challenging. Here, we leverage complementary informationfrom gene expression profiles to guide slide representation learning usingmultimodal pre-training. Expression profiles constitute highly detailedmolecular descriptions of a tissue that we hypothesize offer a strongtask-agnostic training signal for learning slide embeddings. Our slide andexpression (S+E) pre-training strategy, called Tangle, employsmodality-specific encoders, the outputs of which are aligned via contrastivelearning. Tangle was pre-trained on samples from three different organs: liver(n=6,597 S+E pairs), breast (n=1,020), and lung (n=1,012) from two differentspecies (Homo sapiens and Rattus norvegicus). Across three independent testdatasets consisting of 1,265 breast WSIs, 1,946 lung WSIs, and 4,584 liverWSIs, Tangle shows significantly better few-shot performance compared tosupervised and SSL baselines. When assessed using prototype-basedclassification and slide retrieval, Tangle also shows a substantial performanceimprovement over all baselines. Code available athttps://github.com/mahmoodlab/TANGLE.|2024|CVPR|https://github.com/mahmoodlab/TANGLE|oral|未复现|
|182|DiffusionLight: Light Probes for Free by Painting a Chrome Ball|Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn|https://arxiv.org/pdf/2312.09168|We present a simple yet effective technique to estimate lighting in a singleinput image. Current techniques rely heavily on HDR panorama datasets to trainneural networks to regress an input with limited field-of-view to a fullenvironment map. However, these approaches often struggle with real-world,uncontrolled settings due to the limited diversity and size of their datasets.To address this problem, we leverage diffusion models trained on billions ofstandard images to render a chrome ball into the input image. Despite itssimplicity, this task remains challenging: the diffusion models often insertincorrect or inconsistent objects and cannot readily generate images in HDRformat. Our research uncovers a surprising relationship between the appearanceof chrome balls and the initial diffusion noise map, which we utilize toconsistently generate high-quality chrome balls. We further fine-tune an LDRdiffusion model (Stable Diffusion XL) with LoRA, enabling it to performexposure bracketing for HDR light estimation. Our method produces convincinglight estimates across diverse settings and demonstrates superiorgeneralization to in-the-wild scenarios.|2024|CVPR|https://github.com/DiffusionLight/DiffusionLight?tab=readme-ov-file|oral|未复现|
|183|URHand: Universal Relightable Hands|Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael Zollhöfer, Yaser Sheikh, Ziwei Liu, Shunsuke Saito|https://arxiv.org/pdf/2401.05334|Existing photorealistic relightable hand models require extensiveidentity-specific observations in different views, poses, and illuminations,and face challenges in generalizing to natural illuminations and novelidentities. To bridge this gap, we present URHand, the first universalrelightable hand model that generalizes across viewpoints, poses,illuminations, and identities. Our model allows few-shot personalization usingimages captured with a mobile phone, and is ready to be photorealisticallyrendered under novel illuminations. To simplify the personalization processwhile retaining photorealism, we build a powerful universal relightable priorbased on neural relighting from multi-view images of hands captured in a lightstage with hundreds of identities. The key challenge is scaling thecross-identity training while maintaining personalized fidelity and sharpdetails without compromising generalization under natural illuminations. Tothis end, we propose a spatially varying linear lighting model as the neuralrenderer that takes physics-inspired shading as input feature. By removingnon-linear activations and bias, our specifically designed lighting modelexplicitly keeps the linearity of light transport. This enables single-stagetraining from light-stage data while generalizing to real-time rendering underarbitrary continuous illuminations across diverse identities. In addition, weintroduce the joint learning of a physically based model and our neuralrelighting model, which further improves fidelity and generalization. Extensiveexperiments show that our approach achieves superior performance over existingmethods in terms of both quality and generalizability. We also demonstratequick personalization of URHand from a short phone scan of an unseen identity.|2024|CVPR|https://github.com/facebookresearch/goliath|oral|未复现|
|184|Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation|Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler|https://arxiv.org/pdf/2312.02145|Monocular depth estimation is a fundamental computer vision task. Recovering3D depth from a single image is geometrically ill-posed and requires sceneunderstanding, so it is not surprising that the rise of deep learning has ledto a breakthrough. The impressive progress of monocular depth estimators hasmirrored the growth in model capacity, from relatively modest CNNs to largeTransformer architectures. Still, monocular depth estimators tend to strugglewhen presented with images with unfamiliar content and layout, since theirknowledge of the visual world is restricted by the data seen during training,and challenged by zero-shot generalization to new domains. This motivates us toexplore whether the extensive priors captured in recent generative diffusionmodels can enable better, more generalizable depth estimation. We introduceMarigold, a method for affine-invariant monocular depth estimation that isderived from Stable Diffusion and retains its rich prior knowledge. Theestimator can be fine-tuned in a couple of days on a single GPU using onlysynthetic training data. It delivers state-of-the-art performance across a widerange of datasets, including over 20% performance gains in specific cases.Project page: https://marigoldmonodepth.github.io.|2024|CVPR|https://github.com/prs-eth/marigold|oral|未复现|
|185|PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics|Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang|https://arxiv.org/pdf/2311.12198|We introduce PhysGaussian, a new method that seamlessly integrates physicallygrounded Newtonian dynamics within 3D Gaussians to achieve high-quality novelmotion synthesis. Employing a custom Material Point Method (MPM), our approachenriches 3D Gaussian kernels with physically meaningful kinematic deformationand mechanical stress attributes, all evolved in line with continuum mechanicsprinciples. A defining characteristic of our method is the seamless integrationbetween physical simulation and visual rendering: both components utilize thesame 3D Gaussian kernels as their discrete representations. This negates thenecessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," orany other geometry embedding, highlighting the principle of "what you see iswhat you simulate (WS$^2$)." Our method demonstrates exceptional versatilityacross a wide variety of materials--including elastic entities, metals,non-Newtonian fluids, and granular materials--showcasing its strongcapabilities in creating diverse visual content with novel viewpoints andmovements. Our project page is at: https://xpandora.github.io/PhysGaussian/|2024|CVPR|https://github.com/XPandora/PhysGaussian|oral|未复现|
|186|HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting|Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu|https://arxiv.org/pdf/2311.17061|Realistic 3D human generation from text prompts is a desirable yetchallenging task. Existing methods optimize 3D representations like mesh orneural fields via score distillation sampling (SDS), which suffers frominadequate fine details or excessive training time. In this paper, we proposean efficient yet effective framework, HumanGaussian, that generateshigh-quality 3D humans with fine-grained geometry and realistic appearance. Ourkey insight is that 3D Gaussian Splatting is an efficient renderer withperiodic Gaussian shrinkage or growing, where such adaptive density control canbe naturally guided by intrinsic human structures. Specifically, 1) we firstpropose a Structure-Aware SDS that simultaneously optimizes human appearanceand geometry. The multi-modal score function from both RGB and depth space isleveraged to distill the Gaussian densification and pruning process. 2)Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDSinto a noisier generative score and a cleaner classifier score, which welladdresses the over-saturation issue. The floating artifacts are furthereliminated based on Gaussian size in a prune-only phase to enhance generationsmoothness. Extensive experiments demonstrate the superior efficiency andcompetitive quality of our framework, rendering vivid 3D humans under diversescenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian|2024|CVPR|https://github.com/alvinliu0/HumanGaussian|oral|未复现|
|187|Prompt Highlighter: Interactive Control for Multi-Modal LLMs|Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia|https://arxiv.org/pdf/2312.04302|This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)inference: explicit controllable text generation. Multi-modal LLMs empowermulti-modality understanding with the capability of semantic generation yetbring less explainability and heavier reliance on prompt contents due to theirautoregressive generative nature. While manipulating prompt formats couldimprove outputs, designing specific and precise prompts per task can bechallenging and ineffective. To tackle this issue, we introduce a novelinference method, Prompt Highlighter, which enables users to highlight specificprompt spans to interactively control the focus during generation. Motivated bythe classifier-free diffusion guidance, we form regular and unconditionalcontext pairs based on highlighted tokens, demonstrating that theautoregressive generation in models can be guided in a classifier-free way.Notably, we find that, during inference, guiding the models with highlightedtokens through the attention weights leads to more desired outputs. Ourapproach is compatible with current LLMs and VLMs, achieving impressivecustomized generation results without training. Experiments confirm itseffectiveness in focusing on input contexts and generating reliable content.Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and1552.5 in MME-perception. The code is available at:https://github.com/dvlab-research/Prompt-Highlighter/|2024|CVPR|https://github.com/dvlab-research/Prompt-Highlighter/|oral|未复现|
|188|Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation|Qi Yang, Xing Nie, Tong Li, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang|https://arxiv.org/pdf/2312.06462|Recently, an audio-visual segmentation (AVS) task has been introduced, aimingto group pixels with sounding objects within a given video. This tasknecessitates a first-ever audio-driven pixel-level understanding of the scene,posing significant challenges. In this paper, we propose an innovativeaudio-visual transformer framework, termed COMBO, an acronym for COoperation ofMulti-order Bilateral relatiOns. For the first time, our framework exploresthree types of bilateral entanglements within AVS: pixel entanglement, modalityentanglement, and temporal entanglement. Regarding pixel entanglement, weemploy a Siam-Encoder Module (SEM) that leverages prior knowledge to generatemore precise visual features from the foundational model. For modalityentanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO toalign corresponding visual and auditory signals bi-directionally. As fortemporal entanglement, we introduce an innovative adaptive inter-frameconsistency loss according to the inherent rules of temporal. Comprehensiveexperiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIouon MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate thatCOMBO surpasses previous state-of-the-art methods. Code and more results willbe publicly available at https://yannqi.github.io/AVS-COMBO/.|2024|CVPR|https://arxiv.org/pdf/2312.06462|oral|未复现|
|189|Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution|Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, Chen Change Loy|https://arxiv.org/pdf/2312.06640|Text-based diffusion models have exhibited remarkable success in generationand editing, showing great promise for enhancing visual content with theirgenerative prior. However, applying these models to video super-resolutionremains challenging due to the high demands for output fidelity and temporalconsistency, which is complicated by the inherent randomness in diffusionmodels. Our study introduces Upscale-A-Video, a text-guided latent diffusionframework for video upscaling. This framework ensures temporal coherencethrough two key mechanisms: locally, it integrates temporal layers into U-Netand VAE-Decoder, maintaining consistency within short sequences; globally,without training, a flow-guided recurrent latent propagation module isintroduced to enhance overall video stability by propagating and fusing latentacross the entire sequences. Thanks to the diffusion paradigm, our model alsooffers greater flexibility by allowing text prompts to guide texture creationand adjustable noise levels to balance restoration and generation, enabling atrade-off between fidelity and quality. Extensive experiments show thatUpscale-A-Video surpasses existing methods in both synthetic and real-worldbenchmarks, as well as in AI-generated videos, showcasing impressive visualrealism and temporal consistency.|2024|CVPR|https://github.com/sczhou/Upscale-A-Video|oral|未复现|
|190|Putting the Object Back into Video Object Segmentation|Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, Alexander Schwing|https://arxiv.org/pdf/2310.12982|We present Cutie, a video object segmentation (VOS) network with object-levelmemory reading, which puts the object representation from memory back into thevideo object segmentation result. Recent works on VOS employ bottom-uppixel-level memory reading which struggles due to matching noise, especially inthe presence of distractors, resulting in lower performance in more challengingdata. In contrast, Cutie performs top-down object-level memory reading byadapting a small set of object queries. Via those, it interacts with thebottom-up pixel features iteratively with a query-based object transformer (qt,hence Cutie). The object queries act as a high-level summary of the targetobject, while high-resolution feature maps are retained for accuratesegmentation. Together with foreground-background masked attention, Cutiecleanly separates the semantics of the foreground object from the background.On the challenging MOSE dataset, Cutie improves by 8.7 J&F over XMem with asimilar running time and improves by 4.2 J&F over DeAOT while being three timesfaster. Code is available at: https://hkchengrex.github.io/Cutie|2024|CVPR|https://github.com/hkchengrex/Cutie|oral|未复现|
|191|InstanceDiffusion: Instance-level Control for Image Generation|Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra|https://arxiv.org/pdf/2402.03290|Text-to-image diffusion models produce high quality images but do not offercontrol over individual instances in the image. We introduce InstanceDiffusionthat adds precise instance-level control to text-to-image diffusion models.InstanceDiffusion supports free-form language conditions per instance andallows flexible ways to specify instance locations such as simple singlepoints, scribbles, bounding boxes or intricate instance segmentation masks, andcombinations thereof. We propose three major changes to text-to-image modelsthat enable precise instance-level control. Our UniFusion block enablesinstance-level conditions for text-to-image models, the ScaleU block improvesimage fidelity, and our Multi-instance Sampler improves generations formultiple instances. InstanceDiffusion significantly surpasses specializedstate-of-the-art models for each location condition. Notably, on the COCOdataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$for box inputs, and 25.4% IoU for mask inputs.|2024|CVPR|https://github.com/frank-xwang/InstanceDiffusion|oral|未复现|
|192|OMG-Seg: Is One Model Good Enough For All Segmentation?|Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy|https://arxiv.org/pdf/2401.10229|In this work, we address various segmentation tasks, each traditionallytackled by distinct or partially unified models. We propose OMG-Seg, One Modelthat is Good enough to efficiently and effectively handle all the segmentationtasks, including image semantic, instance, and panoptic segmentation, as wellas their video counterparts, open vocabulary settings, prompt-driven,interactive segmentation like SAM, and video object segmentation. To ourknowledge, this is the first model to handle all these tasks in one model andachieve satisfactory performance. We show that OMG-Seg, a transformer-basedencoder-decoder architecture with task-specific queries and outputs, cansupport over ten distinct segmentation tasks and yet significantly reducecomputational and parameter overhead across various tasks and datasets. Werigorously evaluate the inter-task influences and correlations duringco-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.|2024|CVPR|https://github.com/lxtGH/OMG-Seg|oral|未复现|
|193|Towards Language-Driven Video Inpainting via Multimodal Large Language Models|Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy|https://arxiv.org/pdf/2401.10226|We introduce a new task -- language-driven video inpainting, which usesnatural language instructions to guide the inpainting process. This approachovercomes the limitations of traditional video inpainting methods that dependon manually labeled binary masks, a process often tedious and labor-intensive.We present the Remove Objects from Videos by Instructions (ROVI) dataset,containing 5,650 videos and 9,091 inpainting results, to support training andevaluation for this task. We also propose a novel diffusion-basedlanguage-driven video inpainting framework, the first end-to-end baseline forthis task, integrating Multimodal Large Language Models to understand andexecute complex language-based inpainting requests effectively. Ourcomprehensive results showcase the dataset's versatility and the model'seffectiveness in various language-instructed inpainting scenarios. We will makedatasets, code, and models publicly available.|2024|CVPR|https://github.com/jianzongwu/Language-Driven-Video-Inpainting|oral|未复现|
|194|VBench: Comprehensive Benchmark Suite for Video Generative Models|Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu|https://arxiv.org/pdf/2311.17982|Video generation has witnessed significant advancements, yet evaluating thesemodels remains a challenge. A comprehensive evaluation benchmark for videogeneration is indispensable for two reasons: 1) Existing metrics do not fullyalign with human perceptions; 2) An ideal evaluation system should provideinsights to inform future developments of video generation. To this end, wepresent VBench, a comprehensive benchmark suite that dissects "video generationquality" into specific, hierarchical, and disentangled dimensions, each withtailored prompts and evaluation methods. VBench has three appealing properties:1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation(e.g., subject identity inconsistency, motion smoothness, temporal flickering,and spatial relationship, etc). The evaluation metrics with fine-grained levelsreveal individual models' strengths and weaknesses. 2) Human Alignment: We alsoprovide a dataset of human preference annotations to validate our benchmarks'alignment with human perception, for each evaluation dimension respectively. 3)Valuable Insights: We look into current models' ability across variousevaluation dimensions, and various content types. We also investigate the gapsbetween video and image generation models. We will open-source VBench,including all prompts, evaluation methods, generated videos, and humanpreference annotations, and also include more video generation models in VBenchto drive forward the field of video generation.|2024|CVPR|https://github.com/Vchitect/VBench|oral|未复现|
|195|PIGEON: Predicting Image Geolocations|Lukas Haas, Michal Skreta, Silas Alberti, Chelsea Finn|https://arxiv.org/pdf/2307.05845|Planet-scale image geolocalization remains a challenging problem due to thediversity of images originating from anywhere in the world. Although approachesbased on vision transformers have made significant progress in geolocalizationaccuracy, success in prior literature is constrained to narrow distributions ofimages of landmarks, and performance has not generalized to unseen places. Wepresent a new geolocalization system that combines semantic geocell creation,multi-task contrastive pretraining, and a novel loss function. Additionally,our work is the first to perform retrieval over location clusters for guessrefinements. We train two models for evaluations on street-level data andgeneral-purpose image geolocalization; the first model, PIGEON, is trained ondata from the game of Geoguessr and is capable of placing over 40% of itsguesses within 25 kilometers of the target location globally. We also develop abot and deploy PIGEON in a blind experiment against humans, ranking in the top0.01% of players. We further challenge one of the world's foremost professionalGeoguessr players to a series of six matches with millions of viewers, winningall six games. Our second model, PIGEOTTO, differs in that it is trained on adataset of images from Flickr and Wikipedia, achieving state-of-the-art resultson a wide range of image geolocalization benchmarks, outperforming the previousSOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8percentage points on the country level. Our findings suggest that PIGEOTTO isthe first image geolocalization model that effectively generalizes to unseenplaces and that our approach can pave the way for highly accurate, planet-scaleimage geolocalization systems. Our code is available on GitHub.|2024|CVPR|https://github.com/LukasHaas/PIGEON|oral|未复现|
|196|DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis|Yuming Gu, You Xie, Hongyi Xu, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, Linjie Luo|https://arxiv.org/pdf/2312.13016|We present DiffPortrait3D, a conditional diffusion model that is capable ofsynthesizing 3D-consistent photo-realistic novel views from as few as a singlein-the-wild portrait. Specifically, given a single RGB input, we aim tosynthesize plausible but consistent facial details rendered from novel cameraviews with retained both identity and facial expression. In lieu oftime-consuming optimization and fine-tuning, our zero-shot method generalizeswell to arbitrary face portraits with unposed camera views, extreme facialexpressions, and diverse artistic depictions. At its core, we leverage thegenerative prior of 2D diffusion models pre-trained on large-scale imagedatasets as our rendering backbone, while the denoising is guided withdisentangled attentive control of appearance and camera pose. To achieve this,we first inject the appearance context from the reference image into theself-attention layers of the frozen UNets. The rendering view is thenmanipulated with a novel conditional control module that interprets the camerapose by watching a condition image of a crossed subject from the same view.Furthermore, we insert a trainable cross-view attention module to enhance viewconsistency, which is further strengthened with a novel 3D-aware noisegeneration process during inference. We demonstrate state-of-the-art resultsboth qualitatively and quantitatively on our challenging in-the-wild andmulti-view benchmarks.|2024|CVPR|https://github.com/FreedomGu/DiffPortrait3D/|highlight|未复现|
|197|Domain Prompt Learning with Quaternion Networks|Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, Xiaokang Yang|https://arxiv.org/abs/2312.08878|Prompt learning has emerged as an effective and data-efficient technique inlarge Vision-Language Models (VLMs). However, when adapting VLMs to specializeddomains such as remote sensing and medical imaging, domain prompt learningremains underexplored. While large-scale domain-specific foundation models canhelp tackle this challenge, their concentration on a single vision level makesit challenging to prompt both vision and language modalities. To overcome this,we propose to leverage domain-specific knowledge from domain-specificfoundation models to transfer the robust recognition ability of VLMs fromgeneralized to specialized domains, using quaternion networks. Specifically,the proposed method involves using domain-specific vision features fromdomain-specific foundation models to guide the transformation of generalizedcontextual embeddings from the language branch into a specialized space withinthe quaternion networks. Moreover, we present a hierarchical approach thatgenerates vision prompt features by analyzing intermodal relationships betweenhierarchical language prompt features and domain-specific vision features. Inthis way, quaternion networks can effectively mine the intermodal relationshipsin the specific domain, facilitating domain-specific vision-languagecontrastive learning. Extensive experiments on domain-specific datasets showthat our proposed method achieves new state-of-the-art results in promptlearning.|2024|CVPR|https://github.com/caoql98/DPLQ|highlight|未复现|
|198|DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing|Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai|https://arxiv.org/pdf/2306.14435|Accurate and controllable image editing is a challenging task that hasattracted significant attention recently. Notably, DragGAN is an interactivepoint-based image editing framework that achieves impressive editing resultswith pixel-level precision. However, due to its reliance on generativeadversarial networks (GANs), its generality is limited by the capacity ofpretrained GAN models. In this work, we extend this editing framework todiffusion models and propose a novel approach DragDiffusion. By harnessinglarge-scale pretrained diffusion models, we greatly enhance the applicabilityof interactive point-based editing on both real and diffusion-generated images.Our approach involves optimizing the diffusion latents to achieve precisespatial control. The supervision signal of this optimization process is fromthe diffusion model's UNet features, which are known to contain rich semanticand geometric information. Moreover, we introduce two additional techniques,namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identityof the original image. Lastly, we present a challenging benchmark datasetcalled DragBench -- the first benchmark to evaluate the performance ofinteractive point-based image editing methods. Experiments across a wide rangeof challenging cases (e.g., images with multiple objects, diverse objectcategories, various styles, etc.) demonstrate the versatility and generality ofDragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.|2024|CVPR|https://github.com/Yujun-Shi/DragDiffusion|highlight|未复现|
|199|Fast ODE-based Sampling for Diffusion Models in Around 5 Steps|Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen|https://arxiv.org/pdf/2312.00094|Sampling from diffusion models can be treated as solving the correspondingordinary differential equations (ODEs), with the aim of obtaining an accuratesolution with as few number of function evaluations (NFE) as possible.Recently, various fast samplers utilizing higher-order ODE solvers have emergedand achieved better performance than the initial first-order one. However,these numerical methods inherently result in certain approximation errors,which significantly degrades sample quality with extremely small NFE (e.g.,around 5). In contrast, based on the geometric observation that each samplingtrajectory almost lies in a two-dimensional subspace embedded in the ambientspace, we propose Approximate MEan-Direction Solver (AMED-Solver) thateliminates truncation errors by directly learning the mean direction for fastdiffusion sampling. Besides, our method can be easily used as a plugin tofurther improve existing ODE-based samplers. Extensive experiments on imagesynthesis with the resolution ranging from 32 to 512 demonstrate theeffectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,10.74 FID on ImageNet 64$\times$64, and 13.20 FID on LSUN Bedroom. Our code isavailable at https://github.com/zju-pi/diff-sampler.|2024|CVPR|https://github.com/zju-pi/diff-sampler|highlight|未复现|
|200|FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis|Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, Diana Marculescu|https://arxiv.org/pdf/2312.17681|Diffusion models have transformed the image-to-image (I2I) synthesis and arenow permeating into videos. However, the advancement of video-to-video (V2V)synthesis has been hampered by the challenge of maintaining temporalconsistency across video frames. This paper proposes a consistent V2V synthesisframework by jointly leveraging spatial conditions and temporal optical flowclues within the source video. Contrary to prior methods that strictly adhereto optical flow, our approach harnesses its benefits while handling theimperfection in flow estimation. We encode the optical flow via warping fromthe first frame and serve it as a supplementary reference in the diffusionmodel. This enables our model for video synthesis by editing the first framewith any prevalent I2I models and then propagating edits to successive frames.Our V2V model, FlowVid, demonstrates remarkable properties: (1) Flexibility:FlowVid works seamlessly with existing I2I models, facilitating variousmodifications, including stylization, object swaps, and local edits. (2)Efficiency: Generation of a 4-second video with 30 FPS and 512x512 resolutiontakes only 1.5 minutes, which is 3.1x, 7.2x, and 10.5x faster than CoDeF,Rerender, and TokenFlow, respectively. (3) High-quality: In user studies, ourFlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender(10.2%), and TokenFlow (40.4%).|2024|CVPR|https://jeff-liangf.github.io/projects/flowvid/|highlight|未复现|
|201|General Object Foundation Model for Images and Videos at Scale|Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai|https://arxiv.org/pdf/2312.09158|We present GLEE in this work, an object-level foundation model for locatingand identifying objects in images and videos. Through a unified framework, GLEEaccomplishes detection, segmentation, tracking, grounding, and identificationof arbitrary objects in the open world scenario for various object perceptiontasks. Adopting a cohesive learning strategy, GLEE acquires knowledge fromdiverse data sources with varying supervision levels to formulate generalobject representations, excelling in zero-shot transfer to new data and tasks.Specifically, we employ an image encoder, text encoder, and visual prompter tohandle multi-modal inputs, enabling to simultaneously solve variousobject-centric downstream tasks while maintaining state-of-the-art performance.Demonstrated through extensive training on over five million images fromdiverse benchmarks, GLEE exhibits remarkable versatility and improvedgeneralization performance, efficiently tackling downstream tasks without theneed for task-specific adaptation. By integrating large volumes ofautomatically labeled data, we further enhance its zero-shot generalizationcapabilities. Additionally, GLEE is capable of being integrated into LargeLanguage Models, serving as a foundational model to provide universalobject-level information for multi-modal tasks. We hope that the versatilityand universality of our method will mark a significant step in the developmentof efficient visual foundation models for AGI systems. The model and code willbe released at https://glee-vision.github.io .|2024|CVPR|https://github.com/FoundationVision/GLEE|highlight|未复现|
|202|Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding|Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu|https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.html|In precision agriculture the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper we introduce a novel "Insect-1M" dataset a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species our dataset including 1 million images with dense identification labels of taxonomy hierarchy and insect descriptions offers a panoramic view of entomology enabling foundation models to comprehend visual and semantic information about insects like never before. Then to efficiently establish an Insect Foundation Model we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models bringing them closer to the ultimate goal of precision agriculture.|2024|CVPR|https://uark-cviu.github.io/projects/insect-foundation/|highlight|未复现|
|203|Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance|Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang|https://arxiv.org/abs/2403.18036|Despite significant advancements in text-to-motion synthesis, generatinglanguage-guided human motion within 3D environments poses substantialchallenges. These challenges stem primarily from (i) the absence of powerfulgenerative models capable of jointly modeling natural language, 3D scenes, andhuman motion, and (ii) the generative models' intensive data requirementscontrasted with the scarcity of comprehensive, high-quality,language-scene-motion datasets. To tackle these issues, we introduce a noveltwo-stage framework that employs scene affordance as an intermediaterepresentation, effectively linking 3D scene grounding and conditional motiongeneration. Our framework comprises an Affordance Diffusion Model (ADM) forpredicting explicit affordance map and an Affordance-to-Motion Diffusion Model(AMDM) for generating plausible human motions. By leveraging scene affordancemaps, our method overcomes the difficulty in generating human motion undermultimodal condition signals, especially when training with limited datalacking extensive language-scene-motion pairs. Our extensive experimentsdemonstrate that our approach consistently outperforms all baselines onestablished benchmarks, including HumanML3D and HUMANISE. Additionally, wevalidate our model's exceptional generalization capabilities on a speciallycurated evaluation set featuring previously unseen descriptions and scenes.|2024|CVPR|https://github.com/afford-motion/afford-motion|highlight|未复现|
|204|Object Recognition as Next Token Predictio|Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim|https://arxiv.org/abs/2312.02142|We present an approach to pose object recognition as next token prediction.The idea is to apply a language decoder that auto-regressively predicts thetext tokens from image embeddings to form labels. To ground this predictionprocess in auto-regression, we customize a non-causal attention mask for thedecoder, incorporating two key features: modeling tokens from different labelsto be independent, and treating image tokens as a prefix. This maskingmechanism inspires an efficient method - one-shot sampling - to simultaneouslysample tokens of multiple labels in parallel and rank generated labels by theirprobabilities during inference. To further enhance the efficiency, we propose asimple strategy to construct a compact decoder by simply discarding theintermediate blocks of a pretrained language model. This approach yields adecoder that matches the full model's performance while being notably moreefficient. The code is available at https://github.com/kaiyuyue/nxtp|2024|CVPR|https://github.com/KaiyuYue/nxtp|highlight|未复现|
|205|RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models|Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, Pinar Yanardag|https://arxiv.org/abs/2312.04524|Recent advancements in diffusion-based models have demonstrated significantsuccess in generating images from text. However, video editing models have notyet reached the same level of visual quality and user control. To address this,we introduce RAVE, a zero-shot video editing method that leverages pre-trainedtext-to-image diffusion models without additional training. RAVE takes an inputvideo and a text prompt to produce high-quality videos while preserving theoriginal motion and semantic structure. It employs a novel noise shufflingstrategy, leveraging spatio-temporal interactions between frames, to producetemporally consistent videos faster than existing methods. It is also efficientin terms of memory requirements, allowing it to handle longer videos. RAVE iscapable of a wide range of edits, from local attribute modifications to shapetransformations. In order to demonstrate the versatility of RAVE, we create acomprehensive video evaluation dataset ranging from object-focused scenes tocomplex human activities like dancing and typing, and dynamic scenes featuringswimming fish and boats. Our qualitative and quantitative experiments highlightthe effectiveness of RAVE in diverse video editing scenarios compared toexisting methods. Our code, dataset and videos can be found inhttps://rave-video.github.io.|2024|CVPR|https://github.com/rehglab/RAVE|highlight|未复现|
|206|Readout Guidance: Learning Control from Diffusion Features|Grace Luo, Trevor Darrell, Oliver Wang, Dan B Goldman, Aleksander Holynski|https://arxiv.org/abs/2312.02150|We present Readout Guidance, a method for controlling text-to-image diffusionmodels with learned signals. Readout Guidance uses readout heads, lightweightnetworks trained to extract signals from the features of a pre-trained, frozendiffusion model at every timestep. These readouts can encode single-imageproperties, such as pose, depth, and edges; or higher-order properties thatrelate multiple images, such as correspondence and appearance similarity.Furthermore, by comparing the readout estimates to a user-defined target, andback-propagating the gradient through the readout head, these estimates can beused to guide the sampling process. Compared to prior methods for conditionalgeneration, Readout Guidance requires significantly fewer added parameters andtraining samples, and offers a convenient and simple recipe for reproducingdifferent forms of conditional control under a single framework, with a singlearchitecture and sampling procedure. We showcase these benefits in theapplications of drag-based manipulation, identity-consistent generation, andspatially aligned control. Project page: https://readout-guidance.github.io.|2024|CVPR|https://github.com/google-research/readout_guidance|highlight|未复现|
|207|Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark|Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard|https://arxiv.org/abs/2403.18821|We present a new dataset called Real Acoustic Fields (RAF) that captures realacoustic room data from multiple modalities. The dataset includes high-qualityand densely captured room impulse response data paired with multi-view images,and precise 6DoF pose tracking data for sound emitters and listeners in therooms. We used this dataset to evaluate existing methods for novel-viewacoustic synthesis and impulse response generation which previously relied onsynthetic data. In our evaluation, we thoroughly assessed existing audio andaudio-visual models against multiple criteria and proposed settings to enhancetheir performance on real-world data. We also conducted experiments toinvestigate the impact of incorporating visual data (i.e., images and depth)into neural acoustic field models. Additionally, we demonstrated theeffectiveness of a simple sim2real approach, where a model is pre-trained withsimulated data and fine-tuned with sparse real-world data, resulting insignificant improvements in the few-shot learning approach. RAF is the firstdataset to provide densely captured room acoustic data, making it an idealresource for researchers working on audio and audio-visual neural acousticfield modeling techniques. Demos and datasets are available on our projectpage: https://facebookresearch.github.io/real-acoustic-fields/|2024|CVPR|https://github.com/facebookresearch/real-acoustic-fields|highlight|未复现|
|208|RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D|Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, Xiaoguang Han|https://arxiv.org/abs/2311.16918|Lifting 2D diffusion for 3D generation is a challenging problem due to thelack of geometric prior and the complex entanglement of materials and lightingin natural images. Existing methods have shown promise by first creating thegeometry through score-distillation sampling (SDS) applied to rendered surfacenormals, followed by appearance modeling. However, relying on a 2D RGBdiffusion model to optimize surface normals is suboptimal due to thedistribution discrepancy between natural images and normals maps, leading toinstability in optimization. In this paper, recognizing that the normal anddepth information effectively describe scene geometry and be automaticallyestimated from images, we propose to learn a generalizable Normal-Depthdiffusion model for 3D generation. We achieve this by training on thelarge-scale LAION dataset together with the generalizable image-to-depth andnormal prior models. In an attempt to alleviate the mixed illumination effectsin the generated materials, we introduce an albedo diffusion model to imposedata-driven constraints on the albedo component. Our experiments show that whenintegrated into existing text-to-3D pipelines, our models significantly enhancethe detail richness, achieving state-of-the-art results. Our project page ishttps://aigc3d.github.io/richdreamer/.|2024|CVPR|https://github.com/modelscope/RichDreamer|highlight|未复现|
|209|RobustSAM: Segment Anything Robustly on Degraded Images|Wei-Ting Chen, Yu-Jiet Vong, Sy-Yen Kuo, Sizhuo Ma, Jian Wang|https://arxiv.org/abs/2406.09627|Segment Anything Model (SAM) has emerged as a transformative approach inimage segmentation, acclaimed for its robust zero-shot segmentationcapabilities and flexible prompting system. Nonetheless, its performance ischallenged by images with degraded quality. Addressing this limitation, wepropose the Robust Segment Anything Model (RobustSAM), which enhances SAM'sperformance on low-quality images while preserving its promptability andzero-shot generalization. Our method leverages the pre-trained SAM model withonly marginal parameter increments and computational requirements. Theadditional parameters of RobustSAM can be optimized within 30 hours on eightGPUs, demonstrating its feasibility and practicality for typical researchlaboratories. We also introduce the Robust-Seg dataset, a collection of 688Kimage-mask pairs with different degradations designed to train and evaluate ourmodel optimally. Extensive experiments across various segmentation tasks anddatasets confirm RobustSAM's superior performance, especially under zero-shotconditions, underscoring its potential for extensive real-world application.Additionally, our method has been shown to effectively improve the performanceof SAM-based downstream tasks such as single image dehazing and deblurring.|2024|CVPR|https://github.com/robustsam/RobustSAM|highlight|未复现|
|210|Scaling Up Dynamic Human-Scene Interaction Modeling|Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang|https://arxiv.org/abs/2403.08629|Confronting the challenges of data scarcity and advanced motion synthesis inhuman-scene interaction modeling, we introduce the TRUMANS dataset alongside anovel HSI motion synthesis method. TRUMANS stands as the most comprehensivemotion-captured HSI dataset currently available, encompassing over 15 hours ofhuman interactions across 100 indoor scenes. It intricately captures whole-bodyhuman motions and part-level object dynamics, focusing on the realism ofcontact. This dataset is further scaled up by transforming physicalenvironments into exact virtual models and applying extensive augmentations toappearance and motion for both humans and objects while maintaining interactionfidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive modelthat efficiently generates HSI sequences of any length, taking into accountboth scene context and intended actions. In experiments, our approach showsremarkable zero-shot generalizability on a range of 3D scene datasets (e.g.,PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimicoriginal motion-captured sequences, as confirmed by quantitative experimentsand human studies.|2024|CVPR|https://github.com/jnnan/trumans_utils|highlight|未复现|
|211|SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection|Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek|https://arxiv.org/abs/2402.17323|In the field of class incremental learning (CIL), generative replay hasbecome increasingly prominent as a method to mitigate the catastrophicforgetting, alongside the continuous improvements in generative models.However, its application in class incremental object detection (CIOD) has beensignificantly limited, primarily due to the complexities of scenes involvingmultiple labels. In this paper, we propose a novel approach called stablediffusion deep generative replay (SDDGR) for CIOD. Our method utilizes adiffusion-based generative model with pre-trained text-to-diffusion networks togenerate realistic and diverse synthetic images. SDDGR incorporates aniterative refinement strategy to produce high-quality images encompassing oldclasses. Additionally, we adopt an L2 knowledge distillation technique toimprove the retention of prior knowledge in synthetic images. Furthermore, ourapproach includes pseudo-labeling for old objects within new task images,preventing misclassification as background elements. Extensive experiments onthe COCO 2017 dataset demonstrate that SDDGR significantly outperforms existingalgorithms, achieving a new state-of-the-art in various CIOD scenarios. Thesource code will be made available to the public.|2024|CVPR|/|highlight|未复现|
|212|SpatialTracker: Tracking Any 2D Pixels in 3D Space|Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou|https://arxiv.org/abs/2404.04319|Recovering dense and long-range pixel motion in videos is a challengingproblem. Part of the difficulty arises from the 3D-to-2D projection process,leading to occlusions and discontinuities in the 2D motion domain. While 2Dmotion can be intricate, we posit that the underlying 3D motion can often besimple and low-dimensional. In this work, we propose to estimate pointtrajectories in 3D space to mitigate the issues caused by image projection. Ourmethod, named SpatialTracker, lifts 2D pixels to 3D using monocular depthestimators, represents the 3D content of each frame efficiently using atriplane representation, and performs iterative updates using a transformer toestimate 3D trajectories. Tracking in 3D allows us to leverageas-rigid-as-possible (ARAP) constraints while simultaneously learning arigidity embedding that clusters pixels into different rigid parts. Extensiveevaluation shows that our approach achieves state-of-the-art trackingperformance both qualitatively and quantitatively, particularly in challengingscenarios such as out-of-plane rotation.|2024|CVPR|https://github.com/henry123-boy/SpaTracker|highlight|未复现|
|213|TFMQ-DM:Temporal Feature Maintenance Quantization for Diffusion Models|Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu|https://openaccess.thecvf.com/content/CVPR2024/html/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.html|The Diffusion model a prevalent framework for image generation encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models diffusion models heavily depend on the time-step t to achieve satisfactory multi-round denoising. Usually t from the finite set \ 1 \ldots T\  is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods resulting in a severe disturbance of the temporal feature and denoising trajectory as well as a low compression efficiency. To solve these we propose a Temporal Feature Maintenance Quantization (TFMQ) framework building upon a Temporal Information Block which is just related to the time-step t and unrelated to the sampling data. Powered by the pioneering block design we devise temporal information aware reconstruction (TIAR) and finite set calibration (FSC) to align the full-precision temporal features in a limited time. Equipped with the framework we can maintain the most temporal information and ensure the end-to-end generation quality. Extensive experiments on various datasets and diffusion models prove our state-of-the-art results. Remarkably our quantization approach for the first time achieves model performance nearly on par with the full-precision model under 4-bit weight quantization. Additionally our method incurs almost no extra computational cost and accelerates quantization time by 2.0 xon LSUN-Bedrooms 256 x256 compared to previous works. Our code is publicly available at \href https://github.com/ModelTC/TFMQ-DM  https://github.com/ModelTC/TFMQ-DM .|2024|CVPR|https://github.com/ModelTC/TFMQ-DM|highlight|未复现|
|214|Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval|Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao|https://arxiv.org/abs/2403.17998|The increasing prevalence of video clips has sparked growing interest intext-video retrieval. Recent advances focus on establishing a joint embeddingspace for text and video, relying on consistent embedding representations tocompute similarity. However, the text content in existing datasets is generallyshort and concise, making it hard to fully describe the redundant semantics ofa video. Correspondingly, a single text embedding may be less expressive tocapture the video embedding and empower the retrieval. In this study, wepropose a new stochastic text modeling method T-MASS, i.e., text is modeled asa stochastic embedding, to enrich text embedding with a flexible and resilientsemantic range, yielding a text mass. To be specific, we introduce asimilarity-aware radius module to adapt the scale of the text mass upon thegiven text-video pairs. Plus, we design and develop a support textregularization to further control the text mass during the training. Theinference pipeline is also tailored to fully exploit the text mass for accurateretrieval. Empirical evidence suggests that T-MASS not only effectivelyattracts relevant text-video pairs while distancing irrelevant ones, but alsoenables the determination of precise text embeddings for relevant pairs. Ourexperimental results show a substantial improvement of T-MASS over baseline (3%to 6.3% by R@1). Also, T-MASS achieves state-of-the-art performance on fivebenchmark datasets, including MSRVTT, LSMDC, DiDeMo, VATEX, and Charades.|2024|CVPR|https://github.com/patrick-0817/T-MASS-text-video-retrieval|highlight|未复现|
|215|Towards Learning a Generalist Model for Embodied Navigation|Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang|https://arxiv.org/abs/2312.02010|Building a generalist agent that can interact with the world is theintriguing target of AI systems, thus spurring the research for embodiednavigation, where an agent is required to navigate according to instructions orrespond to queries. Despite the major progress attained, previous worksprimarily focus on task-specific agents and lack generalizability to unseenscenarios. Recently, LLMs have presented remarkable capabilities across variousfields, and provided a promising opportunity for embodied navigation. Drawingon this, we propose the first generalist model for embodied navigation,NaviLLM. It adapts LLMs to embodied navigation by introducing schema-basedinstruction. The schema-based instruction flexibly casts various tasks intogeneration problems, thereby unifying a wide range of tasks. This approachallows us to integrate diverse data sources from various datasets into thetraining, equipping NaviLLM with a wide range of capabilities required byembodied navigation. We conduct extensive experiments to evaluate theperformance and generalizability of our model. The experimental resultsdemonstrate that our unified model achieves state-of-the-art performance onCVDN, SOON, and ScanQA. Specifically, it surpasses the previousstats-of-the-art method by a significant margin of 29% in goal progress onCVDN. Moreover, our model also demonstrates strong generalizability andpresents impressive results on unseen tasks, e.g., embodied question answeringand 3D captioning.|2024|CVPR|https://github.com/LaVi-Lab/NaviLLM|highlight|未复现|
|216|UniMODE: Unified Monocular 3D Object Detection|Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao|https://arxiv.org/abs/2402.18573|Realizing unified 3D object detection, including both indoor and outdoorscenes, holds great importance in applications like robot navigation. However,involving various scenarios of data to train models poses challenges due totheir significantly distinct characteristics, \eg, diverse geometry propertiesand heterogeneous domain distributions. In this work, we propose to address thechallenges from two perspectives, the algorithm perspective and dataperspective. In terms of the algorithm perspective, we first build a monocular3D object detector based on the bird's-eye-view (BEV) detection paradigm, wherethe explicit feature projection is beneficial to addressing the geometrylearning ambiguity. In this detector, we split the classical BEV detectionarchitecture into two stages and propose an uneven BEV grid design to handlethe convergence instability caused by geometry difference between scenarios.Besides, we develop a sparse BEV feature projection strategy to reduce thecomputational cost and a unified domain alignment method to handleheterogeneous domains. From the data perspective, we propose to incorporatedepth information to improve training robustness. Specifically, we build thefirst unified multi-modal 3D object detection benchmark MM-Omni3D and extendthe aforementioned monocular detector to its multi-modal version, which is thefirst unified multi-modal 3D object detector. We name the designed monocularand multi-modal detectors as UniMODE and MM-UniMODE, respectively. Theexperimental results reveal several insightful findings highlighting thebenefits of multi-modal data and confirm the effectiveness of all the proposedstrategies.|2024|CVPR|https://github.com/Lizhuoling/UniMODE|highlight|未复现|
|217|Unsupervised Keypoints from Pretrained Diffusion Models|Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi|https://arxiv.org/abs/2312.00065|Unsupervised learning of keypoints and landmarks has seen significantprogress with the help of modern neural network architectures, but performanceis yet to match the supervised counterpart, making their practicabilityquestionable. We leverage the emergent knowledge within text-to-image diffusionmodels, towards more robust unsupervised keypoints. Our core idea is to findtext embeddings that would cause the generative model to consistently attend tocompact regions in images (i.e. keypoints). To do so, we simply optimize thetext embedding such that the cross-attention maps within the denoising networkare localized as Gaussians with small standard deviations. We validate ourperformance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,DeepFashion, and Human3.6m datasets. We achieve significantly improvedaccuracy, sometimes even outperforming supervised ones, particularly for datathat is non-aligned and less curated. Our code is publicly available and can befound through our project page: https://ubc-vision.github.io/StableKeypoints/|2024|CVPR|https://github.com/ubc-vision/StableKeypoints|highlight|未复现|
|218|VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models|Xiang Li, Qianli Shen, Kenji Kawaguchi|https://arxiv.org/abs/2312.00057|The booming use of text-to-image generative models has raised concerns abouttheir high risk of producing copyright-infringing content. While probabilisticcopyright protection methods provide a probabilistic guarantee against suchinfringement, in this paper, we introduce Virtually Assured AmplificationAttack (VA3), a novel online attack framework that exposes the vulnerabilitiesof these protection mechanisms. The proposed framework significantly amplifiesthe probability of generating infringing content on the sustained interactionswith generative models and a non-trivial lower-bound on the success probabilityof each engagement. Our theoretical and experimental results demonstrate theeffectiveness of our approach under various scenarios. These findings highlightthe potential risk of implementing probabilistic copyright protection inpractical applications of text-to-image generative models. Code is available athttps://github.com/South7X/VA3.|2024|CVPR|https://github.com/South7X/VA3|highlight|未复现|
|219|VecFusion: Vector Font Generation with Diffusion|Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michael Gharbi, Oliver Wang, Alec Jacobson, Evangelos Kalogerakis|https://arxiv.org/abs/2312.10540|We present VecFusion, a new neural architecture that can generate vectorfonts with varying topological structures and precise control point positions.Our approach is a cascaded diffusion model which consists of a raster diffusionmodel followed by a vector diffusion model. The raster model generateslow-resolution, rasterized fonts with auxiliary control point information,capturing the global style and shape of the font, while the vector modelsynthesizes vector fonts conditioned on the low-resolution raster fonts fromthe first stage. To synthesize long and complex curves, our vector diffusionmodel uses a transformer architecture and a novel vector representation thatenables the modeling of diverse vector geometry and the precise prediction ofcontrol points. Our experiments show that, in contrast to previous generativemodels for vector graphics, our new cascaded vector diffusion model generateshigher quality vector fonts, with complex structures and diverse styles.|2024|CVPR|https://vikastmz.github.io/VecFusion/|highlight|未复现|
|220|Wonder3D: Single Image to 3D using Cross-Domain Diffusion|Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang|https://arxiv.org/abs/2310.15008|In this work, we introduce Wonder3D, a novel method for efficientlygenerating high-fidelity textured meshes from single-view images.Recent methodsbased on Score Distillation Sampling (SDS) have shown the potential to recover3D geometry from 2D diffusion priors, but they typically suffer fromtime-consuming per-shape optimization and inconsistent geometry. In contrast,certain works directly produce 3D information via fast network inferences, buttheir results are often of low quality and lack geometric details. Toholistically improve the quality, consistency, and efficiency of image-to-3Dtasks, we propose a cross-domain diffusion model that generates multi-viewnormal maps and the corresponding color images. To ensure consistency, weemploy a multi-view cross-domain attention mechanism that facilitatesinformation exchange across views and modalities. Lastly, we introduce ageometry-aware normal fusion algorithm that extracts high-quality surfaces fromthe multi-view 2D representations. Our extensive evaluations demonstrate thatour method achieves high-quality reconstruction results, robust generalization,and reasonably good efficiency compared to prior works.|2024|CVPR|https://github.com/xxlong0/Wonder3D|highlight|未复现|
|221|VTimeLLM: Empower LLM to Grasp Video Moments|Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu|https://arxiv.org/abs/2311.18445|Large language models (LLMs) have shown remarkable text understandingcapabilities, which have been extended as Video LLMs to handle video data forcomprehending visual details. However, existing Video LLMs can only provide acoarse description of the entire video, failing to capture the precise startand end time boundary of specific events. In this paper, we solve this issuevia proposing VTimeLLM, a novel Video LLM designed for fine-grained videomoment understanding and reasoning with respect to time boundary. Specifically,our VTimeLLM adopts a boundary-aware three-stage training strategy, whichrespectively utilizes image-text pairs for feature alignment, multiple-eventvideos to increase temporal-boundary awareness, and high-qualityvideo-instruction tuning to further improve temporal understanding ability aswell as align with human intents. Extensive experiments demonstrate that infine-grained time-related comprehension tasks for videos such as Temporal VideoGrounding and Dense Video Captioning, VTimeLLM significantly outperformsexisting Video LLMs. Besides, benefits from the fine-grained temporalunderstanding of the videos further enable VTimeLLM to beat existing Video LLMsin video dialogue benchmark, showing its superior cross-modal understanding andreasoning abilities.|2024|CVPR|https://github.com/huangb23/VTimeLLM|highlight|未复现|
|222|LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds|Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo|https://arxiv.org/pdf/2503.10625v1|Animatable 3D human reconstruction from a single image is a challengingproblem due to the ambiguity in decoupling geometry, appearance, anddeformation. Recent advances in 3D human reconstruction mainly focus on statichuman modeling, and the reliance of using synthetic 3D scans for traininglimits their generalization ability. Conversely, optimization-based videomethods achieve higher fidelity but demand controlled capture conditions andcomputationally intensive refinement processes. Motivated by the emergence oflarge reconstruction models for efficient static reconstruction, we propose LHM(Large Animatable Human Reconstruction Model) to infer high-fidelity avatarsrepresented as 3D Gaussian splatting in a feed-forward pass. Our modelleverages a multimodal transformer architecture to effectively encode the humanbody positional features and image features with attention mechanism, enablingdetailed preservation of clothing geometry and texture. To further boost theface identity preservation and fine detail recovery, we propose a head featurepyramid encoding scheme to aggregate multi-scale features of the head regions.Extensive experiments demonstrate that our LHM generates plausible animatablehuman in seconds without post-processing for face and hands, outperformingexisting methods in both reconstruction accuracy and generalization ability.|2025|ICCV|https://github.com/aigc3d/LHM?tab=readme-ov-file|poster|未复现|
|223|EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer|Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, Jiaming Liu|https://arxiv.org/abs/2503.07027|Recent advancements in Unet-based diffusion models, such as ControlNet andIP-Adapter, have introduced effective spatial and subject control mechanisms.However, the DiT (Diffusion Transformer) architecture still struggles withefficient and flexible control. To tackle this issue, we propose EasyControl, anovel framework designed to unify condition-guided diffusion transformers withhigh efficiency and flexibility. Our framework is built on three keyinnovations. First, we introduce a lightweight Condition Injection LoRA Module.This module processes conditional signals in isolation, acting as aplug-and-play solution. It avoids modifying the base model weights, ensuringcompatibility with customized models and enabling the flexible injection ofdiverse conditions. Notably, this module also supports harmonious and robustzero-shot multi-condition generalization, even when trained only onsingle-condition data. Second, we propose a Position-Aware Training Paradigm.This approach standardizes input conditions to fixed resolutions, allowing thegeneration of images with arbitrary aspect ratios and flexible resolutions. Atthe same time, it optimizes computational efficiency, making the framework morepractical for real-world applications. Third, we develop a Causal AttentionMechanism combined with the KV Cache technique, adapted for conditionalgeneration tasks. This innovation significantly reduces the latency of imagesynthesis, improving the overall efficiency of the framework. Through extensiveexperiments, we demonstrate that EasyControl achieves exceptional performanceacross various application scenarios. These innovations collectively make ourframework highly efficient, flexible, and suitable for a wide range of tasks.|2025|ICCV|https://github.com/Xiaojiu-z/EasyControl|poster|未复现|
|224|MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images|Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai|https://arxiv.org/pdf/2403.14627|We introduce MVSplat, an efficient model that, given sparse multi-view imagesas input, predicts clean feed-forward 3D Gaussians. To accurately localize theGaussian centers, we build a cost volume representation via plane sweeping,where the cross-view feature similarities stored in the cost volume can providevaluable geometry cues to the estimation of depth. We also learn other Gaussianprimitives' parameters jointly with the Gaussian centers while only relying onphotometric supervision. We demonstrate the importance of the cost volumerepresentation in learning feed-forward Gaussians via extensive experimentalevaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplatachieves state-of-the-art performance with the fastest feed-forward inferencespeed (22~fps). More impressively, compared to the latest state-of-the-artmethod pixelSplat, MVSplat uses $10\times$ fewer parameters and infers morethan $2\times$ faster while providing higher appearance and geometry quality aswell as better cross-dataset generalization.|2024|ECCV|https://github.com/donydchen/mvsplat|Oral|未复现|
|225|FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting|Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang|https://arxiv.org/abs/2312.00451|Novel view synthesis from limited observations remains an important andpersistent task. However, high efficiency in existing NeRF-based few-shot viewsynthesis is often compromised to obtain an accurate 3D representation. Toaddress this challenge, we propose a few-shot view synthesis framework based on3D Gaussian Splatting that enables real-time and photo-realistic view synthesiswith as few as three training views. The proposed method, dubbed FSGS, handlesthe extremely sparse initialized SfM points with a thoughtfully designedGaussian Unpooling process. Our method iteratively distributes new Gaussiansaround the most representative locations, subsequently infilling local detailsin vacant areas. We also integrate a large-scale pre-trained monocular depthestimator within the Gaussians optimization process, leveraging onlineaugmented views to guide the geometric optimization towards an optimalsolution. Starting from sparse points observed from limited input viewpoints,our FSGS can accurately grow into unseen regions, comprehensively covering thescene and boosting the rendering quality of novel views. Overall, FSGS achievesstate-of-the-art performance in both accuracy and rendering efficiency acrossdiverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website:https://zehaozhu.github.io/FSGS/.|2024|ECCV|https://github.com/VITA-Group/FSGS|poster|未复现|
|226|ZigMa: A DiT-style Zigzag Mamba Diffusion Model|Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Schusterbauer, Björn Ommer|https://arxiv.org/pdf/2403.13802|The diffusion model has long been plagued by scalability and quadraticcomplexity issues, especially within transformer-based structures. In thisstudy, we aim to leverage the long sequence modeling capability of aState-Space Model called Mamba to extend its applicability to visual datageneration. Firstly, we identify a critical oversight in most currentMamba-based vision methods, namely the lack of consideration for spatialcontinuity in the scan scheme of Mamba. Secondly, building upon this insight,we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,which outperforms Mamba-based baselines and demonstrates improved speed andmemory utilization compared to transformer-based baselines. Lastly, weintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigatethe scalability of the model on large-resolution visual datasets, such asFacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO$256\times 256$ . Code will be released at https://taohu.me/zigma/|2024|ECCV|https://github.com/CompVis/zigma|poster|未复现|
|227|Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors|Tongkun Guan, Wei Shen, Xue Yang, Xuehui Wang, Xiaokang Yang |https://arxiv.org/pdf/2312.05286|Existing scene text detection methods typically rely on extensive real datafor training. Due to the lack of annotated real images, recent works haveattempted to exploit large-scale labeled synthetic data (LSD) for pre-trainingtext detectors. However, a synth-to-real domain gap emerges, further limitingthe performance of text detectors. Differently, in this work, we proposeFreeReal, a real-domain-aligned pre-training paradigm that enables thecomplementary strengths of both LSD and unlabeled real data (URD).Specifically, to bridge real and synthetic worlds for pre-training, aglyph-based mixing mechanism (GlyphMix) is tailored for text images.GlyphMixdelineates the character structures of synthetic images and embeds them asgraffiti-like units onto real images. Without introducing real domain drift,GlyphMix freely yields real-world images with annotations derived fromsynthetic labels. Furthermore, when given free fine-grained synthetic labels,GlyphMix can effectively bridge the linguistic domain gap stemming fromEnglish-dominated LSD to URD in various languages. Without bells and whistles,FreeReal achieves average gains of 1.97%, 3.90%, 3.85%, and 4.56% in improvingthe performance of FCENet, PSENet, PANet, and DBNet methods, respectively,consistently outperforming previous pre-training methods by a substantialmargin across four public datasets. Code is available athttps://github.com/SJTU-DeepVisionLab/FreeReal.|2024|ECCV|https://github.com/SJTU-DeepVisionLab/FreeReal|poster|未复现|
|228|PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer|Tongkun Guan, Chengyu Lin, Wei Shen, Xiaokang Yang|https://arxiv.org/pdf/2407.07764|Handwritten Mathematical Expression Recognition (HMER) has wide applicationsin human-machine interaction scenarios, such as digitized education andautomated offices. Recently, sequence-based models with encoder-decoderarchitectures have been commonly adopted to address this task by directlypredicting LaTeX sequences of expression images. However, these methods onlyimplicitly learn the syntax rules provided by LaTeX, which may fail to describethe position and hierarchical relationship between symbols due to complexstructural relations and diverse handwriting styles. To overcome thischallenge, we propose a position forest transformer (PosFormer) for HMER, whichjointly optimizes two tasks: expression recognition and position recognition,to explicitly enable position-aware symbol feature representation learning.Specifically, we first design a position forest that models the mathematicalexpression as a forest structure and parses the relative position relationshipsbetween symbols. Without requiring extra annotations, each symbol is assigned aposition identifier in the forest to denote its relative spatial position.Second, we propose an implicit attention correction module to accuratelycapture attention for HMER in the sequence-based decoder architecture.Extensive experiments validate the superiority of PosFormer, which consistentlyoutperforms the state-of-the-art methods 2.03%/1.22%/2.00%, 1.83%, and 4.62%gains on the single-line CROHME 2014/2016/2019, multi-line M2E, and complex MNEdatasets, respectively, with no additional latency or computational cost. Codeis available at https://github.com/SJTU-DeepVisionLab/PosFormer.|2024|ECCV|https://github.com/SJTU-DeepVisionLab/PosFormer|poster|未复现|
|229|Fully Sparse 3D Occupancy Prediction|Haisong Liu, Yang Chen, Haiguang Wang, Zetong Yang, Tianyu Li, Jia Zeng, Li Chen, Hongyang Li, Limin Wang|https://arxiv.org/abs/2312.17118|Occupancy prediction plays a pivotal role in autonomous driving. Previousmethods typically construct dense 3D volumes, neglecting the inherent sparsityof the scene and suffering from high computational costs. To bridge the gap, weintroduce a novel fully sparse occupancy network, termed SparseOcc. SparseOccinitially reconstructs a sparse 3D representation from camera-only inputs andsubsequently predicts semantic/instance occupancy from the 3D sparserepresentation by sparse queries. A mask-guided sparse sampling is designed toenable sparse queries to interact with 2D features in a fully sparse manner,thereby circumventing costly dense features or global attention. Additionally,we design a thoughtful ray-based evaluation metric, namely RayIoU, to solve theinconsistency penalty along the depth axis raised in traditional voxel-levelmIoU criteria. SparseOcc demonstrates its effectiveness by achieving a RayIoUof 34.0, while maintaining a real-time inference speed of 17.3 FPS, with 7history frames inputs. By incorporating more preceding frames to 15, SparseOcccontinuously improves its performance to 35.1 RayIoU without bells andwhistles.|2024|ECCV|https://github.com/MCG-NJU/SparseOcc|poster|未复现|
|230|NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields|Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus|https://arxiv.org/abs/2404.01300|Neural fields excel in computer vision and robotics due to their ability tounderstand the 3D visual world such as inferring semantics, geometry, anddynamics. Given the capabilities of neural fields in densely representing a 3Dscene from 2D images, we ask the question: Can we scale their self-supervisedpretraining, specifically using masked autoencoders, to generate effective 3Drepresentations from posed RGB images. Owing to the astounding success ofextending transformers to novel data modalities, we employ standard 3D VisionTransformers to suit the unique formulation of NeRFs. We leverage NeRF'svolumetric grid as a dense input to the transformer, contrasting it with other3D representations such as pointclouds where the information density can beuneven, and the representation is irregular. Due to the difficulty of applyingmasked autoencoders to an implicit representation, such as NeRF, we opt forextracting an explicit representation that canonicalizes scenes across domainsby employing the camera trajectory for sampling. Our goal is made possible bymasking random patches from NeRF's radiance and density grid and employing astandard 3D Swin Transformer to reconstruct the masked patches. In doing so,the model can learn the semantic and spatial structure of complete scenes. Wepretrain this representation at scale on our proposed curated posed-RGB data,totaling over 1.8 million images. Once pretrained, the encoder is used foreffective 3D transfer learning. Our novel self-supervised pretraining forNeRFs, NeRF-MAE, scales remarkably well and improves performance on variouschallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRFscene understanding baselines on Front3D and ScanNet datasets with an absoluteperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.|2024|ECCV|https://github.com/zubair-irshad/NeRF-MAE|Poster|未复现|
|231|ControlCap: Controllable Region-level Captioning|Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Fang Wan, Qixiang Ye|https://arxiv.org/abs/2401.17910|Region-level captioning is challenged by the caption degeneration issue,which refers to that pre-trained multimodal models tend to predict the mostfrequent captions but miss the less frequent ones. In this study, we propose acontrollable region-level captioning (ControlCap) approach, which introducescontrol words to a multimodal model to address the caption degeneration issue.In specific, ControlCap leverages a discriminative module to generate controlwords within the caption space to partition it to multiple sub-spaces. Themultimodal model is constrained to generate captions within a few sub-spacescontaining the control words, which increases the opportunity of hitting lessfrequent captions, alleviating the caption degeneration issue. Furthermore,interactive control words can be given by either a human or an expert model,which enables captioning beyond the training caption space, enhancing themodel's generalization ability. Extensive experiments on Visual Genome andRefCOCOg datasets show that ControlCap respectively improves the CIDEr score by21.6 and 2.2, outperforming the state-of-the-arts by significant margins. Codeis available at https://github.com/callsys/ControlCap.|2024|ECCV|https://github.com/callsys/ControlCap|Poster|未复现|
|232|GiT: Towards Generalist Vision Transformer through Universal Language Interface|Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang|https://arxiv.org/abs/2403.09394|This paper proposes a simple, yet effective framework, called GiT,simultaneously applicable for various vision tasks only with a vanilla ViT.Motivated by the universality of the Multi-layer Transformer architecture (e.g,GPT) widely used in large language models (LLMs), we seek to broaden its scopeto serve as a powerful vision foundation model (VFM). However, unlike languagemodeling, visual tasks typically require specific modules, such as bounding boxheads for detection and pixel decoders for segmentation, greatly hindering theapplication of powerful multi-layer transformers in the vision domain. To solvethis, we design a universal language interface that empowers the successfulauto-regressive decoding to adeptly unify various visual tasks, fromimage-level understanding (e.g., captioning), over sparse perception (e.g.,detection), to dense prediction (e.g., segmentation). Based on the abovedesigns, the entire model is composed solely of a ViT, without any specificadditions, offering a remarkable architectural simplification. GiT is amulti-task visual model, jointly trained across five representative benchmarkswithout task-specific fine-tuning. Interestingly, our GiT builds a newbenchmark in generalist performance, and fosters mutual enhancement acrosstasks, leading to significant improvements compared to isolated training. Thisreflects a similar impact observed in LLMs. Further enriching training with 27datasets, GiT achieves strong zero-shot results over various tasks. Due to itssimple design, this paradigm holds promise for narrowing the architectural gapbetween vision and language. Code and models will be available at\url{https://github.com/Haiyang-W/GiT}.|2024|ECCV|https://github.com/Haiyang-W/GiT|Oral|未复现|
|233|Relation DETR: Exploring Explicit Position Relation Prior for Object Detection|Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen, Xuguang Lan|https://arxiv.org/abs/2407.11699v1|This paper presents a general scheme for enhancing the convergence andperformance of DETR (DEtection TRansformer). We investigate the slowconvergence problem in transformers from a new perspective, suggesting that itarises from the self-attention that introduces no structural bias over inputs.To address this issue, we explore incorporating position relation prior asattention bias to augment object detection, following the verification of itsstatistical significance using a proposed quantitative macroscopic correlation(MC) metric. Our approach, termed Relation-DETR, introduces an encoder toconstruct position relation embeddings for progressive attention refinement,which further extends the traditional streaming pipeline of DETR into acontrastive relation pipeline to address the conflicts between non-duplicatepredictions and positive supervision. Extensive experiments on both generic andtask-specific datasets demonstrate the effectiveness of our approach. Under thesame configurations, Relation-DETR achieves a significant improvement (+2.0% APcompared to DINO), state-of-the-art performance (51.7% AP for 1x and 52.1% APfor 2x settings), and a remarkably faster convergence speed (over 40% AP withonly 2 training epochs) than existing DETR detectors on COCO val2017. Moreover,the proposed relation encoder serves as a universal plug-in-and-play component,bringing clear improvements for theoretically any DETR-like methods.Furthermore, we introduce a class-agnostic detection dataset, SA-Det-100k. Theexperimental results on the dataset illustrate that the proposed explicitposition relation achieves a clear improvement of 1.3% AP, highlighting itspotential towards universal object detection. The code and dataset areavailable at https://github.com/xiuqhou/Relation-DETR.|2024|ECCV|https://github.com/xiuqhou/Relation-DETR|Oral|未复现|
|234|FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification|Yu Tian, Congcong Wen, Min Shi, Muhammad Muneeb Afzal, Hao Huang, Muhammad Osama Khan, Yan Luo, Yi Fang, Mengyu Wang|https://arxiv.org/abs/2407.08813|Addressing fairness in artificial intelligence (AI), particularly in medicalAI, is crucial for ensuring equitable healthcare outcomes. Recent efforts toenhance fairness have introduced new methodologies and datasets in medical AI.However, the fairness issue under the setting of domain transfer is almostunexplored, while it is common that clinics rely on different imagingtechnologies (e.g., different retinal imaging modalities) for patientdiagnosis. This paper presents FairDomain, a pioneering systemic study intoalgorithmic fairness under domain shifts, employing state-of-the-art domainadaptation (DA) and generalization (DG) algorithms for both medicalsegmentation and classification tasks to understand how biases are transferredbetween different domains. We also introduce a novel plug-and-play fairidentity attention (FIA) module that adapts to various DA and DG algorithms toimprove fairness by using self-attention to adjust feature importance based ondemographic attributes. Additionally, we curate the first fairness-focuseddataset with two paired imaging modalities for the same patient cohort onmedical segmentation and classification tasks, to rigorously assess fairness indomain-shift scenarios. Excluding the confounding impact of demographicdistribution variation between source and target domains will allow clearerquantification of the performance of domain transfer models. Our extensiveevaluations reveal that the proposed FIA significantly enhances both modelperformance accounted for fairness across all domain shift settings (i.e., DAand DG) with respect to different demographics, which outperforms existingmethods on both segmentation and classification. The code and data can beaccessed at https://ophai.hms.harvard.edu/datasets/harvard-fairdomain20k.|2024|ECCV|https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain|poster|未复现|
|235|OneRestore: A Universal Restoration Framework for Composite Degradation|Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, Shengfeng He|https://arxiv.org/abs/2407.04621|In real-world scenarios, image impairments often manifest as compositedegradations, presenting a complex interplay of elements such as low light,haze, rain, and snow. Despite this reality, existing restoration methodstypically target isolated degradation types, thereby falling short inenvironments where multiple degrading factors coexist. To bridge this gap, ourstudy proposes a versatile imaging model that consolidates four physicalcorruption paradigms to accurately represent complex, composite degradationscenarios. In this context, we propose OneRestore, a novel transformer-basedframework designed for adaptive, controllable scene restoration. The proposedframework leverages a unique cross-attention mechanism, merging degraded scenedescriptors with image features, allowing for nuanced restoration. Our modelallows versatile input scene descriptors, ranging from manual text embeddingsto automatic extractions based on visual attributes. Our methodology is furtherenhanced through a composite degradation restoration loss, using extra degradedimages as negative samples to fortify model constraints. Comparative results onsynthetic and real-world datasets demonstrate OneRestore as a superiorsolution, significantly advancing the state-of-the-art in addressing complex,composite degradations.|2024|ECCV|https://github.com/gy65896/OneRestore|poster|未复现|
|236|VideoStudio: Generating Consistent-Content and Multi-Scene Videos|Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei|https://arxiv.org/pdf/2401.01256|The recent innovations and breakthroughs in diffusion models havesignificantly expanded the possibilities of generating high-quality videos forthe given prompts. Most existing works tackle the single-scene scenario withonly one video event occurring in a single background. Extending to generatemulti-scene videos nevertheless is not trivial and necessitates to nicelymanage the logic in between while preserving the consistent visual appearanceof key content across video scenes. In this paper, we propose a novelframework, namely VideoStudio, for consistent-content and multi-scene videogeneration. Technically, VideoStudio leverages Large Language Models (LLM) toconvert the input prompt into comprehensive multi-scene script that benefitsfrom the logical knowledge learnt by LLM. The script for each scene includes aprompt describing the event, the foreground/background entities, as well ascamera movement. VideoStudio identifies the common entities throughout thescript and asks LLM to detail each entity. The resultant entity description isthen fed into a text-to-image model to generate a reference image for eachentity. Finally, VideoStudio outputs a multi-scene video by generating eachscene video via a diffusion process that takes the reference images, thedescriptive prompt of the event and camera movement into account. The diffusionmodel incorporates the reference images as the condition and alignment tostrengthen the content consistency of multi-scene videos. Extensive experimentsdemonstrate that VideoStudio outperforms the SOTA video generation models interms of visual quality, content consistency, and user preference. Source codeis available at \url{https://github.com/FuchenUSTC/VideoStudio}.|2024|ECCV|https://github.com/FuchenUSTC/VideoStudio|poster|未复现|
|237|Zero-shot Object Counting with Good Exemplars|Huilin Zhu, Jingling Yuan, Zhengwei Yang, Yu Guo, Zheng Wang, Xian Zhong, Shengfeng He|https://arxiv.org/abs/2407.04948|Zero-shot object counting (ZOC) aims to enumerate objects in images usingonly the names of object classes during testing, without the need for manualannotations. However, a critical challenge in current ZOC methods lies in theirinability to identify high-quality exemplars effectively. This deficiencyhampers scalability across diverse classes and undermines the development ofstrong visual associations between the identified classes and image content. Tothis end, we propose the Visual Association-based Zero-shot Object Counting(VA-Count) framework. VA-Count consists of an Exemplar Enhancement Module (EEM)and a Noise Suppression Module (NSM) that synergistically refine the process ofclass exemplar identification while minimizing the consequences of incorrectobject identification. The EEM utilizes advanced vision-language pretainingmodels to discover potential exemplars, ensuring the framework's adaptabilityto various classes. Meanwhile, the NSM employs contrastive learning todifferentiate between optimal and suboptimal exemplar pairs, reducing thenegative effects of erroneous exemplars. VA-Count demonstrates itseffectiveness and scalability in zero-shot contexts with superior performanceon two object counting datasets.|2024|ECCV|https://github.com/HopooLinZ/VA-Count|poster|未复现|
|238|SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments|Niklas Gard, Anna Hilsmann, Peter Eisert|https://arxiv.org/abs/2404.10527|In this paper, we present SPVLoc, a global indoor localization method thataccurately determines the six-dimensional (6D) camera pose of a query image andrequires minimal scene-specific prior knowledge and no scene-specific training.Our approach employs a novel matching procedure to localize the perspectivecamera's viewport, given as an RGB image, within a set of panoramic semanticlayout representations of the indoor environment. The panoramas are renderedfrom an untextured 3D reference model, which only comprises approximatestructural information about room shapes, along with door and windowannotations. We demonstrate that a straightforward convolutional networkstructure can successfully achieve image-to-panorama and ultimatelyimage-to-model matching. Through a viewport classification score, we rankreference panoramas and select the best match for the query image. Then, a 6Drelative pose is estimated between the chosen panorama and query image. Ourexperiments demonstrate that this approach not only efficiently bridges thedomain gap but also generalizes well to previously unseen scenes that are notpart of the training data. Moreover, it achieves superior localization accuracycompared to the state of the art methods and also estimates more degrees offreedom of the camera pose. Our source code is publicly available athttps://fraunhoferhhi.github.io/spvloc .|2024|ECCV|https://github.com/fraunhoferhhi/spvloc|Oral|未复现|
|239|Stereo Any Video: Temporally Consistent Stereo Matching|Junpeng Jing;Weixun Luo;Ye Mao;  Krystian Mikolajczyk|https://arxiv.org/html/2503.05549v1#S4|This paper introduces Stereo Any Video, a powerful framework for video stereomatching. It can estimate spatially accurate and temporally consistentdisparities without relying on auxiliary information such as camera poses oroptical flow. The strong capability is driven by rich priors from monocularvideo depth models, which are integrated with convolutional features to producestable representations. To further enhance performance, key architecturalinnovations are introduced: all-to-all-pairs correlation, which constructssmooth and robust matching cost volumes, and temporal convex upsampling, whichimproves temporal coherence. These components collectively ensure robustness,accuracy, and temporal consistency, setting a new standard in video stereomatching. Extensive experiments demonstrate that our method achievesstate-of-the-art performance across multiple datasets both qualitatively andquantitatively in zero-shot settings, as well as strong generalization toreal-world indoor and outdoor scenarios.|2025|ICCV|https://github.com/TomTomTommi/stereoanyvideo|highlight|未复现|
|240|InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity|Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, Xin Lu|https://arxiv.org/pdf/2503.16418|Achieving flexible and high-fidelity identity-preserved image generationremains formidable, particularly with advanced Diffusion Transformers (DiTs)like FLUX. We introduce InfiniteYou (InfU), one of the earliest robustframeworks leveraging DiTs for this task. InfU addresses significant issues ofexisting methods, such as insufficient identity similarity, poor text-imagealignment, and low generation quality and aesthetics. Central to InfU isInfuseNet, a component that injects identity features into the DiT base modelvia residual connections, enhancing identity similarity while maintaininggeneration capabilities. A multi-stage training strategy, including pretrainingand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample(SPMS) data, further improves text-image alignment, ameliorates image quality,and alleviates face copy-pasting. Extensive experiments demonstrate that InfUachieves state-of-the-art performance, surpassing existing baselines. Inaddition, the plug-and-play design of InfU ensures compatibility with variousexisting methods, offering a valuable contribution to the broader community.|2025|ICCV|https://github.com/bytedance/InfiniteYou|highlight|未复现|
|241|MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization|Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin|https://arxiv.org/abs/2408.02555|Meshes are the de facto 3D representation in the industry but arelabor-intensive to produce. Recently, a line of research has focused onautoregressively generating meshes. This approach processes meshes into asequence composed of vertices and then generates them vertex by vertex, similarto how a language model generates text. These methods have achieved somesuccess but still struggle to generate complex meshes. One primary reason forthis limitation is their inefficient tokenization methods. To address thisissue, we introduce MeshAnything V2, an advanced mesh generation model designedto create Artist-Created Meshes that align precisely with specified shapes. Akey innovation behind MeshAnything V2 is our novel Adjacent Mesh Tokenization(AMT) method. Unlike traditional approaches that represent each face usingthree vertices, AMT optimizes this by employing a single vertex whereverfeasible, effectively reducing the token sequence length by about half onaverage. This not only streamlines the tokenization process but also results inmore compact and well-structured sequences, enhancing the efficiency of meshgeneration. With these improvements, MeshAnything V2 effectively doubles theface limit compared to previous models, delivering superior performance withoutincreasing computational costs. We will make our code and models publiclyavailable. Project Page: https://buaacyw.github.io/meshanything-v2/|2025|ICCV|https://github.com/buaacyw/MeshAnythingV2|poster|未复现|
|242|From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers|Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang|https://arxiv.org/abs/2503.06923|Diffusion Transformers (DiT) have revolutionized high-fidelity image andvideo synthesis, yet their computational demands remain prohibitive forreal-time applications. To solve this problem, feature caching has beenproposed to accelerate diffusion models by caching the features in the previoustimesteps and then reusing them in the following timesteps. However, attimesteps with significant intervals, the feature similarity in diffusionmodels decreases substantially, leading to a pronounced increase in errorsintroduced by feature caching, significantly harming the generation quality. Tosolve this problem, we propose TaylorSeer, which firstly shows that features ofdiffusion models at future timesteps can be predicted based on their values atprevious timesteps. Based on the fact that features change slowly andcontinuously across timesteps, TaylorSeer employs a differential method toapproximate the higher-order derivatives of features and predict features infuture timesteps with Taylor series expansion. Extensive experimentsdemonstrate its significant effectiveness in both image and video synthesis,especially in high acceleration ratios. For instance, it achieves an almostlossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideowithout additional training. On DiT, it achieves $3.41$ lower FID compared withprevious SOTA at $4.53$$\times$ acceleration. %Our code is provided in thesupplementary materials and will be made publicly available on GitHub. Ourcodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer|2025|ICCV|https://github.com/Shenyi-Z/TaylorSeer|poster|未复现|
|243|MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion|Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, Yu-Gang Jiang|https://arxiv.org/abs/2405.20325|Despite impressive advancements in diffusion-based video editing models inaltering video attributes, there has been limited exploration into modifyingmotion information while preserving the original protagonist's appearance andbackground. In this paper, we propose MotionFollower, a lightweightscore-guided diffusion model for video motion editing. To introduce conditionalcontrols to the denoising process, MotionFollower leverages two of our proposedlightweight signal controllers, one for poses and the other for appearances,both of which consist of convolution blocks without involving heavy attentioncalculations. Further, we design a score guidance principle based on atwo-branch architecture, including the reconstruction and editing branches,which significantly enhance the modeling capability of texture details andcomplicated backgrounds. Concretely, we enforce several consistencyregularizers and losses during the score estimation. The resulting gradientsthus inject appropriate guidance to the intermediate latents, forcing the modelto preserve the original background details and protagonists' appearanceswithout interfering with the motion modification. Experiments demonstrate thecompetitive motion editing ability of MotionFollower qualitatively andquantitatively. Compared with MotionEditor, the most advanced motion editingmodel, MotionFollower achieves an approximately 80% reduction in GPU memorywhile delivering superior motion editing performance and exclusively supportinglarge camera movements and actions.|2025|ICCV|https://github.com/Francis-Rings/MotionFollower?tab=readme-ov-file|poster|未复现|
|244|AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation|Zijie Wu, Chaohui Yu, Fan Wang, Xiang Bai|https://arxiv.org/abs/2506.09982|Recent advances in 4D content generation have attracted increasing attention,yet creating high-quality animated 3D models remains challenging due to thecomplexity of modeling spatio-temporal distributions and the scarcity of 4Dtraining data. In this paper, we present AnimateAnyMesh, the first feed-forwardframework that enables efficient text-driven animation of arbitrary 3D meshes.Our approach leverages a novel DyMeshVAE architecture that effectivelycompresses and reconstructs dynamic mesh sequences by disentangling spatial andtemporal features while preserving local topological structures. To enablehigh-quality text-conditional generation, we employ a Rectified Flow-basedtraining strategy in the compressed latent space. Additionally, we contributethe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with textannotations. Experimental results demonstrate that our method generatessemantically accurate and temporally coherent mesh animations in a few seconds,significantly outperforming existing approaches in both quality and efficiency.Our work marks a substantial step forward in making 4D content creation moreaccessible and practical. All the data, code, and models will be open-released.|2025|ICCV|https://github.com/JarrentWu1031/AnimateAnyMesh|poster|未复现|
|245|VSSD: Vision Mamba with Non-Causal State Space Duality|Yuheng Shi, Minjing Dong, Mingjia Li, Chang Xu|https://arxiv.org/abs/2407.18559|Vision transformers have significantly advanced the field of computer vision,offering robust modeling capabilities and global receptive field. However,their high computational demands limit their applicability in processing longsequences. To tackle this issue, State Space Models (SSMs) have gainedprominence in vision tasks as they offer linear computational complexity.Recently, State Space Duality (SSD), an improved variant of SSMs, wasintroduced in Mamba2 to enhance model performance and efficiency. However, theinherent causal nature of SSD/SSMs restricts their applications in non-causalvision tasks. To address this limitation, we introduce Visual State SpaceDuality (VSSD) model, which has a non-causal format of SSD. Specifically, wepropose to discard the magnitude of interactions between the hidden state andtokens while preserving their relative weights, which relieves the dependenciesof token contribution on previous tokens. Together with the involvement ofmulti-scan strategies, we show that the scanning results can be integrated toachieve non-causality, which not only improves the performance of SSD in visiontasks but also enhances its efficiency. We conduct extensive experiments onvarious benchmarks including image classification, detection, and segmentation,where VSSD surpasses existing state-of-the-art SSM-based models. Code andweights are available at \url{https://github.com/YuHengsss/VSSD}.|2025|ICCV|https://github.com/YuHengsss/VSSD?tab=readme-ov-file|poster|未复现|
|246| Balanced Image Stylization with Style Matching Score|Yuxin Jiang, Liming Jiang, Shuai Yang, Jia-Wei Liu, Ivor Tsang, Mike Zheng Shou|https://arxiv.org/abs/2503.07601|We present Style Matching Score (SMS), a novel optimization method for imagestylization with diffusion models. Balancing effective style transfer withcontent preservation is a long-standing challenge. Unlike existing efforts, ourmethod reframes image stylization as a style distribution matching problem. Thetarget style distribution is estimated from off-the-shelf style-dependent LoRAsvia carefully designed score functions. To preserve content informationadaptively, we propose Progressive Spectrum Regularization, which operates inthe frequency domain to guide stylization progressively from low-frequencylayouts to high-frequency details. In addition, we devise a Semantic-AwareGradient Refinement technique that leverages relevance maps derived fromdiffusion semantic priors to selectively stylize semantically importantregions. The proposed optimization formulation extends stylization from pixelspace to parameter space, readily applicable to lightweight feedforwardgenerators for efficient one-step stylization. SMS effectively balances stylealignment and content preservation, outperforming state-of-the-art approaches,verified by extensive experiments.|2025|ICCV|https://github.com/showlab/SMS|poster|未复现|
|247|FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model|Yukang Cao、Chenyang Si、Jinghao Wang、Ziwei Liu|https://arxiv.org/html/2507.01953v1#S4|We present FreeMorph, the first tuning-free method for image morphing thataccommodates inputs with different semantics or layouts. Unlike existingmethods that rely on finetuning pre-trained diffusion models and are limited bytime constraints and semantic/layout discrepancies, FreeMorph delivershigh-fidelity image morphing without requiring per-instance training. Despitetheir efficiency and potential, tuning-free methods face challenges inmaintaining high-quality results due to the non-linear nature of the multi-stepdenoising process and biases inherited from the pre-trained diffusion model. Inthis paper, we introduce FreeMorph to address these challenges by integratingtwo key innovations. 1) We first propose a guidance-aware sphericalinterpolation design that incorporates explicit guidance from the input imagesby modifying the self-attention modules, thereby addressing identity loss andensuring directional transitions throughout the generated sequence. 2) Wefurther introduce a step-oriented variation trend that blends self-attentionmodules derived from each input image to achieve controlled and consistenttransitions that respect both inputs. Our extensive evaluations demonstratethat FreeMorph outperforms existing methods, being 10x ~ 50x faster andestablishing a new state-of-the-art for image morphing.|2025|ICCV|https://github.com/yukangcao/FreeMorph|poster|未复现|
|248|GENMO: A GENeralist Model for Human MOtion|Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan |https://arxiv.org/abs/2505.01425|Human motion modeling traditionally separates motion generation andestimation into distinct tasks with specialized models. Motion generationmodels focus on creating diverse, realistic motions from inputs like text,audio, or keyframes, while motion estimation models aim to reconstruct accuratemotion trajectories from observations like videos. Despite sharing underlyingrepresentations of temporal dynamics and kinematics, this separation limitsknowledge transfer between tasks and requires maintaining separate models. Wepresent GENMO, a unified Generalist Model for Human Motion that bridges motionestimation and generation in a single framework. Our key insight is toreformulate motion estimation as constrained motion generation, where theoutput motion must precisely satisfy observed conditioning signals. Leveragingthe synergy between regression and diffusion, GENMO achieves accurate globalmotion estimation while enabling diverse motion generation. We also introducean estimation-guided training objective that exploits in-the-wild videos with2D annotations and text descriptions to enhance generative diversity.Furthermore, our novel architecture handles variable-length motions and mixedmultimodal conditions (text, audio, video) at different time intervals,offering flexible control. This unified approach creates synergistic benefits:generative priors improve estimated motions under challenging conditions likeocclusions, while diverse video data enhances generation capabilities.Extensive experiments demonstrate GENMO's effectiveness as a generalistframework that successfully handles multiple human motion tasks within a singlemodel.|2025|ICCV|https://research.nvidia.com/labs/dair/genmo/|poster|未复现|
|249|Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers|Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, Wenhu Chen|https://arxiv.org/abs/2503.11579|State-of-the-art transformer-based large multimodal models (LMMs) struggle tohandle hour-long video inputs due to the quadratic complexity of the causalself-attention operations, leading to high computational costs during trainingand inference. Existing token compression-based methods reduce the number ofvideo tokens but often incur information loss and remain inefficient forextremely long sequences. In this paper, we explore an orthogonal direction tobuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks toencode video tokens with linear complexity. Without any token reduction, VAMBAcan encode more than 1024 frames (640$\times$360) on a single GPU, whiletransformer-based models can only encode 256 frames. On long video input, VAMBAachieves at least 50% reduction in GPU memory usage during training andinference, and nearly doubles the speed per training step compared totransformer-based LMMs. Our experimental results demonstrate that VAMBAimproves accuracy by 4.3% on the challenging hour-long video understandingbenchmark LVBench over prior efficient video LMMs, and maintains strongperformance on a broad spectrum of long and short video understanding tasks.|2025|ICCV|https://github.com/TIGER-AI-Lab/Vamba|poster|未复现|
|250|Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning|Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen|https://arxiv.org/abs/2506.02327|Providing effective treatment and making informed clinical decisions areessential goals of modern medicine and clinical care. We are interested insimulating disease dynamics for clinical decision-making, leveraging recentadvances in large generative models. To this end, we introduce the MedicalWorld Model (MeWM), the first world model in medicine that visually predictsfuture disease states based on clinical decisions. MeWM comprises (i)vision-language models to serve as policy models, and (ii) tumor generativemodels as dynamics models. The policy model generates action plans, such asclinical treatments, while the dynamics model simulates tumor progression orregression under given treatment conditions. Building on this, we propose theinverse dynamics model that applies survival analysis to the simulatedpost-treatment tumor, enabling the evaluation of treatment efficacy and theselection of the optimal clinical action plan. As a result, the proposed MeWMsimulates disease dynamics by synthesizing post-treatment tumors, withstate-of-the-art specificity in Turing tests evaluated by radiologists.Simultaneously, its inverse dynamics model outperforms medical-specialized GPTsin optimizing individualized treatment protocols across all metrics. Notably,MeWM improves clinical decision-making for interventional physicians, boostingF1-score in selecting the optimal TACE protocol by 13%, paving the way forfuture integration of medical world models as the second readers.|2025|ICCV|https://github.com/scott-yjyang/MeWM|poster|未复现|
|251|Where, What, Why: Towards Explainable Driver Attention Prediction|Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, Xiaobo Xia, Chao Gou|https://arxiv.org/pdf/2506.23088|Modeling task-driven attention in driving is a fundamental challenge for bothautonomous vehicles and cognitive science. Existing methods primarily predictwhere drivers look by generating spatial heatmaps, but fail to capture thecognitive motivations behind attention allocation in specific contexts, whichlimits deeper understanding of attention mechanisms. To bridge this gap, weintroduce Explainable Driver Attention Prediction, a novel task paradigm thatjointly predicts spatial attention regions (where), parses attended semantics(what), and provides cognitive reasoning for attention allocation (why). Tosupport this, we present W3DA, the first large-scale explainable driverattention dataset. It enriches existing benchmarks with detailed semantic andcausal annotations across diverse driving scenarios, including normalconditions, safety-critical situations, and traffic accidents. We furtherpropose LLada, a Large Language model-driven framework for driver attentionprediction, which unifies pixel modeling, semantic parsing, and cognitivereasoning within an end-to-end architecture. Extensive experiments demonstratethe effectiveness of LLada, exhibiting robust generalization across datasetsand driving conditions. This work serves as a key step toward a deeperunderstanding of driver attention mechanisms, with significant implications forautonomous driving, intelligent driver training, and human-computerinteraction.|2025|ICCV|https://github.com/yuchen2199/Explainable-Driver-Attention-Prediction|poster|未复现|
