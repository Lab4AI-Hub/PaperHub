<table>
    <tr>
        <td>ID</td>
        <td>æ¡ˆä¾‹ç±»å‹</td>
        <td>è®ºæ–‡åç§°</td>
        <td>è®ºæ–‡ä½œè€…</td>
        <td>è®ºæ–‡é“¾æ¥</td>
        <td>è®ºæ–‡å¹´ä»½</td>
        <td>æ¥æºæ ‡ç­¾<br>ï¼ˆä¼šè®®æœŸåˆŠï¼‰</td>
        <td>githubé“¾æ¥</td>
        <td>çŠ¶æ€</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>1</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Attention is all you need</td>
        <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ï¾…ã€Œkasz Kaiser, Illia Polosukhin</td>
        <td>https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</td>
        <td>2017</td>
        <td>NIPS</td>
        <td>https://github.com/jadore801120/attention-is-all-you-need-pytorch</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>2</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Improving language understanding by generative pre-training</td>
        <td>Alec Radfordï¼ŒKarthik Narasimhanï¼ŒTim Salimansï¼ŒIlya Sutskever</td>
        <td>https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</td>
        <td>2018</td>
        <td>OpenAI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>3</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
        <td>Jacob Devlin,Â Ming-Wei Chang,Â Kenton Lee,Â Kristina Toutanova</td>
        <td>https://aclanthology.org/N19-1423/?utm_campaign=The+Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC</td>
        <td>2019</td>
        <td>NAACL</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>4</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Language models are unsupervised multitask learners</td>
        <td>Alec Radfordï¼ŒJeffrey Wuï¼ŒRewon Childï¼ŒDavid Luanï¼ŒDario Amodeiï¼ŒIlya Sutskever</td>
        <td>https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</td>
        <td>2019</td>
        <td>OpenAI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>5</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td>
        <td>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu</td>
        <td>https://www.jmlr.org/papers/v21/20-074.html</td>
        <td>2020</td>
        <td>JMLR</td>
        <td>https://github.com/google-research/text-to-text-transfer-transformer</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>6</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>From Local to Global: A GraphRAG Approach to<br>Query-Focused Summarization</td>
        <td>Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson</td>
        <td>https://arxiv.org/abs/2404.16130</td>
        <td>2024</td>
        <td></td>
        <td>https://github.com/microsoft/graphrag</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>7</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Language Models are Few-Shot Learners</td>
        <td>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei</td>
        <td>https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&amp;utm_medium=email&amp;utm_campaign=linkedin_newsletter</td>
        <td>2020</td>
        <td>NIPS</td>
        <td>https://github.com/openai/gpt-3</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>8</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>LoRA: Low-Rank Adaptation of Large Language Models</td>
        <td>Edward J Hu,yelong shen,Phillip Wallis,Zeyuan Allen-Zhu,Yuanzhi Li,Shean Wang,Lu Wang,Weizhu Chen</td>
        <td>https://openreview.net/forum?id=nZeVKeeFYf9</td>
        <td>2022</td>
        <td>ICLR</td>
        <td>https://github.com/microsoft/LoRA</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>9</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Finetuned Language Models are Zero-Shot Learners</td>
        <td>Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,Adams Wei Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V Le</td>
        <td>https://openreview.net/forum?id=gEZrGCozdqR&amp;ref=morioh.com&amp;utm_source=morioh.com</td>
        <td>2022</td>
        <td>ICLR</td>
        <td>https://github.com/google-research/FLAN</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>10</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>LLaMA: Open and Efficient Foundation Language Models</td>
        <td>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample</td>
        <td>https://arxiv.org/abs/2302.13971</td>
        <td>2023</td>
        <td></td>
        <td>https://github.com/meta-llama/llama</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>11</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
        <td>Xuezhi Wang,Jason Wei,Dale Schuurmans,Quoc V Le,Ed H. Chi,Sharan Narang,Aakanksha Chowdhery,Denny Zhou</td>
        <td>https://openreview.net/forum?id=1PL1NIMMrw&amp;utm_source=chatgpt.com</td>
        <td>2023</td>
        <td>ICLR</td>
        <td>https://github.com/dj-sorry/self_consistency</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>12</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers</td>
        <td>Damai Dai,Â Yutao Sun,Â Li Dong,Â Yaru Hao,Â Shuming Ma,Â Zhifang Sui,Â Furu Wei</td>
        <td>https://aclanthology.org/2023.findings-acl.247/</td>
        <td>2023</td>
        <td>ACL</td>
        <td>https://github.com/microsoft/LMOps/tree/main/understand_icl</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>13</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
        <td>Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom</td>
        <td>https://proceedings.neurips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html</td>
        <td>2023</td>
        <td>NIPS</td>
        <td>https://github.com/lucidrains/toolformer-pytorch</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>14</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</td>
        <td>Eric Mitchell,Â Yoonho Lee,Â Alexander Khazatsky,Â Christopher D Manning,Â Chelsea Finn</td>
        <td>https://proceedings.mlr.press/v202/mitchell23a.html</td>
        <td>2023</td>
        <td>PMLR</td>
        <td>https://github.com/eric-mitchell/detect-gpt</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>15</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Recitation-augmented language models</td>
        <td>Zhiqing Sun,Xuezhi Wang,Yi Tay,Yiming Yang,Denny Zhou</td>
        <td>https://openreview.net/forum?id=-cqvvvb-NkI</td>
        <td>2023</td>
        <td>ICLR</td>
        <td>https://github.com/Edward-Sun/RECITE</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>16</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Self-Instruct: Aligning Language Models with Self-Generated Instructions</td>
        <td>Yizhong Wang,Â Yeganeh Kordi,Â Swaroop Mishra,Â Alisa Liu,Â Noah A. Smith,Â Daniel Khashabi,Â Hannaneh Hajishirzi</td>
        <td>https://aclanthology.org/2023.acl-long.754/</td>
        <td>2023</td>
        <td>ACL</td>
        <td>https://github.com/yizhongw/self-instruct</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>17</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Automatic chain of thought prompting in large language models</td>
        <td>Zhuosheng Zhang,Aston Zhang,Mu Li,Alex Smola</td>
        <td>https://openreview.net/forum?id=5NTt8GFjUHkr</td>
        <td>2023</td>
        <td>ICLR</td>
        <td>https://github.com/amazon-science/auto-cot</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>18</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>REALM: retrieval-augmented language model pre-training</td>
        <td>KelvinÂ Guu,Â KentonÂ Lee,Â ZoraÂ Tung,Â PanupongÂ Pasupat,Â Ming-WeiÂ Chang</td>
        <td>https://dl.acm.org/doi/abs/10.5555/3524938.3525306</td>
        <td>2020</td>
        <td>ICML</td>
        <td>https://github.com/google-research/language/tree/master/language/realm</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>19</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Language Is Not All You Need: Aligning Perception with Language Models</td>
        <td>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Nils Bjorck, Vishrav Chaudhary, Subhojit Som, XIA SONG, Furu Wei</td>
        <td>https://proceedings.neurips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html</td>
        <td>2023</td>
        <td>NIPS</td>
        <td>https://github.com/microsoft/unilm/tree/master/unilm-v1</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>20</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>
        <td>Kashun Shum,Â Shizhe Diao,Â Tong Zhang</td>
        <td>https://aclanthology.org/2023.findings-emnlp.811/</td>
        <td>2023</td>
        <td>EMNLP</td>
        <td>https://github.com/SHUMKASHUN/Automate-CoT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>21</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching</td>
        <td>Dingwen Zhang, Wenyuan Zeng, Guangyu Guo, Chaowei Fang, Lechao Cheng, Ming-Ming Cheng, Junwei Han</td>
        <td>https://arxiv.org/abs/2112.09459</td>
        <td>2025</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>22</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning</td>
        <td>Chun-Mei Feng, Kai Yu, Xinxing Xu, Salman Khan, Rick Siow Mong Goh, Wangmeng Zuo, Yong Liu</td>
        <td>https://arxiv.org/abs/2506.10575</td>
        <td>2024</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>23</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Segment Concealed Objects with Incomplete Supervision</td>
        <td>Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, Youwei Pang, Longxiang Tang, Chengyu Fang, Yulun Zhang, Linghe Kong, Xiu Li, Sina Farsiu</td>
        <td>https://arxiv.org/pdf/2506.08955</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td>https://github.com/ChunmingHe/SEE</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>24</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Event-based Stereo Depth Estimation: A Survey</td>
        <td>Suman Ghosh, Guillermo Gallego</td>
        <td>https://arxiv.org/abs/2409.17680</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>25</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Efficient Low-Resolution Face Recognition via Bridge Distillation</td>
        <td>Shiming Ge, Shengwei Zhao, Chenyu Li, Yu Zhang, Jia Li</td>
        <td>https://arxiv.org/abs/2409.11786</td>
        <td>2024</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>26</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning</td>
        <td>Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin</td>
        <td>https://arxiv.org/abs/2403.05770</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td>https://github.com/YicongHong/Recurrent-VLN-BERT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>27</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning</td>
        <td>Wei Tan, Lan Du, Wray Buntine</td>
        <td>https://arxiv.org/abs/2312.10116</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>28</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Paragraph-to-Image Generation with Information-Enriched Diffusion Model</td>
        <td>Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang</td>
        <td>https://arxiv.org/abs/2311.14284</td>
        <td>2025</td>
        <td>IJCV</td>
        <td>https://github.com/weijiawu/ParaDiffusion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>29</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory</td>
        <td>Danpei Zhao, Bo Yuan, Zhenwei Shi</td>
        <td>https://arxiv.org/abs/2309.15413</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>30</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Dual Compensation Residual Networks for Class Imbalanced Learning</td>
        <td>Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen</td>
        <td>https://arxiv.org/abs/2308.13165</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>31</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>End-to-end Alternating Optimization for Real-World Blind Super Resolution</td>
        <td>Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan</td>
        <td>https://arxiv.org/abs/2308.08816</td>
        <td>2023</td>
        <td>IJCV</td>
        <td>https://github.com/greatlog/RealDAN</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>32</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection</td>
        <td>Yuming Chen, Xinbin Yuan, Jiabao Wang, Ruiqi Wu, Xiang Li, Qibin Hou, Ming-Ming Cheng</td>
        <td>https://arxiv.org/abs/2308.05480</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td>https://github.com/FishAndWasabi/YOLO-MS</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>33</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection</td>
        <td>Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I. Webb, Irwin King, Shirui Pan</td>
        <td>https://arxiv.org/abs/2307.03759</td>
        <td>2024</td>
        <td>TPAMI</td>
        <td>https://github.com/KimMeen/Awesome-GNN4TS</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>34</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SplatFlow: Learning Multi-frame Optical Flow via Splatting</td>
        <td>Bo Wang, Yifan Zhang, Jian Li, Yang Yu, Zhenping Sun, Li Liu, Dewen Hu</td>
        <td>https://arxiv.org/abs/2306.08887</td>
        <td>2024</td>
        <td>IJCV</td>
        <td>https://github.com/wwsource/SplatFlow</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>35</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Towards Expressive Spectral-Temporal Graph Neural Networks for Time Series Forecasting</td>
        <td>Ming Jin, Guangsi Shi, Yuan-Fang Li, Bo Xiong, Tian Zhou, Flora D. Salim, Liang Zhao, Lingfei Wu, Qingsong Wen, Shirui Pan</td>
        <td>https://arxiv.org/abs/2305.06587</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>36</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Efficient Halftoning via Deep Reinforcement Learning</td>
        <td>Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Li Ding, Liang Chen, Kai Huang</td>
        <td>https://arxiv.org/abs/2304.12152</td>
        <td>2023</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>37</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison</td>
        <td>Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters</td>
        <td>https://arxiv.org/abs/2211.16110</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>38</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Salient Object Detection via Dynamic Scale Routing</td>
        <td>Zhenyu Wu, Shuai Li, Chenglizhao Chen, Hong Qin, Aimin Hao</td>
        <td>https://arxiv.org/abs/2210.13821</td>
        <td>2022</td>
        <td>TIP</td>
        <td>https://github.com/wuzhenyubuaa/DPNet</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>39</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Twin Contrastive Learning for Online Clustering</td>
        <td>Yunfan Li, Mouxing Yang, Dezhong Peng, Taihao Li, Jiantao Huang, Xi Peng</td>
        <td>https://arxiv.org/abs/2210.11680</td>
        <td>2022</td>
        <td>IJCV</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>40</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Kernel-Based Generalized Median Computation for Consensus Learning</td>
        <td>Andreas NienkÃ¶tter, Xiaoyi Jiang</td>
        <td>https://arxiv.org/abs/2209.10208</td>
        <td>2022</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>41</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>A Tale of HodgeRank and Spectral Method: Target Attack Against Rank Aggregation Is the Fixed Point of Adversarial Game</td>
        <td>Ke Ma, Qianqian Xu, Jinshan Zeng, Guorong Li, Xiaochun Cao, Qingming Huang</td>
        <td>https://arxiv.org/abs/2209.05742</td>
        <td>2022</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>42</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Boosting Night-time Scene Parsing with Learnable Frequency</td>
        <td>Zhifeng Xie, Sen Wang, Ke Xu, Zhizhong Zhang, Xin Tan, Yuan Xie, Lizhuang Ma</td>
        <td>https://arxiv.org/abs/2208.14241</td>
        <td>2023</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>43</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SiamMask: A Framework for Fast Online Object Tracking and Segmentation</td>
        <td>Weiming Hu, Qiang Wang, Li Zhang, Luca Bertinetto, Philip H. S. Torr</td>
        <td>https://arxiv.org/abs/2207.02088</td>
        <td>2022</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>44</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SERE: Exploring Feature Self-relation for Self-supervised Transformer</td>
        <td>Zhong-Yu Li, Shanghua Gao, Ming-Ming Cheng</td>
        <td>https://arxiv.org/abs/2206.05184</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td>https://github.com/MCG-NKU/SERE</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>45</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis</td>
        <td>Maciej Besta, Torsten Hoefler</td>
        <td>https://arxiv.org/abs/2205.09702</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>46</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network</td>
        <td>Dasong Li, Yi Zhang, Ka Lung Law, Xiaogang Wang, Hongwei Qin, Hongsheng Li</td>
        <td>https://arxiv.org/abs/2205.04721</td>
        <td>2022</td>
        <td>IJCV</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>47</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Incomplete Gamma Kernels: Generalizing Locally Optimal Projection Operators</td>
        <td>Patrick Stotko, Michael Weinmann, Reinhard Klein</td>
        <td>https://arxiv.org/abs/2205.01087</td>
        <td>2024</td>
        <td>TPAMI</td>
        <td>https://github.com/stotko/incomplete-gamma-kernels</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>48</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>From Slow Bidirectional to Fast Autoregressive Video Diffusion Models</td>
        <td>Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</td>
        <td>https://arxiv.org/abs/2412.07772</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/tianweiy/CausVid</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>49</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention</td>
        <td>Guangxuan Xiao, Tianwei Yin, William T. Freeman, FrÃ©do Durand, Song Han</td>
        <td>https://arxiv.org/abs/2305.10431</td>
        <td>2024</td>
        <td>IJCV</td>
        <td>https://github.com/mit-han-lab/fastcomposer</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>50</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>ROGRAG: A Robustly Optimized GraphRAG Framework</td>
        <td>Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong</td>
        <td>https://arxiv.org/abs/2503.06474</td>
        <td>2025</td>
        <td>ACL</td>
        <td>https://github.com/tpoisonooo/ROGRAG</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>51</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images</td>
        <td>Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Aysegul Dundar</td>
        <td>https://arxiv.org/abs/2409.20530</td>
        <td>2024</td>
        <td>NIPS</td>
        <td>berkegokmen1/dual-enc-3d-gan-inversion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>52</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Can We Leave Deepfake Data Behind in Training Deepfake Detector</td>
        <td>Jikang Cheng, Zhiyuan Yan, Ying Zhang, Yuhao Luo, Zhongyuan Wang, Chen Li</td>
        <td>https://arxiv.org/pdf/2408.17052</td>
        <td>2024</td>
        <td>NIPS</td>
        <td>https://github.com/beautyremain/ProDet</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>53</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</td>
        <td>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</td>
        <td>https://arxiv.org/abs/2305.18500</td>
        <td>2023</td>
        <td>NIPS</td>
        <td>https://github.com/TXH-mercury/VAST</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>54</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</td>
        <td>Riku Murai, Eric Dexheimer, Andrew J. Davison</td>
        <td>https://arxiv.org/abs/2412.12392</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/rmurai0610/MASt3R-SLAM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>55</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM</td>
        <td>Vladimir Yugay, Theo Gevers, Martin R. Oswald</td>
        <td>https://arxiv.org/abs/2411.16785</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/VladimirYugay/MAGiC-SLAM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>56</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Murre: Multi-view Reconstruction via SfM-guided Monocular Depth Estimation</td>
        <td>Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao</td>
        <td>https://arxiv.org/abs/2503.14483</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zju3dv/Murre</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>57</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</td>
        <td>Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, Ying Tai</td>
        <td>https://arxiv.org/abs/2501.02976</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/NJU-PCALab/STAR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>58</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models</td>
        <td>Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee</td>
        <td>https://arxiv.org/abs/2403.09055</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ironjr/semantic-draw</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>59</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>HSMR: Reconstructing Humans with a Biomechanically Accurate Skeleton</td>
        <td>Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos</td>
        <td>https://arxiv.org/abs/2503.21751</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/IsshikiHugh/HSMR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>60</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness</td>
        <td>Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun</td>
        <td>https://arxiv.org/abs/2405.17220</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/RLHF-V/RLAIF-V</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>61</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DFormerï¼šRethinking RGBD Representation Learning for Semantic Segmentation</td>
        <td>Bowen Yin, Xuying Zhang, Zhongyu Li, Li Liu, Ming-Ming Cheng, Qibin Hou</td>
        <td>https://arxiv.org/abs/2309.09668</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/VCIP-RGBD/DFormer</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>62</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation</td>
        <td>Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</td>
        <td>https://arxiv.org/abs/2406.06526</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/hzxie/GaussianCity</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>63</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos</td>
        <td>Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li</td>
        <td>https://arxiv.org/abs/2503.17973</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Jianghanxiao/PhysTwin</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>64</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</td>
        <td>Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu</td>
        <td>https://arxiv.org/abs/2503.10630</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/bagh2178/UniGoal</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>65</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction</td>
        <td>Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang</td>
        <td>https://arxiv.org/abs/2412.04887</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Jixuan-Fan/Momentum-GS</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>66</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MINIMA: Modality Invariant Image Matching</td>
        <td>Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai</td>
        <td>https://arxiv.org/abs/2412.19412</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LSXI7/MINIMA</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>67</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video</td>
        <td>David Yifan Yao, Albert J. Zhai, Shenlong Wang</td>
        <td>https://arxiv.org/abs/2503.21761</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Davidyao99/uni4d</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>68</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models</td>
        <td>Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang</td>
        <td>https://arxiv.org/abs/2501.18590</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/52CV/CVPR-2025-Papers</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>69</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Linear Programming Bounds on k-Uniform States</td>
        <td>Yu Ning, Fei Shi, Tao Luo, Xiande Zhang</td>
        <td>https://arxiv.org/abs/2503.02222</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Epona-World-Model/Epona</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>70</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</td>
        <td>Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, Ziwei Liu</td>
        <td>https://arxiv.org/abs/2402.05054</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/3DTopia/LGM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>71</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VideoMamba: State Space Model for Efficient Video Understanding</td>
        <td>Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao</td>
        <td>https://arxiv.org/abs/2403.06977</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/OpenGVLab/video-mamba-suite</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>72</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DriveLM: Driving with Graph Visual Question Answering</td>
        <td>Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens BeiÃŸwenger, Ping Luo, Andreas Geiger, Hongyang Li</td>
        <td>https://arxiv.org/abs/2312.14150</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/OpenDriveLab/DriveLM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>73</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>GRiT: A Generative Region-to-text Transformer for Object Understanding</td>
        <td>Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang</td>
        <td>https://arxiv.org/abs/2212.00280</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/JialianW/GRiT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>74</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PointLLM: Empowering Large Language Models to Understand Point Clouds</td>
        <td>Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin</td>
        <td>https://arxiv.org/abs/2308.16911</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/OpenRobotLab/PointLLM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>75</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding</td>
        <td>Benjin Zhu, Zhe Wang, and Hongsheng Li</td>
        <td>https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00730.pdf</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>/</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>76</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Adversarial Diffusion Distillation</td>
        <td>Axel Sauer,Â Dominik Lorenz,Â Andreas Blattmann,Â Robin Rombach</td>
        <td>https://arxiv.org/abs/2311.17042</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/AMD-AIG-AIMA/AMD-Diffusion-Distillation</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>77</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Generative Image Dynamics</td>
        <td>Zhengqi Li,Â Richard Tucker,Â Noah Snavely,Â Aleksander Holynski</td>
        <td>https://arxiv.org/abs/2309.07906</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://generative-dynamics.github.io/</td>
        <td>æœ€ä½³è®ºæ–‡</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>78</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Rich Human Feedback for Text-to-Image Generation</td>
        <td>Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai J Kohlhoff, Deepak Ramachandran, Vidhya Navalpakkam</td>
        <td>https://arxiv.org/pdf/2312.10240</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/youweiliang/RichHF</td>
        <td>æœ€ä½³è®ºæ–‡</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>79</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Mip-Splatting: Alias-free 3D Gaussian Splatting</td>
        <td>Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger</td>
        <td>https://arxiv.org/abs/2311.16493</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/autonomousvision/mip-splatting</td>
        <td>æœ€ä½³å­¦ç”Ÿè®ºæ–‡</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>80</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>BioCLIP: A Vision Foundation Model for the Tree of Life</td>
        <td>Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su</td>
        <td>https://arxiv.org/abs/2311.18803</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Imageomics/bioclip</td>
        <td>æœ€ä½³å­¦ç”Ÿè®ºæ–‡</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>81</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</td>
        <td>Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</td>
        <td>https://arxiv.org/abs/2310.08528</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/hustvl/4DGaussians</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>82</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Depth Anything: Unleashing The Power of Large-Scale Unlabeled Data</td>
        <td>Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao</td>
        <td>https://arxiv.org/abs/2401.10891</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LiheYoung/Depth-Anything</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>83</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>LISA: Reasoning Segmentation Via Large Language Model</td>
        <td>Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia</td>
        <td>https://arxiv.org/abs/2308.00692</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/dvlab-research/LISA</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>84</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</td>
        <td>Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai</td>
        <td>https://arxiv.org/abs/2312.14238</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/OpenGVLab/InternVL</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>85</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark</td>
        <td>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen</td>
        <td>https://arxiv.org/abs/2311.16502</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/MMMU-Benchmark/MMMU</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>86</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</td>
        <td>Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra</td>
        <td>https://arxiv.org/abs/2312.00863</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/yformer/EfficientSAM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>87</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)</td>
        <td>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee</td>
        <td>https://arxiv.org/abs/2310.03744</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LLaVA-VL/LLaVA-NeXT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>88</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DemoFusion: Democratising High-Resolution Image Generation With No $$$</td>
        <td>Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma</td>
        <td>https://arxiv.org/abs/2311.16973</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/PRIS-CV/DemoFusion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>89</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</td>
        <td>Lukas HÃ¶llein,Â AljaÅ¾ BoÅ¾iÄ,Â Norman MÃ¼ller,Â David Novotny,Â Hung-Yu Tseng,Â Christian Richardt,Â Michael ZollhÃ¶fer,Â Matthias NieÃŸner</td>
        <td>https://arxiv.org/abs/2403.01807</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/ViewDiff</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>90</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</td>
        <td>Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andre Araujo</td>
        <td>https://arxiv.org/abs/2405.12979</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/google-research/omniglue</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>91</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks</td>
        <td>Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, Lianwen Jin</td>
        <td>https://arxiv.org/abs/2405.04408</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ZZZHANG-jx/DocRes</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>92</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</td>
        <td>Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel</td>
        <td>https://arxiv.org/abs/2311.17049</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/apple/ml-mobileclip</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>93</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Describing Differences in Image Sets with Natural Language</td>
        <td>Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy</td>
        <td>https://arxiv.org/abs/2312.02974</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Understanding-Visual-Datasets/VisDiff</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>94</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>XFeat: Accelerated Features for Lightweight Image Matching</td>
        <td>Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, Erickson R. Nascimento</td>
        <td>https://arxiv.org/abs/2404.19174</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/verlab/accelerated_features</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>95</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction</td>
        <td>David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</td>
        <td>https://arxiv.org/abs/2312.12337</td>
        <td>2024</td>
        <td>CVPR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>96</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>GPT4Point: A Unified Framework for Point-Language Understanding and Generation</td>
        <td>Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao</td>
        <td>https://arxiv.org/abs/2312.02980</td>
        <td>2024</td>
        <td>CVPR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>97</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</td>
        <td>Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan</td>
        <td>https://arxiv.org/abs/2311.06242</td>
        <td>2024</td>
        <td>CVPR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>98</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Identity-Preserving Text-to-Video Generation by Frequency Decomposition</td>
        <td>Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, Li Yuan</td>
        <td>https://arxiv.org/abs/2411.17440</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/PKU-YuanGroup/ConsisID</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>99</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models</td>
        <td>Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Yuan-Fang Li, Cunjian Chen, Yu Qiao</td>
        <td>https://arxiv.org/abs/2407.15642</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/maxin-cn/Cinemo</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>100</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>X-Dyna: Expressive Dynamic Human Image Animation</td>
        <td>Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, Mohammad Soleymani</td>
        <td>https://arxiv.org/abs/2501.10021</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/bytedance/X-Dyna</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>101</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation</td>
        <td>Qiyao Xue, Xiangyu Yin, Boyuan Yang, Wei Gao</td>
        <td>https://arxiv.org/pdf/2412.00596</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/pittisl/PhyT2V</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>102</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Timestep Embedding Tells: It&#39;s Time to Cache for Video Diffusion Model</td>
        <td>Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan</td>
        <td>https://arxiv.org/abs/2411.19108</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ali-vilab/TeaCache</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>103</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion</td>
        <td>Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, Jing Liu</td>
        <td>https://arxiv.org/abs/2503.07418</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/iva-mzsun/AR-Diffusion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>104</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Number it: Temporal Grounding Videos like Flipping Manga</td>
        <td>Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, Xu Yang</td>
        <td>https://arxiv.org/abs/2411.10332</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/yongliang-wu/NumPro</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>105</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing</td>
        <td>Hanhui Wang, Yihua Zhang, Ruizheng Bai, Yue Zhao, Sijia Liu, Zhengzhong Tu</td>
        <td>https://arxiv.org/abs/2411.16832</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/taco-group/FaceLock</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>106</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>h-Edit: Effective and Flexible Diffusion-Based Editing via Doobâ€™s h-Transform</td>
        <td>Toan Nguyen, Kien Do, Duc Kieu, Thin Nguyen</td>
        <td>https://arxiv.org/abs/2503.02187</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/nktoan/h-edit</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>107</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels</td>
        <td>Meng Lou, Yizhou Yu</td>
        <td>https://arxiv.org/abs/2502.20087</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LMMMEng/OverLoCK</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>108</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space</td>
        <td>Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan</td>
        <td>https://arxiv.org/pdf/2503.09419</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/SingleZombie/AFLDM</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>109</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>3D Student Splatting and Scooping</td>
        <td>Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang</td>
        <td>https://arxiv.org/abs/2503.10148</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/realcrane/3D-student-splating-and-scooping</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>110</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</td>
        <td>Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell</td>
        <td>https://arxiv.org/pdf/2412.12093</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/felixtaubner/cap4d/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>111</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Multi-view Reconstruction via SfM-guided Monocular Depth Estimation</td>
        <td>Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao</td>
        <td>https://arxiv.org/pdf/2503.14483</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zju3dv/Murre</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>112</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models</td>
        <td>Zhejun Zhang, Peter Karkus, Maximilian Igl, Wenhao Ding, Yuxiao Chen, Boris Ivanovic, Marco Pavone</td>
        <td>https://arxiv.org/pdf/2412.05334</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/NVlabs/catk</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>113</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>CustAny: Customizing Anything from A Single Example</td>
        <td>Lingjie Kong, Kai Wu, Xiaobin Hu, Wenhui Han, Jinlong Peng, Chengming Xu, Donghao Luo, Mengtian Li, Jiangning Zhang, Chengjie Wang, Yanwei Fu</td>
        <td>https://arxiv.org/pdf/2406.11643v4</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LingjieKong-fdu/CustAny</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>114</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VGGT:Visual Geometry Grounded Transformer</td>
        <td>Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny</td>
        <td>https://arxiv.org/pdf/2503.11651</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/vggt</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>115</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Navigation World Models</td>
        <td>Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun</td>
        <td>https://arxiv.org/pdf/2412.03572</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/nwm/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>116</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos</td>
        <td>Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, Noah Snavely</td>
        <td>https://arxiv.org/pdf/2412.04463</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/mega-sam/mega-sam</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>117</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>FoundationStereo: Zero-Shot Stereo Matching</td>
        <td>Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield</td>
        <td>https://arxiv.org/pdf/2501.09898</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/NVlabs/FoundationStereo/</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>118</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition</td>
        <td>Otto Brookes, Maksim Kukushkin, Majid Mirmehdi, Colleen Stephens, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus ZuberbÃ¼hler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar KÃ¼hl, Tilo Burghardt</td>
        <td>https://arxiv.org/pdf/2502.21201</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://obrookes.github.io/panaf-fgbg.github.io/</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>119</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing</td>
        <td>Bingliang Zhang, Wenda Chu, Julius Berner, Chenlin Meng, Anima Anandkumar, Yang Song</td>
        <td>https://arxiv.org/pdf/2407.01521</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zhangbingliang2019/DAPS</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>120</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MV-DUSt3R+: Single-StageSceneReconstruction fromSparseViewsIn2Seconds</td>
        <td>Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan</td>
        <td>https://arxiv.org/pdf/2412.06974</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/mvdust3r</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>121</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</td>
        <td>Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</td>
        <td>https://arxiv.org/pdf/2503.01774</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/nv-tlabs/Difix3D</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>122</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DIFFUSIONRENDERER: Neural Inverse and Forward Rendering with Video Diffusion Models</td>
        <td>Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang</td>
        <td>https://arxiv.org/pdf/2501.18590</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>123</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation</td>
        <td>Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao, Kaipeng Zhang</td>
        <td>https://arxiv.org/pdf/2411.18499</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LanceZPF/OpenING</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>124</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>RandAR: Decoder-only Autoregressive Visual Generation in Random Orders</td>
        <td>Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang</td>
        <td>https://arxiv.org/pdf/2412.01827</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ziqipang/RandAR</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>125</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</td>
        <td>Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, Yueting Zhuang</td>
        <td>https://arxiv.org/pdf/2411.15738</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/DCDmllm/AnyEdit</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>126</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</td>
        <td>Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, Si Liu</td>
        <td>https://arxiv.org/pdf/2411.14794</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/hshjerry/VideoEspresso</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>127</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images</td>
        <td>Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang</td>
        <td>https://arxiv.org/pdf/2410.01768</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/likyoo/SegEarth-OV</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>128</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Minority-Focused Text-to-Image Generation via Prompt Optimization</td>
        <td>Soobin Um, Jong Chul Ye</td>
        <td>https://arxiv.org/pdf/2410.07838</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/soobin-um/MinorityPrompt</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>129</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Autoregressive Distillation of Diffusion Transformers</td>
        <td>Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar SchÃ¶nfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu</td>
        <td>https://arxiv.org/pdf/2504.11295</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/alsdudrla10/ARD</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>130</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Molmo and PixMo:Open Weights and Open Data for State-of-the-Art Vision-Language Mo</td>
        <td>Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi</td>
        <td>https://arxiv.org/pdf/2409.17146</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/allenai/molmo</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>131</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Continuous 3D Perception Model with Persistent State</td>
        <td>Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa</td>
        <td>https://arxiv.org/pdf/2501.12387</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/CUT3R/CUT3R</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>132</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</td>
        <td>Zicheng Zhang, Tengchuan Kou, Shushi Wang, Chunyi Li, Wei Sun, Wei Wang, Xiaoyu Li, Zongyu Wang, Xuezhi Cao, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai</td>
        <td>https://arxiv.org/pdf/2503.02357</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zzc-1998/Q-Eval</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>133</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Video-XL:Extra-Long Vision Language Model for Hour-Scale Video Understanding</td>
        <td>Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao</td>
        <td>https://arxiv.org/pdf/2409.14485</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/VectorSpaceLab/Video-XL</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>134</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</td>
        <td>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu</td>
        <td>https://arxiv.org/pdf/2501.08331</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Eyeline-Research/Go-with-the-Flow</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>135</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>CleanDIFT: Diffusion Features without Noise</td>
        <td>Nick Stracke, Stefan Andreas Baumann, Kolja Bauer, Frank Fundel, BjÃ¶rn Ommer</td>
        <td>https://arxiv.org/pdf/2412.03439</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/CompVis/cleandift</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>136</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner</td>
        <td>Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</td>
        <td>https://arxiv.org/pdf/2405.14979</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/wyysf-98/CraftsMan3D</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>137</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DreamRelation: Bridging Customization and Relation Generation</td>
        <td>Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li</td>
        <td>https://arxiv.org/pdf/2410.23280</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Shi-qingyu/DreamRelation</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>138</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens</td>
        <td>Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, Hanwang Zhang</td>
        <td>https://arxiv.org/pdf/2504.14666</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/selftok-team/SelftokTokenizer/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>139</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion</td>
        <td>Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, Liang Pan, Dahua Lin, Ziwei Liu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/3DTopia/3DTopia-XL</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>140</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</td>
        <td>Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu</td>
        <td>https://arxiv.org/abs/2412.14123</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/gastruc/AnySat</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>141</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Cross-modal Causal Relation Alignment for Video Question Grounding</td>
        <td>Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin</td>
        <td>https://arxiv.org/abs/2503.07635</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/WissingChen/CRA-GQA</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>142</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</td>
        <td>Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan</td>
        <td>https://arxiv.org/abs/2409.02095</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Tencent/DepthCrafter</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>143</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery</td>
        <td>Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang</td>
        <td>https://arxiv.org/abs/2503.16964</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/BITyia/DroneSplat</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>144</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility</td>
        <td>Yidi Li, Jun Xiao, Zhengda Lu, Yiqun Wang, Haiyong Jiang</td>
        <td>https://arxiv.org/abs/2505.21377</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/chenxinl/Dream3DVG</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>145</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</td>
        <td>Jie Tian, Xiaoye Qu, Zhenyi Lu, Wei Wei, Sichen Liu, Yu Cheng</td>
        <td>https://arxiv.org/abs/2503.00948</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Chuge0335/EDG</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>146</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>ETAP: Event-based Tracking of Any Point</td>
        <td>Friedhelm Hamann, Daniel Gehrig, Filbert Febryanto, Kostas Daniilidis, Guillermo Gallego</td>
        <td>https://arxiv.org/abs/2412.00133</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/tub-rip/ETAP</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>147</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality</td>
        <td>Liyan Chen, Gregory P. Meyer, Zaiwei Zhang, Eric M. Wolff, Paul Vernaza</td>
        <td>https://arxiv.org/abs/2412.16481</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/liyanc/Flash3DTransformer</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>148</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution</td>
        <td>Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh</td>
        <td>https://arxiv.org/abs/2412.15213</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/qihao067/CrossFlow</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>149</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis</td>
        <td>Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, Stanley Chan</td>
        <td>https://arxiv.org/abs/2412.02168</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/pandayuanyu/generative-photography</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>150</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</td>
        <td>Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin</td>
        <td>https://arxiv.org/abs/2412.15211</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://relight-to-reconstruct.github.io/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>151</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction</td>
        <td>Seungtae Nam, Xiangyu Sun, Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park</td>
        <td>https://arxiv.org/abs/2412.06234</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/stnamjef/GenerativeDensification</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>152</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</td>
        <td>Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas MÃ¼ller, Alexander Keller, Sanja Fidler, Jun Gao</td>
        <td>https://arxiv.org/abs/2503.03751</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/nv-tlabs/GEN3C</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>153</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity</td>
        <td>Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, Nong Sang</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/pipixin321/HolmesVAU</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>154</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation</td>
        <td>Mehdi Zayene, Jannik Endres, Albias Havolli, Charles CorbiÃ¨re, Salim Cherkaoui, Alexandre Kontouli, Alexandre Alahi</td>
        <td>https://arxiv.org/abs/2411.18335</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/vita-epfl/Helvipad</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>155</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>ImViD: Immersive Volumetric Videos for Enhanced VR Engagement</td>
        <td>Zhengxian Yang, Shi Pan, Shengqi Wang, Haoxiang Wang, Li Lin, Guanjun Li, Zhengqi Wen, Borong Lin, Jianhua Tao, Tao Yu</td>
        <td>https://arxiv.org/abs/2503.14359</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Metaverse-AI-Lab-THU/ImViD</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>156</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning</td>
        <td>Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/hanxunyu/Inst3D-LMM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>157</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene</td>
        <td>Shengqiong Wu, Hao Fei, Jingkang Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, Tat-seng Chua</td>
        <td>https://arxiv.org/abs/2503.15019</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ChocoWu/PSG-4D-LLM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>158</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</td>
        <td>Xin Zhang, Robby T. Tan</td>
        <td>https://arxiv.org/abs/2504.03193</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/devinxzhang/MFuser</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>159</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps</td>
        <td>Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer SumbÃ¼l, Alexander Mathis, Devis Tuia</td>
        <td>https://arxiv.org/abs/2503.18223</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/eceo-epfl/MammAlps</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>160</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Matrix3D: Large Photogrammetry Model All-in-One</td>
        <td>Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, Shiwei Li</td>
        <td>https://arxiv.org/abs/2502.07685</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/apple/ml-matrix3d</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>161</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MITracker: Multi-View Integration for Visual Object Tracking</td>
        <td>Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang</td>
        <td>https://arxiv.org/abs/2502.20111</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/XuM007/MITracker</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>162</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Open-Canopy: Towards Very High Resolution Forest Monitoring</td>
        <td>Fajwel Fogel, Yohann Perron, Nikola Besic, Laurent Saint-AndrÃ©, AgnÃ¨s Pellissier-Tanon, Martin Schwartz, Thomas Boudras, Ibrahim Fayad, Alexandre d&#39;Aspremont, Loic Landrieu, Philippe Ciais</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/fajwel/Open-Canopy</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>163</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</td>
        <td>Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu</td>
        <td>https://arxiv.org/abs/2412.00115</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/fudan-generative-vision/OpenHumanVid</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>164</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit</td>
        <td>Benquan Wang, Ruyi An, Jin-Kyu So, Sergei Kurdiumov, Eng Aik Chan, Giorgio Adamo, Yuhan Peng, Yewen Li, Bo An</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Deep-See/OpticalNet</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>165</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Optimizing for the Shortest Path in Denoising Diffusion Model</td>
        <td>Ping Chen, Xingpeng Zhang, Zhaoxiang Liu, Huan Hu, Xiang Liu, Kai Wang, Min Wang, Yanlin Qian, Shiguo Lian</td>
        <td>https://arxiv.org/abs/2503.03265</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/UnicomAI/ShortDF</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>166</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval</td>
        <td>Yuanmin Tang, Xiaoting Qin, Jue Zhang, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Ling, Saravan Rajmohan, Dongmei Zhang, Qi Wu</td>
        <td>https://arxiv.org/abs/2412.11077</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Pter61/osrcir</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>167</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</td>
        <td>Shaoan Xie, Lingjing Lingjing, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>168</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Structured 3D Latents for Scalable and Versatile 3D Generation</td>
        <td>Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, Jiaolong Yang</td>
        <td>https://arxiv.org/abs/2412.01506</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/microsoft/TRELLIS</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>169</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer</td>
        <td>Ruojun Xu, Weijie Xi, Xiaodi Wang, Yongbo Mao, Zach Cheng</td>
        <td>https://arxiv.org/abs/2501.11319</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/bytedance/StyleSSP</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>170</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Towards Autonomous Micromobility through Scalable Urban Simulation</td>
        <td>Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou</td>
        <td>https://arxiv.org/abs/2505.00690</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/metadriverse/urban-sim</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>171</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models</td>
        <td>Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao</td>
        <td>https://arxiv.org/abs/2412.11441</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/TheLaoLab/UIBDiffusion</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>172</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning</td>
        <td>Weiqi Yan, Lvhai Chen, Huaijia Kou, Shengchuan Zhang, Yan Zhang, Liujuan Cao</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Heartfirey/UCOD-DPL</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>173</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</td>
        <td>Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang</td>
        <td>https://arxiv.org/abs/2501.12375</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/DepthAnything/Video-Depth-Anything</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>174</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>World-consistent Video Diffusion with Explicit 3D Modeling</td>
        <td>Qihang Zhang, Shuangfei Zhai, Miguel Ãngel Bautista Martin, Kevin Miao, Alexander Toshev, Joshua Susskind, Jiatao Gu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://zqh0253.github.io/wvd/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>175</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Your ViT is Secretly an Image Segmentation Model</td>
        <td>Tommie Kerssies, NiccolÃ² Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, Daan de Geus</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/tue-mps/EoMT</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>176</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>WonderWorld: Interactive 3D Scene Generation from a Single Image</td>
        <td>Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, Jiajun Wu</td>
        <td>https://arxiv.org/abs/2406.09394</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/KovenYu/WonderWorld</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>177</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Relightable Gaussian Codec Avatars</td>
        <td>Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam</td>
        <td>https://arxiv.org/pdf/2312.03704</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/goliath</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>178</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</td>
        <td>Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, Jingren Zhou</td>
        <td>https://arxiv.org/pdf/2311.17002</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ali-vilab/Ranni</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>179</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Rethinking Inductive Biases for Surface Normal Estimation</td>
        <td>Gwangbin Bae, Andrew J. Davison</td>
        <td>https://arxiv.org/pdf/2403.00712</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/baegwangbin/DSINE</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>180</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness</td>
        <td>Anh-Quan Cao, Angela Dai, Raoul de Charette</td>
        <td>https://arxiv.org/abs/2312.02158</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/astra-vision/PaSCo</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>181</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Transcriptomics-guided Slide Representation Learning in Computational Pathology</td>
        <td>Guillaume Jaume, Lukas Oldenburg, Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Thomas Peeters, Andrew H. Song, Faisal Mahmood</td>
        <td>https://arxiv.org/pdf/2405.11618</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/mahmoodlab/TANGLE</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>182</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</td>
        <td>Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn</td>
        <td>https://arxiv.org/pdf/2312.09168</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/DiffusionLight/DiffusionLight?tab=readme-ov-file</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>183</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>URHand: Universal Relightable Hands</td>
        <td>Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael ZollhÃ¶fer, Yaser Sheikh, Ziwei Liu, Shunsuke Saito</td>
        <td>https://arxiv.org/pdf/2401.05334</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/goliath</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>184</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</td>
        <td>Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler</td>
        <td>https://arxiv.org/pdf/2312.02145</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/prs-eth/marigold</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>185</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</td>
        <td>Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang</td>
        <td>https://arxiv.org/pdf/2311.12198</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/XPandora/PhysGaussian</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>186</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</td>
        <td>Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</td>
        <td>https://arxiv.org/pdf/2311.17061</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/alvinliu0/HumanGaussian</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>187</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Prompt Highlighter: Interactive Control for Multi-Modal LLMs</td>
        <td>Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia</td>
        <td>https://arxiv.org/pdf/2312.04302</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/dvlab-research/Prompt-Highlighter/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>188</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation</td>
        <td>Qi Yang, Xing Nie, Tong Li, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang</td>
        <td>https://arxiv.org/pdf/2312.06462</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://arxiv.org/pdf/2312.06462</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>189</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution</td>
        <td>Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, Chen Change Loy</td>
        <td>https://arxiv.org/pdf/2312.06640</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/sczhou/Upscale-A-Video</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>190</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Putting the Object Back into Video Object Segmentation</td>
        <td>Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, Alexander Schwing</td>
        <td>https://arxiv.org/pdf/2310.12982</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/hkchengrex/Cutie</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>191</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>InstanceDiffusion: Instance-level Control for Image Generation</td>
        <td>Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</td>
        <td>https://arxiv.org/pdf/2402.03290</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/frank-xwang/InstanceDiffusion</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>192</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OMG-Seg: Is One Model Good Enough For All Segmentation?</td>
        <td>Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy</td>
        <td>https://arxiv.org/pdf/2401.10229</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/lxtGH/OMG-Seg</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>193</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Towards Language-Driven Video Inpainting via Multimodal Large Language Models</td>
        <td>Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy</td>
        <td>https://arxiv.org/pdf/2401.10226</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/jianzongwu/Language-Driven-Video-Inpainting</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>194</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VBench: Comprehensive Benchmark Suite for Video Generative Models</td>
        <td>Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu</td>
        <td>https://arxiv.org/pdf/2311.17982</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Vchitect/VBench</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>195</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PIGEON: Predicting Image Geolocations</td>
        <td>Lukas Haas, Michal Skreta, Silas Alberti, Chelsea Finn</td>
        <td>https://arxiv.org/pdf/2307.05845</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LukasHaas/PIGEON</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>196</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</td>
        <td>Yuming Gu, You Xie, Hongyi Xu, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, Linjie Luo</td>
        <td>https://arxiv.org/pdf/2312.13016</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/FreedomGu/DiffPortrait3D/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>197</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Domain Prompt Learning with Quaternion Networks</td>
        <td>Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, Xiaokang Yang</td>
        <td>https://arxiv.org/abs/2312.08878</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/caoql98/DPLQ</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>198</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</td>
        <td>Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai</td>
        <td>https://arxiv.org/pdf/2306.14435</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Yujun-Shi/DragDiffusion</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>199</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps</td>
        <td>Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen</td>
        <td>https://arxiv.org/pdf/2312.00094</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/zju-pi/diff-sampler</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>200</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</td>
        <td>Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, Diana Marculescu</td>
        <td>https://arxiv.org/pdf/2312.17681</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://jeff-liangf.github.io/projects/flowvid/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>201</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>General Object Foundation Model for Images and Videos at Scale</td>
        <td>Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai</td>
        <td>https://arxiv.org/pdf/2312.09158</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/FoundationVision/GLEE</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>202</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding</td>
        <td>Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.html</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://uark-cviu.github.io/projects/insect-foundation/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>203</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</td>
        <td>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang</td>
        <td>https://arxiv.org/abs/2403.18036</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/afford-motion/afford-motion</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>204</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Object Recognition as Next Token Predictio</td>
        <td>Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim</td>
        <td>https://arxiv.org/abs/2312.02142</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/KaiyuYue/nxtp</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>205</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</td>
        <td>Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, Pinar Yanardag</td>
        <td>https://arxiv.org/abs/2312.04524</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/rehglab/RAVE</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>206</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Readout Guidance: Learning Control from Diffusion Features</td>
        <td>Grace Luo, Trevor Darrell, Oliver Wang, Dan B Goldman, Aleksander Holynski</td>
        <td>https://arxiv.org/abs/2312.02150</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/google-research/readout_guidance</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>207</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark</td>
        <td>Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard</td>
        <td>https://arxiv.org/abs/2403.18821</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/real-acoustic-fields</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>208</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D</td>
        <td>Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, Xiaoguang Han</td>
        <td>https://arxiv.org/abs/2311.16918</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/modelscope/RichDreamer</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>209</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>RobustSAM: Segment Anything Robustly on Degraded Images</td>
        <td>Wei-Ting Chen, Yu-Jiet Vong, Sy-Yen Kuo, Sizhuo Ma, Jian Wang</td>
        <td>https://arxiv.org/abs/2406.09627</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/robustsam/RobustSAM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>210</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Scaling Up Dynamic Human-Scene Interaction Modeling</td>
        <td>Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang</td>
        <td>https://arxiv.org/abs/2403.08629</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/jnnan/trumans_utils</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>211</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection</td>
        <td>Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek</td>
        <td>https://arxiv.org/abs/2402.17323</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>212</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SpatialTracker: Tracking Any 2D Pixels in 3D Space</td>
        <td>Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou</td>
        <td>https://arxiv.org/abs/2404.04319</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/henry123-boy/SpaTracker</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>213</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>TFMQ-DM:Temporal Feature Maintenance Quantization for Diffusion Models</td>
        <td>Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2024/html/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.html</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ModelTC/TFMQ-DM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>214</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval</td>
        <td>Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao</td>
        <td>https://arxiv.org/abs/2403.17998</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/patrick-0817/T-MASS-text-video-retrieval</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>215</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Towards Learning a Generalist Model for Embodied Navigation</td>
        <td>Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang</td>
        <td>https://arxiv.org/abs/2312.02010</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LaVi-Lab/NaviLLM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>216</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>UniMODE: Unified Monocular 3D Object Detection</td>
        <td>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao</td>
        <td>https://arxiv.org/abs/2402.18573</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Lizhuoling/UniMODE</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>217</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Unsupervised Keypoints from Pretrained Diffusion Models</td>
        <td>Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi</td>
        <td>https://arxiv.org/abs/2312.00065</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ubc-vision/StableKeypoints</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>218</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models</td>
        <td>Xiang Li, Qianli Shen, Kenji Kawaguchi</td>
        <td>https://arxiv.org/abs/2312.00057</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/South7X/VA3</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>219</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VecFusion: Vector Font Generation with Diffusion</td>
        <td>Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michael Gharbi, Oliver Wang, Alec Jacobson, Evangelos Kalogerakis</td>
        <td>https://arxiv.org/abs/2312.10540</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://vikastmz.github.io/VecFusion/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>220</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Wonder3D: Single Image to 3D using Cross-Domain Diffusion</td>
        <td>Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang</td>
        <td>https://arxiv.org/abs/2310.15008</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/xxlong0/Wonder3D</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>221</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VTimeLLM: Empower LLM to Grasp Video Moments</td>
        <td>Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu</td>
        <td>https://arxiv.org/abs/2311.18445</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/huangb23/VTimeLLM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>222</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</td>
        <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>
        <td>https://arxiv.org/pdf/2503.10625v1</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/aigc3d/LHM?tab=readme-ov-file</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>223</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer</td>
        <td>Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, Jiaming Liu</td>
        <td>https://arxiv.org/abs/2503.07027</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Xiaojiu-z/EasyControl</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>224</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</td>
        <td>Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</td>
        <td>https://arxiv.org/pdf/2403.14627</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/donydchen/mvsplat</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>225</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
        <td>Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang</td>
        <td>https://arxiv.org/abs/2312.00451</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/VITA-Group/FSGS</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>226</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>ZigMa: A DiT-style Zigzag Mamba Diffusion Model</td>
        <td>Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Schusterbauer, BjÃ¶rn Ommer</td>
        <td>https://arxiv.org/pdf/2403.13802</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/CompVis/zigma</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>227</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors</td>
        <td>Tongkun Guan, Wei Shen, Xue Yang, Xuehui Wang, Xiaokang Yang<br></td>
        <td>https://arxiv.org/pdf/2312.05286</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/SJTU-DeepVisionLab/FreeReal</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>228</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer</td>
        <td>Tongkun Guan, Chengyu Lin, Wei Shen, Xiaokang Yang</td>
        <td>https://arxiv.org/pdf/2407.07764</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/SJTU-DeepVisionLab/PosFormer</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>229</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Fully Sparse 3D Occupancy Prediction</td>
        <td>Haisong Liu, Yang Chen, Haiguang Wang, Zetong Yang, Tianyu Li, Jia Zeng, Li Chen, Hongyang Li, Limin Wang</td>
        <td>https://arxiv.org/abs/2312.17118</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/MCG-NJU/SparseOcc</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>230</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields</td>
        <td>Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</td>
        <td>https://arxiv.org/abs/2404.01300</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/zubair-irshad/NeRF-MAE</td>
        <td>Poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>231</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>ControlCap: Controllable Region-level Captioning</td>
        <td>Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Fang Wan, Qixiang Ye</td>
        <td>https://arxiv.org/abs/2401.17910</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/callsys/ControlCap</td>
        <td>Poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>232</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>GiT: Towards Generalist Vision Transformer through Universal Language Interface</td>
        <td>Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang</td>
        <td>https://arxiv.org/abs/2403.09394</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/Haiyang-W/GiT</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>233</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Relation DETR: Exploring Explicit Position Relation Prior for Object Detection</td>
        <td>Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen, Xuguang Lan</td>
        <td>https://arxiv.org/abs/2407.11699v1</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/xiuqhou/Relation-DETR</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>234</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification</td>
        <td>Yu Tian, Congcong Wen, Min Shi, Muhammad Muneeb Afzal, Hao Huang, Muhammad Osama Khan, Yan Luo, Yi Fang, Mengyu Wang</td>
        <td>https://arxiv.org/abs/2407.08813</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>235</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>OneRestore: A Universal Restoration Framework for Composite Degradation</td>
        <td>Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, Shengfeng He</td>
        <td>https://arxiv.org/abs/2407.04621</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/gy65896/OneRestore</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>236</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VideoStudio: Generating Consistent-Content and Multi-Scene Videos</td>
        <td>Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei</td>
        <td>https://arxiv.org/pdf/2401.01256</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/FuchenUSTC/VideoStudio</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>237</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Zero-shot Object Counting with Good Exemplars</td>
        <td>Huilin Zhu, Jingling Yuan, Zhengwei Yang, Yu Guo, Zheng Wang, Xian Zhong, Shengfeng He</td>
        <td>https://arxiv.org/abs/2407.04948</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/HopooLinZ/VA-Count</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>238</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments</td>
        <td>Niklas Gard, Anna Hilsmann, Peter Eisert</td>
        <td>https://arxiv.org/abs/2404.10527</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/fraunhoferhhi/spvloc</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>239</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Stereo Any Video: Temporally Consistent Stereo Matching</td>
        <td>Junpeng Jing;Weixun Luo;Ye Mao;  Krystian Mikolajczyk</td>
        <td>https://arxiv.org/html/2503.05549v1#S4</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/TomTomTommi/stereoanyvideo</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>240</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</td>
        <td>Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, Xin Lu</td>
        <td>https://arxiv.org/pdf/2503.16418</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/bytedance/InfiniteYou</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>241</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization</td>
        <td>Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin</td>
        <td>https://arxiv.org/abs/2408.02555</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/buaacyw/MeshAnythingV2</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>242</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers</td>
        <td>Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang</td>
        <td>https://arxiv.org/abs/2503.06923</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Shenyi-Z/TaylorSeer</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>243</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion</td>
        <td>Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, Yu-Gang Jiang</td>
        <td>https://arxiv.org/abs/2405.20325</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Francis-Rings/MotionFollower?tab=readme-ov-file</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>244</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</td>
        <td>Zijie Wu, Chaohui Yu, Fan Wang, Xiang Bai</td>
        <td>https://arxiv.org/abs/2506.09982</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/JarrentWu1031/AnimateAnyMesh</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>245</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>VSSD: Vision Mamba with Non-Causal State Space Duality</td>
        <td>Yuheng Shi, Minjing Dong, Mingjia Li, Chang Xu</td>
        <td>https://arxiv.org/abs/2407.18559</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/YuHengsss/VSSD?tab=readme-ov-file</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>246</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Balanced Image Stylization with Style Matching Score&quot;</td>
        <td>Yuxin Jiang, Liming Jiang, Shuai Yang, Jia-Wei Liu, Ivor Tsang, Mike Zheng Shou</td>
        <td>https://arxiv.org/abs/2503.07601</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/showlab/SMS</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>247</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model</td>
        <td>Yukang Caoã€Chenyang Siã€Jinghao Wangã€Ziwei Liu</td>
        <td>https://arxiv.org/html/2507.01953v1#S4</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/yukangcao/FreeMorph</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>248</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>GENMO: A GENeralist Model for Human MOtion</td>
        <td>Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan<br></td>
        <td>https://arxiv.org/abs/2505.01425</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://research.nvidia.com/labs/dair/genmo/</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>249</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers</td>
        <td>Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, Wenhu Chen</td>
        <td>https://arxiv.org/abs/2503.11579</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/TIGER-AI-Lab/Vamba</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>250</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning</td>
        <td>Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen</td>
        <td>https://arxiv.org/abs/2506.02327</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/scott-yjyang/MeWM</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>251</td>
        <td>ğŸ’¡ æœªå¤ç°</td>
        <td>Where, What, Why: Towards Explainable Driver Attention Prediction</td>
        <td>Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, Xiaobo Xia, Chao Gou</td>
        <td>https://arxiv.org/pdf/2506.23088</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/yuchen2199/Explainable-Driver-Attention-Prediction</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>