
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Lab4AI å¾…å¤ç°è®ºæ–‡æ¸…å•</title>
        <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji"; margin: 0; padding: 2em; background-color: #f6f8fa; color: #24292f; }
            .container { max-width: 1280px; margin: 0 auto; background-color: #ffffff; padding: 2em; border: 1px solid #d0d7de; border-radius: 8px; box-shadow: 0 4px 12px rgba(27,31,36,0.08); }
            header { text-align: center; margin-bottom: 2em; }
            header h1 { font-size: 2em; margin-bottom: 0.5em; }
            header p { font-size: 1.2em; color: #57606a; }
            header a { color: #0969da; text-decoration: none; font-weight: bold; }
            header a:hover { text-decoration: underline; }
            table.dataTable thead th { background-color: #f6f8fa; border-bottom: 2px solid #d0d7de; }
            .claim-btn { background-color: #238636; color: white; padding: 8px 16px; text-decoration: none; border-radius: 6px; font-weight: bold; white-space: nowrap; }
            .claim-btn:hover { background-color: #2ea043; }
            .status-claimed { font-weight: bold; color: #8B4513; white-space: nowrap; }
        </style>
    </head>
    <body>
        <div class="container">
            <header>
                <h1>Lab4AI å¾…å¤ç°è®ºæ–‡æ¸…å•</h1>
                <p>åœ¨ç”³è¯·ä»»åŠ¡å‰ï¼Œè¯·åŠ¡å¿…ä»”ç»†é˜…è¯»æˆ‘ä»¬çš„ 
                   <a href="https://github.com/Lab4AI-Hub/PaperHub/blob/main/CONTRIBUTING.md" target="_blank">è´¡çŒ®æµç¨‹å’Œå¥–åŠ±è§„åˆ™</a>ã€‚
                </p>
            </header>
            <table id="paperTable" class="display" style="width:100%">
                <thead>
                    <tr>
                        <th>è®ºæ–‡åç§° & ä½œè€…</th>
                        <th>ä¼šè®®æ¥æº & å¹´ä»½</th>
                        <th>è®ºæ–‡é“¾æ¥</th>
                        <th>è®¤é¢†çŠ¶æ€</th>
                        <th>æ“ä½œ</th>
                    </tr>
                </thead>
                <tbody>
                    
        <tr>
            <td>Attention is all you need<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>âœ… å·²å®Œæˆ</td>
            <td><span class="status-claimed">âœ… å·²å®Œæˆ</span></td>
        </tr>
        
        <tr>
            <td>Improving language understanding by generative pre-training<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://aclanthology.org/N19-1423/?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Language models are unsupervised multitask learners<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://www.jmlr.org/papers/v21/20-074.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>From Local to Global: A GraphRAG Approach to
Query-Focused Summarization<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2404.16130" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Language Models are Few-Shot Learners<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LoRA: Low-Rank Adaptation of Large Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openreview.net/forum?id=nZeVKeeFYf9" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Finetuned Language Models are Zero-Shot Learners<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openreview.net/forum?id=gEZrGCozdqR&ref=morioh.com&utm_source=morioh.com" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LLaMA: Open and Efficient Foundation Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2302.13971" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openreview.net/forum?id=1PL1NIMMrw&utm_source=chatgpt.com" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://aclanthology.org/2023.findings-acl.247/" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Toolformer: Language Models Can Teach Themselves to Use Tools<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://proceedings.mlr.press/v202/mitchell23a.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Recitation-augmented language models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openreview.net/forum?id=-cqvvvb-NkI" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Self-Instruct: Aligning Language Models with Self-Generated Instructions<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://aclanthology.org/2023.acl-long.754/" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Automatic chain of thought prompting in large language models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openreview.net/forum?id=5NTt8GFjUHkr" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>REALM: retrieval-augmented language model pre-training<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://dl.acm.org/doi/abs/10.5555/3524938.3525306" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Language Is Not All You Need: Aligning Perception with Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://aclanthology.org/2023.findings-emnlp.811/" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2112.09459" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2506.10575" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Segment Concealed Objects with Incomplete Supervision<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2506.08955" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Event-based Stereo Depth Estimation: A Survey<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2409.17680" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Efficient Low-Resolution Face Recognition via Bridge Distillation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2409.11786" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.05770" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.10116" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Paragraph-to-Image Generation with Information-Enriched Diffusion Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.14284" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2309.15413" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Dual Compensation Residual Networks for Class Imbalanced Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2308.13165" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>End-to-end Alternating Optimization for Real-World Blind Super Resolution<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2308.08816" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2308.05480" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2307.03759" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SplatFlow: Learning Multi-frame Optical Flow via Splatting<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2306.08887" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Towards Expressive Spectral-Temporal Graph Neural Networks for Time Series Forecasting<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2305.06587" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Efficient Halftoning via Deep Reinforcement Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2304.12152" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2211.16110" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Salient Object Detection via Dynamic Scale Routing<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2210.13821" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Twin Contrastive Learning for Online Clustering<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2210.11680" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Kernel-Based Generalized Median Computation for Consensus Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2209.10208" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A Tale of HodgeRank and Spectral Method: Target Attack Against Rank Aggregation Is the Fixed Point of Adversarial Game<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2209.05742" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Boosting Night-time Scene Parsing with Learnable Frequency<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2208.14241" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SiamMask: A Framework for Fast Online Object Tracking and Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2207.02088" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SERE: Exploring Feature Self-relation for Self-supervised Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2206.05184" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2205.09702" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2205.04721" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Incomplete Gamma Kernels: Generalizing Locally Optimal Projection Operators<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2205.01087" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>From Slow Bidirectional to Fast Autoregressive Video Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.07772" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2305.10431" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ROGRAG: A Robustly Optimized GraphRAG Framework<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.06474" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2409.20530" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Can We Leave Deepfake Data Behind in Training Deepfake Detector<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2408.17052" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2305.18500" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.12392" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2411.16785" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Murre: Multi-view Reconstruction via SfM-guided Monocular Depth Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.14483" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2501.02976" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.09055" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>HSMR: Reconstructing Humans with a Biomechanically Accurate Skeleton<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.21751" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2405.17220" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DFormerï¼šRethinking RGBD Representation Learning for Semantic Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2309.09668" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2406.06526" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.17973" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.10630" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.04887" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MINIMA: Modality Invariant Image Matching<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.19412" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.21761" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2501.18590" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Linear Programming Bounds on k-Uniform States<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.02222" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2402.05054" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VideoMamba: State Space Model for Efficient Video Understanding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.06977" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DriveLM: Driving with Graph Visual Question Answering<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.14150" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GRiT: A Generative Region-to-text Transformer for Object Understanding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2212.00280" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PointLLM: Empowering Large Language Models to Understand Point Clouds<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2308.16911" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00730.pdf" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Adversarial Diffusion Distillation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.17042" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Generative Image Dynamics<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2309.07906" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Rich Human Feedback for Text-to-Image Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.10240" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Mip-Splatting: Alias-free 3D Gaussian Splatting<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.16493" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>BioCLIP: A Vision Foundation Model for the Tree of Life<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.18803" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2310.08528" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Depth Anything: Unleashing The Power of Large-Scale Unlabeled Data<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2401.10891" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LISA: Reasoning Segmentation Via Large Language Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2308.00692" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.14238" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.16502" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.00863" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2310.03744" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DemoFusion: Democratising High-Resolution Image Generation With No $$$<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.16973" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.01807" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2405.12979" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2405.04408" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.17049" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Describing Differences in Image Sets with Natural Language<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.02974" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>XFeat: Accelerated Features for Lightweight Image Matching<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2404.19174" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.12337" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GPT4Point: A Unified Framework for Point-Language Understanding and Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.02980" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.06242" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Identity-Preserving Text-to-Video Generation by Frequency Decomposition<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2411.17440" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2407.15642" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>X-Dyna: Expressive Dynamic Human Image Animation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2501.10021" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.00596" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Timestep Embedding Tells: It&#x27;s Time to Cache for Video Diffusion Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2411.19108" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.07418" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Number it: Temporal Grounding Videos like Flipping Manga<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2411.10332" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2411.16832" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>h-Edit: Effective and Flexible Diffusion-Based Editing via Doobâ€™s h-Transform<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.02187" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2502.20087" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.09419" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D Student Splatting and Scooping<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.10148" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.12093" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Multi-view Reconstruction via SfM-guided Monocular Depth Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.14483" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.05334" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>CustAny: Customizing Anything from A Single Example<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2406.11643v4" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VGGT:Visual Geometry Grounded Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.11651" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Navigation World Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.03572" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.04463" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>FoundationStereo: Zero-Shot Stereo Matching<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2501.09898" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2502.21201" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2407.01521" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MV-DUSt3R+: Single-StageSceneReconstruction fromSparseViewsIn2Seconds<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.06974" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.01774" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DIFFUSIONRENDERER: Neural Inverse and Forward Rendering with Video Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2501.18590" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2411.18499" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>RandAR: Decoder-only Autoregressive Visual Generation in Random Orders<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.01827" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2411.15738" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2411.14794" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2410.01768" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Minority-Focused Text-to-Image Generation via Prompt Optimization<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2410.07838" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Autoregressive Distillation of Diffusion Transformers<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2504.11295" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Molmo and PixMo:Open Weights and Open Data for State-of-the-Art Vision-Language Mo<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2409.17146" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Continuous 3D Perception Model with Persistent State<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2501.12387" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.02357" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Video-XL:Extra-Long Vision Language Model for Hour-Scale Video Understanding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2409.14485" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2501.08331" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>CleanDIFT: Diffusion Features without Noise<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.03439" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2405.14979" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DreamRelation: Bridging Customization and Relation Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2410.23280" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2504.14666" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.14123" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Cross-modal Causal Relation Alignment for Video Question Grounding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.07635" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2409.02095" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.16964" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2505.21377" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.00948" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ETAP: Event-based Tracking of Any Point<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.00133" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.16481" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.15213" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.02168" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.15211" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.06234" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.03751" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2411.18335" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ImViD: Immersive Volumetric Videos for Enhanced VR Engagement<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.14359" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.15019" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2504.03193" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.18223" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Matrix3D: Large Photogrammetry Model All-in-One<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2502.07685" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MITracker: Multi-View Integration for Visual Object Tracking<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2502.20111" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Open-Canopy: Towards Very High Resolution Forest Monitoring<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.00115" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Optimizing for the Shortest Path in Denoising Diffusion Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.03265" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.11077" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SmartCLIP: Modular Vision-language Alignment with Identification Guarantees<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Structured 3D Latents for Scalable and Versatile 3D Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.01506" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2501.11319" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Towards Autonomous Micromobility through Scalable Urban Simulation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2505.00690" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2412.11441" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2501.12375" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>World-consistent Video Diffusion with Explicit 3D Modeling<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Your ViT is Secretly an Image Segmentation Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>WonderWorld: Interactive 3D Scene Generation from a Single Image<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2406.09394" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Relightable Gaussian Codec Avatars<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.03704" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2311.17002" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Rethinking Inductive Biases for Surface Normal Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2403.00712" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.02158" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Transcriptomics-guided Slide Representation Learning in Computational Pathology<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2405.11618" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DiffusionLight: Light Probes for Free by Painting a Chrome Ball<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.09168" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>URHand: Universal Relightable Hands<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.05334" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.02145" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2311.12198" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2311.17061" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Prompt Highlighter: Interactive Control for Multi-Modal LLMs<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.04302" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.06462" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.06640" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Putting the Object Back into Video Object Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2310.12982" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>InstanceDiffusion: Instance-level Control for Image Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2402.03290" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OMG-Seg: Is One Model Good Enough For All Segmentation?<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.10229" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Towards Language-Driven Video Inpainting via Multimodal Large Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.10226" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VBench: Comprehensive Benchmark Suite for Video Generative Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2311.17982" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PIGEON: Predicting Image Geolocations<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2307.05845" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.13016" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Domain Prompt Learning with Quaternion Networks<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.08878" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2306.14435" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.00094" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.17681" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>General Object Foundation Model for Images and Videos at Scale<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.09158" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.18036" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Object Recognition as Next Token Predictio<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.02142" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.04524" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Readout Guidance: Learning Control from Diffusion Features<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.02150" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.18821" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.16918" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>RobustSAM: Segment Anything Robustly on Degraded Images<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2406.09627" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Scaling Up Dynamic Human-Scene Interaction Modeling<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.08629" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2402.17323" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SpatialTracker: Tracking Any 2D Pixels in 3D Space<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2404.04319" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>TFMQ-DM:Temporal Feature Maintenance Quantization for Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.html" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.17998" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Towards Learning a Generalist Model for Embodied Navigation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.02010" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>UniMODE: Unified Monocular 3D Object Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2402.18573" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Unsupervised Keypoints from Pretrained Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.00065" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.00057" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VecFusion: Vector Font Generation with Diffusion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.10540" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Wonder3D: Single Image to 3D using Cross-Domain Diffusion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2310.15008" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VTimeLLM: Empower LLM to Grasp Video Moments<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2311.18445" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.10625v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.07027" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2403.14627" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.00451" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ZigMa: A DiT-style Zigzag Mamba Diffusion Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2403.13802" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.05286" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2407.07764" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Fully Sparse 3D Occupancy Prediction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2312.17118" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2404.01300" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ControlCap: Controllable Region-level Captioning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2401.17910" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GiT: Towards Generalist Vision Transformer through Universal Language Interface<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2403.09394" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Relation DETR: Exploring Explicit Position Relation Prior for Object Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2407.11699v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2407.08813" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>OneRestore: A Universal Restoration Framework for Composite Degradation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2407.04621" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VideoStudio: Generating Consistent-Content and Multi-Scene Videos<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.01256" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Zero-shot Object Counting with Good Exemplars<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2407.04948" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2404.10527" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Stereo Any Video: Temporally Consistent Stereo Matching<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2503.05549v1#S4" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.16418" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2408.02555" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.06923" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2405.20325" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2506.09982" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VSSD: Vision Mamba with Non-Causal State Space Duality<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2407.18559" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2507.01953v1#S4" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GENMO: A GENeralist Model for Human MOtion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2505.01425" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2503.11579" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/abs/2506.02327" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Where, What, Why: Towards Explainable Driver Attention Prediction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2506.23088" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>I Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.07680" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.14089" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2307.09312" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Quantile-Regression-Ensemble: A Deep Learning Algorithm for Downscaling Extreme Precipitation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/30193" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Spatial-Logic-Aware Weakly Supervised Learning for Flood Mapping on Earth Imagery<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/30253" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Vector Field Oriented Diffusion Model for Crystal Material Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.05402" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Periodic Graph Transformers for Crystal Material Property Prediction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2209.11807" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ContraNovo: A Contrastive Learning Approach to Enhance De Novo Peptide Sequencing<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.11584" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Dual-Channel Learning Framework for Drug-Drug Interaction Prediction via Relation-Aware Heterogeneous Graph Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/27777" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Explore 3D Dance Generation via Reward Model from Automatically-Ranked Demonstrations<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.11442" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2402.13028" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.03167" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SeGA: Preference-Aware Self-Contrastive Learning with Prompts for Anomalous User Detection on Twitter<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.11553" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Text-Guided Molecule Generation with Diffusion Language Model<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2402.13040" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Unsupervised Gene-Cell Collective Representation Learning with Optimal Transport<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/27789" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Bi-directional Adapter for Multi-modal Tracking<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.10611" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal Contrastive Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2303.03870" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Deep Linear Array Pushbroom Image Restoration: A Degradation Pipeline and Jitter-Aware Restoration Network<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.08171" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DiffSED: Sound Event Detection with Denoising Diffusion<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.07293" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Domain-Controlled Prompt Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2310.07730" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2307.00300" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2309.06933" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Evaluate Geometry of Radiance Fields with Low-frequency Color Prior<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2304.04351" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.11971" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Exploiting Polarized Material Cues for Robust Car Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.02606" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Federated Modality-specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2403.11803" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Markerless Multi-view 3D Human Pose Estimation: a survey<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2407.03817v2" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Generating and Reweighting Dense Contrastive Patterns for Unsupervised Anomaly Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.15911" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SimROD: A Simple Baseline for Raw Object Detection with Global and Local Enhancements<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.07101" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2306.14291" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>iDet3D: Towards Efficient Interactive Object Detection for LiDAR Point Clouds<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.15449" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content Counterfactually<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.11035" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Improving Diffusion-Based Image Synthesis with Context Prediction<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.02015" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>UniPre3D: Unified Pre-training of 3D Point Cloud Models withÂ Cross-Modal Gaussian Splatting
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2506.09952v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SpatioTemporal Difference Network for Video Depth Super-Resolution<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://www.arxiv.org/pdf/2508.01259" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Iterative Token Evaluation and Refinement for Real-World Super-Resolution<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.05616" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LDMVFI: Video Frame Interpolation with Latent Diffusion Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2303.09508" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Joint Demosaicing and Denoising for Spike Camera<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/27924" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable Medical Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2407.14153" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.09935" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>M-BEV: Masked BEV Perception for Robust Autonomous Driving<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.12144" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.10079" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.10877" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>NILUT: Conditional Neural Implicit 3D Lookup Tables for Image Enhancement<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2306.11920" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A Range-Null Space Decomposition Approach for Fast and Flexible Spectral Compressive Imaging<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2305.09746" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Mixed Degradation Image Restoration via Local Dynamic Optimization and Conditional Embedding
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2411.16217v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Revisiting Point Cloud Completion: Are We Ready For The Real-Worldï¼Ÿ<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2411.17580v4" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PuzzleFusion++: Auto-agglomerative 3DÂ Fracture Assembly by Denoise and Verify
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2406.00259v2" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2403.08252" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>PPEA-Depth: Progressive Parameter-Efficient Adaptation for Self-Supervised Monocular Depth Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.13066" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>ResMatch: Residual Attention Learning for Local Feature Matching<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2307.05180" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2404.01547?" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Self-Supervised Birdâ€™s Eye View Motion Prediction with Cross-Modality Signals<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.11499" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SHaRPose: Sparse High-Resolution Representation for Human Pose Estimation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.10758" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Simple Image-level Classification Improves Open-vocabulary Object Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.10439" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2305.07024" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2506.06710v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>TDeLTA: A Light-weight and Robust Table Detection Method based on Learning Text Arrangement<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.11043" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Variance-insensitive and Target-preserving Mask Refinement for Interactive Image Segmentation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.14387" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>VIXEN: Visual Text Comparison Network for Image Difference Captioning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2402.19119" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2404.00603" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>WebVLN: Vision-and-Language Navigation on Websites<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.15820" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>WeditGAN: Few-Shot Image Generation via Latent Space Relocation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2305.06671" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D Visibility-aware Generalizable Neural Radiance Fields for Interacting Hands<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.00979" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>MAC: A Benchmark for Multiple Attribute Compositional Zero-Shot Learning
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2406.12757v2" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A General Implicit Framework for Fast NeRF Composition and Rendering<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.04669" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2402.12760" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>AnomalyGPT: Detecting Industrial Anomalies Using Large Vision-Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.15366" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Arbitrary-Scale Video Super-Resolution with Structural and Textural Priors
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2407.09919v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2308.09936" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2403.02063" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Distribution Matching for Multi-Task Learning of Classification Tasks: a Large-Scale Study on Faces &amp; Beyond<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.01219" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Open-Vocabulary HOI Detection withÂ Interaction-aware Prompt and Concept Calibration
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2508.03207v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.07342" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Expediting Contrastive Language-Image Pretraining via Self-distilled Encoders<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.12659" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.05676" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2407.05680" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Frequency-Adaptive Pan-Sharpening with Mixture of Experts<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.02151" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2407.03006" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>GSN: Generalisable Segmentation in Neural Radiance Field<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2402.04632" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Hand-Centric Motion Refinement for 3D Hand-Object Interaction via Hierarchical Spatial-Temporal Modeling<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2401.15987" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>High-Fidelity Diffusion-based Image Editing<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.15707" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models with Minimal Feedback<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2312.12227" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model
<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/html/2407.05352v1" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.00678" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D Dental Model Segmentation with Geometrical Boundary Preserving<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.23702" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2504.14967" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D Gaussian Inpainting with Depth-Guided Cross-View Consistency<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2502.11801" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2406.05132" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D-GSW: 3D Gaussian Splatting for Robust Watermarking<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2409.13222" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D-HGS: 3D Half-Gaussian Splatting <br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2406.02720" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2501.01163" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2411.17735" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3DENHANCER: Consistent Multi-View Diffusion for 3D Enhancement<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.18565" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.12507" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.10437" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2505.22859" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A Bias-Free Training Paradigm for More General AI-generated Image Detection<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2412.17671" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
        <tr>
            <td>A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning<br><em style='color:#57606a;'></em></td>
            <td> </td>
            <td><a href="https://arxiv.org/pdf/2503.06960" target="_blank">æŸ¥çœ‹è®ºæ–‡</a></td>
            <td>ğŸ’¡ æœªå¤ç°</td>
            <td><span class="status-claimed">ğŸ’¡ æœªå¤ç°</span></td>
        </tr>
        
                </tbody>
            </table>
        </div>
        <script src="https://code.jquery.com/jquery-3.7.0.js"></script>
        <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js"></script>
        <script>
            $(document).ready(function() {
                $('#paperTable').DataTable({
                    "pageLength": 25,
                    "order": [], // é»˜è®¤ä¸æ’åº
                    "language": {
                        "search": "ğŸ” æœç´¢:",
                        "lengthMenu": "æ¯é¡µæ˜¾ç¤º _MENU_ æ¡",
                        "info": "æ˜¾ç¤ºç¬¬ _START_ åˆ° _END_ æ¡ï¼Œå…± _TOTAL_ æ¡",
                        "infoEmpty": "æš‚æ— æ•°æ®",
                        "infoFiltered": "(ä» _MAX_ æ¡æ€»è®°å½•ä¸­ç­›é€‰)",
                        "paginate": { "first": "é¦–é¡µ", "last": "æœ«é¡µ", "next": "ä¸‹ä¸€é¡µ", "previous": "ä¸Šä¸€é¡µ" },
                        "zeroRecords": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•"
                    }
                });
            });
        </script>
    </body>
    </html>
    