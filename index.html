<table>
    <tr>
        <td>ID</td>
        <td>案例类型</td>
        <td>论文名称</td>
        <td>论文作者</td>
        <td>论文链接</td>
        <td>论文年份</td>
        <td>来源标签<br>（会议期刊）</td>
        <td>github链接</td>
        <td>状态</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>1</td>
        <td>💡 未复现</td>
        <td>Attention is all you need</td>
        <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ﾅ「kasz Kaiser, Illia Polosukhin</td>
        <td>https://papers.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</td>
        <td>2017</td>
        <td>NIPS</td>
        <td>https://github.com/jadore801120/attention-is-all-you-need-pytorch</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>2</td>
        <td>💡 未复现</td>
        <td>Improving language understanding by generative pre-training</td>
        <td>Alec Radford，Karthik Narasimhan，Tim Salimans，Ilya Sutskever</td>
        <td>https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</td>
        <td>2018</td>
        <td>OpenAI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>3</td>
        <td>💡 未复现</td>
        <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
        <td>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</td>
        <td>https://aclanthology.org/N19-1423/?utm_campaign=The+Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_m9bbH_7ECE1h3lZ3D61TYg52rKpifVNjL4fvJ85uqggrXsWDBTB7YooFLJeNXHWqhvOyC</td>
        <td>2019</td>
        <td>NAACL</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>4</td>
        <td>💡 未复现</td>
        <td>Language models are unsupervised multitask learners</td>
        <td>Alec Radford，Jeffrey Wu，Rewon Child，David Luan，Dario Amodei，Ilya Sutskever</td>
        <td>https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</td>
        <td>2019</td>
        <td>OpenAI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>5</td>
        <td>💡 未复现</td>
        <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td>
        <td>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu</td>
        <td>https://www.jmlr.org/papers/v21/20-074.html</td>
        <td>2020</td>
        <td>JMLR</td>
        <td>https://github.com/google-research/text-to-text-transfer-transformer</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>6</td>
        <td>💡 未复现</td>
        <td>From Local to Global: A GraphRAG Approach to<br>Query-Focused Summarization</td>
        <td>Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson</td>
        <td>https://arxiv.org/abs/2404.16130</td>
        <td>2024</td>
        <td></td>
        <td>https://github.com/microsoft/graphrag</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>7</td>
        <td>💡 未复现</td>
        <td>Language Models are Few-Shot Learners</td>
        <td>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei</td>
        <td>https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&amp;utm_medium=email&amp;utm_campaign=linkedin_newsletter</td>
        <td>2020</td>
        <td>NIPS</td>
        <td>https://github.com/openai/gpt-3</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>8</td>
        <td>💡 未复现</td>
        <td>LoRA: Low-Rank Adaptation of Large Language Models</td>
        <td>Edward J Hu,yelong shen,Phillip Wallis,Zeyuan Allen-Zhu,Yuanzhi Li,Shean Wang,Lu Wang,Weizhu Chen</td>
        <td>https://openreview.net/forum?id=nZeVKeeFYf9</td>
        <td>2022</td>
        <td>ICLR</td>
        <td>https://github.com/microsoft/LoRA</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>9</td>
        <td>💡 未复现</td>
        <td>Finetuned Language Models are Zero-Shot Learners</td>
        <td>Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,Adams Wei Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V Le</td>
        <td>https://openreview.net/forum?id=gEZrGCozdqR&amp;ref=morioh.com&amp;utm_source=morioh.com</td>
        <td>2022</td>
        <td>ICLR</td>
        <td>https://github.com/google-research/FLAN</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>10</td>
        <td>💡 未复现</td>
        <td>LLaMA: Open and Efficient Foundation Language Models</td>
        <td>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample</td>
        <td>https://arxiv.org/abs/2302.13971</td>
        <td>2023</td>
        <td></td>
        <td>https://github.com/meta-llama/llama</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>11</td>
        <td>💡 未复现</td>
        <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
        <td>Xuezhi Wang,Jason Wei,Dale Schuurmans,Quoc V Le,Ed H. Chi,Sharan Narang,Aakanksha Chowdhery,Denny Zhou</td>
        <td>https://openreview.net/forum?id=1PL1NIMMrw&amp;utm_source=chatgpt.com</td>
        <td>2023</td>
        <td>ICLR</td>
        <td>https://github.com/dj-sorry/self_consistency</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>12</td>
        <td>💡 未复现</td>
        <td>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers</td>
        <td>Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei</td>
        <td>https://aclanthology.org/2023.findings-acl.247/</td>
        <td>2023</td>
        <td>ACL</td>
        <td>https://github.com/microsoft/LMOps/tree/main/understand_icl</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>13</td>
        <td>💡 未复现</td>
        <td>Toolformer: Language Models Can Teach Themselves to Use Tools</td>
        <td>Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom</td>
        <td>https://proceedings.neurips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html</td>
        <td>2023</td>
        <td>NIPS</td>
        <td>https://github.com/lucidrains/toolformer-pytorch</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>14</td>
        <td>💡 未复现</td>
        <td>DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</td>
        <td>Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn</td>
        <td>https://proceedings.mlr.press/v202/mitchell23a.html</td>
        <td>2023</td>
        <td>PMLR</td>
        <td>https://github.com/eric-mitchell/detect-gpt</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>15</td>
        <td>💡 未复现</td>
        <td>Recitation-augmented language models</td>
        <td>Zhiqing Sun,Xuezhi Wang,Yi Tay,Yiming Yang,Denny Zhou</td>
        <td>https://openreview.net/forum?id=-cqvvvb-NkI</td>
        <td>2023</td>
        <td>ICLR</td>
        <td>https://github.com/Edward-Sun/RECITE</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>16</td>
        <td>💡 未复现</td>
        <td>Self-Instruct: Aligning Language Models with Self-Generated Instructions</td>
        <td>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi</td>
        <td>https://aclanthology.org/2023.acl-long.754/</td>
        <td>2023</td>
        <td>ACL</td>
        <td>https://github.com/yizhongw/self-instruct</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>17</td>
        <td>💡 未复现</td>
        <td>Automatic chain of thought prompting in large language models</td>
        <td>Zhuosheng Zhang,Aston Zhang,Mu Li,Alex Smola</td>
        <td>https://openreview.net/forum?id=5NTt8GFjUHkr</td>
        <td>2023</td>
        <td>ICLR</td>
        <td>https://github.com/amazon-science/auto-cot</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>18</td>
        <td>💡 未复现</td>
        <td>REALM: retrieval-augmented language model pre-training</td>
        <td>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang</td>
        <td>https://dl.acm.org/doi/abs/10.5555/3524938.3525306</td>
        <td>2020</td>
        <td>ICML</td>
        <td>https://github.com/google-research/language/tree/master/language/realm</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>19</td>
        <td>💡 未复现</td>
        <td>Language Is Not All You Need: Aligning Perception with Language Models</td>
        <td>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Nils Bjorck, Vishrav Chaudhary, Subhojit Som, XIA SONG, Furu Wei</td>
        <td>https://proceedings.neurips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html</td>
        <td>2023</td>
        <td>NIPS</td>
        <td>https://github.com/microsoft/unilm/tree/master/unilm-v1</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>20</td>
        <td>💡 未复现</td>
        <td>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data</td>
        <td>Kashun Shum, Shizhe Diao, Tong Zhang</td>
        <td>https://aclanthology.org/2023.findings-emnlp.811/</td>
        <td>2023</td>
        <td>EMNLP</td>
        <td>https://github.com/SHUMKASHUN/Automate-CoT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>21</td>
        <td>💡 未复现</td>
        <td>Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching</td>
        <td>Dingwen Zhang, Wenyuan Zeng, Guangyu Guo, Chaowei Fang, Lechao Cheng, Ming-Ming Cheng, Junwei Han</td>
        <td>https://arxiv.org/abs/2112.09459</td>
        <td>2025</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>22</td>
        <td>💡 未复现</td>
        <td>Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning</td>
        <td>Chun-Mei Feng, Kai Yu, Xinxing Xu, Salman Khan, Rick Siow Mong Goh, Wangmeng Zuo, Yong Liu</td>
        <td>https://arxiv.org/abs/2506.10575</td>
        <td>2024</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>23</td>
        <td>💡 未复现</td>
        <td>Segment Concealed Objects with Incomplete Supervision</td>
        <td>Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, Youwei Pang, Longxiang Tang, Chengyu Fang, Yulun Zhang, Linghe Kong, Xiu Li, Sina Farsiu</td>
        <td>https://arxiv.org/pdf/2506.08955</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td>https://github.com/ChunmingHe/SEE</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>24</td>
        <td>💡 未复现</td>
        <td>Event-based Stereo Depth Estimation: A Survey</td>
        <td>Suman Ghosh, Guillermo Gallego</td>
        <td>https://arxiv.org/abs/2409.17680</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>25</td>
        <td>💡 未复现</td>
        <td>Efficient Low-Resolution Face Recognition via Bridge Distillation</td>
        <td>Shiming Ge, Shengwei Zhao, Chenyu Li, Yu Zhang, Jia Li</td>
        <td>https://arxiv.org/abs/2409.11786</td>
        <td>2024</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>26</td>
        <td>💡 未复现</td>
        <td>Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning</td>
        <td>Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin</td>
        <td>https://arxiv.org/abs/2403.05770</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td>https://github.com/YicongHong/Recurrent-VLN-BERT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>27</td>
        <td>💡 未复现</td>
        <td>Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning</td>
        <td>Wei Tan, Lan Du, Wray Buntine</td>
        <td>https://arxiv.org/abs/2312.10116</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>28</td>
        <td>💡 未复现</td>
        <td>Paragraph-to-Image Generation with Information-Enriched Diffusion Model</td>
        <td>Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang</td>
        <td>https://arxiv.org/abs/2311.14284</td>
        <td>2025</td>
        <td>IJCV</td>
        <td>https://github.com/weijiawu/ParaDiffusion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>29</td>
        <td>💡 未复现</td>
        <td>Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory</td>
        <td>Danpei Zhao, Bo Yuan, Zhenwei Shi</td>
        <td>https://arxiv.org/abs/2309.15413</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>30</td>
        <td>💡 未复现</td>
        <td>Dual Compensation Residual Networks for Class Imbalanced Learning</td>
        <td>Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen</td>
        <td>https://arxiv.org/abs/2308.13165</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>31</td>
        <td>💡 未复现</td>
        <td>End-to-end Alternating Optimization for Real-World Blind Super Resolution</td>
        <td>Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan</td>
        <td>https://arxiv.org/abs/2308.08816</td>
        <td>2023</td>
        <td>IJCV</td>
        <td>https://github.com/greatlog/RealDAN</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>32</td>
        <td>💡 未复现</td>
        <td>YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection</td>
        <td>Yuming Chen, Xinbin Yuan, Jiabao Wang, Ruiqi Wu, Xiang Li, Qibin Hou, Ming-Ming Cheng</td>
        <td>https://arxiv.org/abs/2308.05480</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td>https://github.com/FishAndWasabi/YOLO-MS</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>33</td>
        <td>💡 未复现</td>
        <td>A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection</td>
        <td>Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I. Webb, Irwin King, Shirui Pan</td>
        <td>https://arxiv.org/abs/2307.03759</td>
        <td>2024</td>
        <td>TPAMI</td>
        <td>https://github.com/KimMeen/Awesome-GNN4TS</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>34</td>
        <td>💡 未复现</td>
        <td>SplatFlow: Learning Multi-frame Optical Flow via Splatting</td>
        <td>Bo Wang, Yifan Zhang, Jian Li, Yang Yu, Zhenping Sun, Li Liu, Dewen Hu</td>
        <td>https://arxiv.org/abs/2306.08887</td>
        <td>2024</td>
        <td>IJCV</td>
        <td>https://github.com/wwsource/SplatFlow</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>35</td>
        <td>💡 未复现</td>
        <td>Towards Expressive Spectral-Temporal Graph Neural Networks for Time Series Forecasting</td>
        <td>Ming Jin, Guangsi Shi, Yuan-Fang Li, Bo Xiong, Tian Zhou, Flora D. Salim, Liang Zhao, Lingfei Wu, Qingsong Wen, Shirui Pan</td>
        <td>https://arxiv.org/abs/2305.06587</td>
        <td>2025</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>36</td>
        <td>💡 未复现</td>
        <td>Efficient Halftoning via Deep Reinforcement Learning</td>
        <td>Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Li Ding, Liang Chen, Kai Huang</td>
        <td>https://arxiv.org/abs/2304.12152</td>
        <td>2023</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>37</td>
        <td>💡 未复现</td>
        <td>PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison</td>
        <td>Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters</td>
        <td>https://arxiv.org/abs/2211.16110</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>38</td>
        <td>💡 未复现</td>
        <td>Salient Object Detection via Dynamic Scale Routing</td>
        <td>Zhenyu Wu, Shuai Li, Chenglizhao Chen, Hong Qin, Aimin Hao</td>
        <td>https://arxiv.org/abs/2210.13821</td>
        <td>2022</td>
        <td>TIP</td>
        <td>https://github.com/wuzhenyubuaa/DPNet</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>39</td>
        <td>💡 未复现</td>
        <td>Twin Contrastive Learning for Online Clustering</td>
        <td>Yunfan Li, Mouxing Yang, Dezhong Peng, Taihao Li, Jiantao Huang, Xi Peng</td>
        <td>https://arxiv.org/abs/2210.11680</td>
        <td>2022</td>
        <td>IJCV</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>40</td>
        <td>💡 未复现</td>
        <td>Kernel-Based Generalized Median Computation for Consensus Learning</td>
        <td>Andreas Nienkötter, Xiaoyi Jiang</td>
        <td>https://arxiv.org/abs/2209.10208</td>
        <td>2022</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>41</td>
        <td>💡 未复现</td>
        <td>A Tale of HodgeRank and Spectral Method: Target Attack Against Rank Aggregation Is the Fixed Point of Adversarial Game</td>
        <td>Ke Ma, Qianqian Xu, Jinshan Zeng, Guorong Li, Xiaochun Cao, Qingming Huang</td>
        <td>https://arxiv.org/abs/2209.05742</td>
        <td>2022</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>42</td>
        <td>💡 未复现</td>
        <td>Boosting Night-time Scene Parsing with Learnable Frequency</td>
        <td>Zhifeng Xie, Sen Wang, Ke Xu, Zhizhong Zhang, Xin Tan, Yuan Xie, Lizhuang Ma</td>
        <td>https://arxiv.org/abs/2208.14241</td>
        <td>2023</td>
        <td>TIP</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>43</td>
        <td>💡 未复现</td>
        <td>SiamMask: A Framework for Fast Online Object Tracking and Segmentation</td>
        <td>Weiming Hu, Qiang Wang, Li Zhang, Luca Bertinetto, Philip H. S. Torr</td>
        <td>https://arxiv.org/abs/2207.02088</td>
        <td>2022</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>44</td>
        <td>💡 未复现</td>
        <td>SERE: Exploring Feature Self-relation for Self-supervised Transformer</td>
        <td>Zhong-Yu Li, Shanghua Gao, Ming-Ming Cheng</td>
        <td>https://arxiv.org/abs/2206.05184</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td>https://github.com/MCG-NKU/SERE</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>45</td>
        <td>💡 未复现</td>
        <td>Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis</td>
        <td>Maciej Besta, Torsten Hoefler</td>
        <td>https://arxiv.org/abs/2205.09702</td>
        <td>2023</td>
        <td>TPAMI</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>46</td>
        <td>💡 未复现</td>
        <td>Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network</td>
        <td>Dasong Li, Yi Zhang, Ka Lung Law, Xiaogang Wang, Hongwei Qin, Hongsheng Li</td>
        <td>https://arxiv.org/abs/2205.04721</td>
        <td>2022</td>
        <td>IJCV</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>47</td>
        <td>💡 未复现</td>
        <td>Incomplete Gamma Kernels: Generalizing Locally Optimal Projection Operators</td>
        <td>Patrick Stotko, Michael Weinmann, Reinhard Klein</td>
        <td>https://arxiv.org/abs/2205.01087</td>
        <td>2024</td>
        <td>TPAMI</td>
        <td>https://github.com/stotko/incomplete-gamma-kernels</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>48</td>
        <td>💡 未复现</td>
        <td>From Slow Bidirectional to Fast Autoregressive Video Diffusion Models</td>
        <td>Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</td>
        <td>https://arxiv.org/abs/2412.07772</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/tianweiy/CausVid</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>49</td>
        <td>💡 未复现</td>
        <td>FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention</td>
        <td>Guangxuan Xiao, Tianwei Yin, William T. Freeman, Frédo Durand, Song Han</td>
        <td>https://arxiv.org/abs/2305.10431</td>
        <td>2024</td>
        <td>IJCV</td>
        <td>https://github.com/mit-han-lab/fastcomposer</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>50</td>
        <td>💡 未复现</td>
        <td>ROGRAG: A Robustly Optimized GraphRAG Framework</td>
        <td>Zhefan Wang, Huanjun Kong, Jie Ying, Wanli Ouyang, Nanqing Dong</td>
        <td>https://arxiv.org/abs/2503.06474</td>
        <td>2025</td>
        <td>ACL</td>
        <td>https://github.com/tpoisonooo/ROGRAG</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>51</td>
        <td>💡 未复现</td>
        <td>Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images</td>
        <td>Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Aysegul Dundar</td>
        <td>https://arxiv.org/abs/2409.20530</td>
        <td>2024</td>
        <td>NIPS</td>
        <td>berkegokmen1/dual-enc-3d-gan-inversion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>52</td>
        <td>💡 未复现</td>
        <td>Can We Leave Deepfake Data Behind in Training Deepfake Detector</td>
        <td>Jikang Cheng, Zhiyuan Yan, Ying Zhang, Yuhao Luo, Zhongyuan Wang, Chen Li</td>
        <td>https://arxiv.org/pdf/2408.17052</td>
        <td>2024</td>
        <td>NIPS</td>
        <td>https://github.com/beautyremain/ProDet</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>53</td>
        <td>💡 未复现</td>
        <td>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</td>
        <td>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</td>
        <td>https://arxiv.org/abs/2305.18500</td>
        <td>2023</td>
        <td>NIPS</td>
        <td>https://github.com/TXH-mercury/VAST</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>54</td>
        <td>💡 未复现</td>
        <td>MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</td>
        <td>Riku Murai, Eric Dexheimer, Andrew J. Davison</td>
        <td>https://arxiv.org/abs/2412.12392</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/rmurai0610/MASt3R-SLAM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>55</td>
        <td>💡 未复现</td>
        <td>MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM</td>
        <td>Vladimir Yugay, Theo Gevers, Martin R. Oswald</td>
        <td>https://arxiv.org/abs/2411.16785</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/VladimirYugay/MAGiC-SLAM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>56</td>
        <td>💡 未复现</td>
        <td>Murre: Multi-view Reconstruction via SfM-guided Monocular Depth Estimation</td>
        <td>Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao</td>
        <td>https://arxiv.org/abs/2503.14483</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zju3dv/Murre</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>57</td>
        <td>💡 未复现</td>
        <td>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</td>
        <td>Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, Ying Tai</td>
        <td>https://arxiv.org/abs/2501.02976</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/NJU-PCALab/STAR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>58</td>
        <td>💡 未复现</td>
        <td>SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models</td>
        <td>Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee</td>
        <td>https://arxiv.org/abs/2403.09055</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ironjr/semantic-draw</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>59</td>
        <td>💡 未复现</td>
        <td>HSMR: Reconstructing Humans with a Biomechanically Accurate Skeleton</td>
        <td>Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos</td>
        <td>https://arxiv.org/abs/2503.21751</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/IsshikiHugh/HSMR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>60</td>
        <td>💡 未复现</td>
        <td>RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness</td>
        <td>Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun</td>
        <td>https://arxiv.org/abs/2405.17220</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/RLHF-V/RLAIF-V</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>61</td>
        <td>💡 未复现</td>
        <td>DFormer：Rethinking RGBD Representation Learning for Semantic Segmentation</td>
        <td>Bowen Yin, Xuying Zhang, Zhongyu Li, Li Liu, Ming-Ming Cheng, Qibin Hou</td>
        <td>https://arxiv.org/abs/2309.09668</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/VCIP-RGBD/DFormer</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>62</td>
        <td>💡 未复现</td>
        <td>GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation</td>
        <td>Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</td>
        <td>https://arxiv.org/abs/2406.06526</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/hzxie/GaussianCity</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>63</td>
        <td>💡 未复现</td>
        <td>PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos</td>
        <td>Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li</td>
        <td>https://arxiv.org/abs/2503.17973</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Jianghanxiao/PhysTwin</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>64</td>
        <td>💡 未复现</td>
        <td>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</td>
        <td>Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu</td>
        <td>https://arxiv.org/abs/2503.10630</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/bagh2178/UniGoal</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>65</td>
        <td>💡 未复现</td>
        <td>Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction</td>
        <td>Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang</td>
        <td>https://arxiv.org/abs/2412.04887</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Jixuan-Fan/Momentum-GS</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>66</td>
        <td>💡 未复现</td>
        <td>MINIMA: Modality Invariant Image Matching</td>
        <td>Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai</td>
        <td>https://arxiv.org/abs/2412.19412</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LSXI7/MINIMA</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>67</td>
        <td>💡 未复现</td>
        <td>Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video</td>
        <td>David Yifan Yao, Albert J. Zhai, Shenlong Wang</td>
        <td>https://arxiv.org/abs/2503.21761</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Davidyao99/uni4d</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>68</td>
        <td>💡 未复现</td>
        <td>Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models</td>
        <td>Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang</td>
        <td>https://arxiv.org/abs/2501.18590</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/52CV/CVPR-2025-Papers</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>69</td>
        <td>💡 未复现</td>
        <td>Linear Programming Bounds on k-Uniform States</td>
        <td>Yu Ning, Fei Shi, Tao Luo, Xiande Zhang</td>
        <td>https://arxiv.org/abs/2503.02222</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Epona-World-Model/Epona</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>70</td>
        <td>💡 未复现</td>
        <td>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</td>
        <td>Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, Ziwei Liu</td>
        <td>https://arxiv.org/abs/2402.05054</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/3DTopia/LGM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>71</td>
        <td>💡 未复现</td>
        <td>VideoMamba: State Space Model for Efficient Video Understanding</td>
        <td>Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao</td>
        <td>https://arxiv.org/abs/2403.06977</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/OpenGVLab/video-mamba-suite</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>72</td>
        <td>💡 未复现</td>
        <td>DriveLM: Driving with Graph Visual Question Answering</td>
        <td>Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, Hongyang Li</td>
        <td>https://arxiv.org/abs/2312.14150</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/OpenDriveLab/DriveLM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>73</td>
        <td>💡 未复现</td>
        <td>GRiT: A Generative Region-to-text Transformer for Object Understanding</td>
        <td>Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang</td>
        <td>https://arxiv.org/abs/2212.00280</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/JialianW/GRiT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>74</td>
        <td>💡 未复现</td>
        <td>PointLLM: Empowering Large Language Models to Understand Point Clouds</td>
        <td>Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin</td>
        <td>https://arxiv.org/abs/2308.16911</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/OpenRobotLab/PointLLM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>75</td>
        <td>💡 未复现</td>
        <td>nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding</td>
        <td>Benjin Zhu, Zhe Wang, and Hongsheng Li</td>
        <td>https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00730.pdf</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>/</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>76</td>
        <td>💡 未复现</td>
        <td>Adversarial Diffusion Distillation</td>
        <td>Axel Sauer, Dominik Lorenz, Andreas Blattmann, Robin Rombach</td>
        <td>https://arxiv.org/abs/2311.17042</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/AMD-AIG-AIMA/AMD-Diffusion-Distillation</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>77</td>
        <td>💡 未复现</td>
        <td>Generative Image Dynamics</td>
        <td>Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski</td>
        <td>https://arxiv.org/abs/2309.07906</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://generative-dynamics.github.io/</td>
        <td>最佳论文</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>78</td>
        <td>💡 未复现</td>
        <td>Rich Human Feedback for Text-to-Image Generation</td>
        <td>Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, Junjie Ke, Krishnamurthy Dj Dvijotham, Katie Collins, Yiwen Luo, Yang Li, Kai J Kohlhoff, Deepak Ramachandran, Vidhya Navalpakkam</td>
        <td>https://arxiv.org/pdf/2312.10240</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/youweiliang/RichHF</td>
        <td>最佳论文</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>79</td>
        <td>💡 未复现</td>
        <td>Mip-Splatting: Alias-free 3D Gaussian Splatting</td>
        <td>Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger</td>
        <td>https://arxiv.org/abs/2311.16493</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/autonomousvision/mip-splatting</td>
        <td>最佳学生论文</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>80</td>
        <td>💡 未复现</td>
        <td>BioCLIP: A Vision Foundation Model for the Tree of Life</td>
        <td>Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su</td>
        <td>https://arxiv.org/abs/2311.18803</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Imageomics/bioclip</td>
        <td>最佳学生论文</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>81</td>
        <td>💡 未复现</td>
        <td>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</td>
        <td>Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</td>
        <td>https://arxiv.org/abs/2310.08528</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/hustvl/4DGaussians</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>82</td>
        <td>💡 未复现</td>
        <td>Depth Anything: Unleashing The Power of Large-Scale Unlabeled Data</td>
        <td>Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao</td>
        <td>https://arxiv.org/abs/2401.10891</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LiheYoung/Depth-Anything</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>83</td>
        <td>💡 未复现</td>
        <td>LISA: Reasoning Segmentation Via Large Language Model</td>
        <td>Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia</td>
        <td>https://arxiv.org/abs/2308.00692</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/dvlab-research/LISA</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>84</td>
        <td>💡 未复现</td>
        <td>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</td>
        <td>Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai</td>
        <td>https://arxiv.org/abs/2312.14238</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/OpenGVLab/InternVL</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>85</td>
        <td>💡 未复现</td>
        <td>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark</td>
        <td>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen</td>
        <td>https://arxiv.org/abs/2311.16502</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/MMMU-Benchmark/MMMU</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>86</td>
        <td>💡 未复现</td>
        <td>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</td>
        <td>Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra</td>
        <td>https://arxiv.org/abs/2312.00863</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/yformer/EfficientSAM</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>87</td>
        <td>💡 未复现</td>
        <td>Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)</td>
        <td>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee</td>
        <td>https://arxiv.org/abs/2310.03744</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LLaVA-VL/LLaVA-NeXT</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>88</td>
        <td>💡 未复现</td>
        <td>DemoFusion: Democratising High-Resolution Image Generation With No $$$</td>
        <td>Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma</td>
        <td>https://arxiv.org/abs/2311.16973</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/PRIS-CV/DemoFusion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>89</td>
        <td>💡 未复现</td>
        <td>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</td>
        <td>Lukas Höllein, Aljaž Božič, Norman Müller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhöfer, Matthias Nießner</td>
        <td>https://arxiv.org/abs/2403.01807</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/ViewDiff</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>90</td>
        <td>💡 未复现</td>
        <td>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</td>
        <td>Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andre Araujo</td>
        <td>https://arxiv.org/abs/2405.12979</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/google-research/omniglue</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>91</td>
        <td>💡 未复现</td>
        <td>DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks</td>
        <td>Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, Lianwen Jin</td>
        <td>https://arxiv.org/abs/2405.04408</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ZZZHANG-jx/DocRes</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>92</td>
        <td>💡 未复现</td>
        <td>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</td>
        <td>Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel</td>
        <td>https://arxiv.org/abs/2311.17049</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/apple/ml-mobileclip</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>93</td>
        <td>💡 未复现</td>
        <td>Describing Differences in Image Sets with Natural Language</td>
        <td>Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy</td>
        <td>https://arxiv.org/abs/2312.02974</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Understanding-Visual-Datasets/VisDiff</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>94</td>
        <td>💡 未复现</td>
        <td>XFeat: Accelerated Features for Lightweight Image Matching</td>
        <td>Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, Erickson R. Nascimento</td>
        <td>https://arxiv.org/abs/2404.19174</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/verlab/accelerated_features</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>95</td>
        <td>💡 未复现</td>
        <td>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction</td>
        <td>David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</td>
        <td>https://arxiv.org/abs/2312.12337</td>
        <td>2024</td>
        <td>CVPR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>96</td>
        <td>💡 未复现</td>
        <td>GPT4Point: A Unified Framework for Point-Language Understanding and Generation</td>
        <td>Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao</td>
        <td>https://arxiv.org/abs/2312.02980</td>
        <td>2024</td>
        <td>CVPR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>97</td>
        <td>💡 未复现</td>
        <td>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</td>
        <td>Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan</td>
        <td>https://arxiv.org/abs/2311.06242</td>
        <td>2024</td>
        <td>CVPR</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>98</td>
        <td>💡 未复现</td>
        <td>Identity-Preserving Text-to-Video Generation by Frequency Decomposition</td>
        <td>Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, Li Yuan</td>
        <td>https://arxiv.org/abs/2411.17440</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/PKU-YuanGroup/ConsisID</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>99</td>
        <td>💡 未复现</td>
        <td>Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models</td>
        <td>Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Yuan-Fang Li, Cunjian Chen, Yu Qiao</td>
        <td>https://arxiv.org/abs/2407.15642</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/maxin-cn/Cinemo</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>100</td>
        <td>💡 未复现</td>
        <td>X-Dyna: Expressive Dynamic Human Image Animation</td>
        <td>Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, Mohammad Soleymani</td>
        <td>https://arxiv.org/abs/2501.10021</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/bytedance/X-Dyna</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>101</td>
        <td>💡 未复现</td>
        <td>PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation</td>
        <td>Qiyao Xue, Xiangyu Yin, Boyuan Yang, Wei Gao</td>
        <td>https://arxiv.org/pdf/2412.00596</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/pittisl/PhyT2V</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>102</td>
        <td>💡 未复现</td>
        <td>Timestep Embedding Tells: It&#39;s Time to Cache for Video Diffusion Model</td>
        <td>Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan</td>
        <td>https://arxiv.org/abs/2411.19108</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ali-vilab/TeaCache</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>103</td>
        <td>💡 未复现</td>
        <td>AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion</td>
        <td>Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, Jing Liu</td>
        <td>https://arxiv.org/abs/2503.07418</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/iva-mzsun/AR-Diffusion</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>104</td>
        <td>💡 未复现</td>
        <td>Number it: Temporal Grounding Videos like Flipping Manga</td>
        <td>Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, Xu Yang</td>
        <td>https://arxiv.org/abs/2411.10332</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/yongliang-wu/NumPro</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>105</td>
        <td>💡 未复现</td>
        <td>Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing</td>
        <td>Hanhui Wang, Yihua Zhang, Ruizheng Bai, Yue Zhao, Sijia Liu, Zhengzhong Tu</td>
        <td>https://arxiv.org/abs/2411.16832</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/taco-group/FaceLock</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>106</td>
        <td>💡 未复现</td>
        <td>h-Edit: Effective and Flexible Diffusion-Based Editing via Doob’s h-Transform</td>
        <td>Toan Nguyen, Kien Do, Duc Kieu, Thin Nguyen</td>
        <td>https://arxiv.org/abs/2503.02187</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/nktoan/h-edit</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>107</td>
        <td>💡 未复现</td>
        <td>OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels</td>
        <td>Meng Lou, Yizhou Yu</td>
        <td>https://arxiv.org/abs/2502.20087</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LMMMEng/OverLoCK</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>108</td>
        <td>💡 未复现</td>
        <td>Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space</td>
        <td>Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan</td>
        <td>https://arxiv.org/pdf/2503.09419</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/SingleZombie/AFLDM</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>109</td>
        <td>💡 未复现</td>
        <td>3D Student Splatting and Scooping</td>
        <td>Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang</td>
        <td>https://arxiv.org/abs/2503.10148</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/realcrane/3D-student-splating-and-scooping</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>110</td>
        <td>💡 未复现</td>
        <td>CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models</td>
        <td>Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell</td>
        <td>https://arxiv.org/pdf/2412.12093</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/felixtaubner/cap4d/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>111</td>
        <td>💡 未复现</td>
        <td>Multi-view Reconstruction via SfM-guided Monocular Depth Estimation</td>
        <td>Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao</td>
        <td>https://arxiv.org/pdf/2503.14483</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zju3dv/Murre</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>112</td>
        <td>💡 未复现</td>
        <td>Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models</td>
        <td>Zhejun Zhang, Peter Karkus, Maximilian Igl, Wenhao Ding, Yuxiao Chen, Boris Ivanovic, Marco Pavone</td>
        <td>https://arxiv.org/pdf/2412.05334</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/NVlabs/catk</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>113</td>
        <td>💡 未复现</td>
        <td>CustAny: Customizing Anything from A Single Example</td>
        <td>Lingjie Kong, Kai Wu, Xiaobin Hu, Wenhui Han, Jinlong Peng, Chengming Xu, Donghao Luo, Mengtian Li, Jiangning Zhang, Chengjie Wang, Yanwei Fu</td>
        <td>https://arxiv.org/pdf/2406.11643v4</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LingjieKong-fdu/CustAny</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>114</td>
        <td>💡 未复现</td>
        <td>VGGT:Visual Geometry Grounded Transformer</td>
        <td>Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny</td>
        <td>https://arxiv.org/pdf/2503.11651</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/vggt</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>115</td>
        <td>💡 未复现</td>
        <td>Navigation World Models</td>
        <td>Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun</td>
        <td>https://arxiv.org/pdf/2412.03572</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/nwm/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>116</td>
        <td>💡 未复现</td>
        <td>MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos</td>
        <td>Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, Noah Snavely</td>
        <td>https://arxiv.org/pdf/2412.04463</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/mega-sam/mega-sam</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>117</td>
        <td>💡 未复现</td>
        <td>FoundationStereo: Zero-Shot Stereo Matching</td>
        <td>Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield</td>
        <td>https://arxiv.org/pdf/2501.09898</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/NVlabs/FoundationStereo/</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>118</td>
        <td>💡 未复现</td>
        <td>The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition</td>
        <td>Otto Brookes, Maksim Kukushkin, Majid Mirmehdi, Colleen Stephens, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus Zuberbühler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar Kühl, Tilo Burghardt</td>
        <td>https://arxiv.org/pdf/2502.21201</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://obrookes.github.io/panaf-fgbg.github.io/</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>119</td>
        <td>💡 未复现</td>
        <td>Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing</td>
        <td>Bingliang Zhang, Wenda Chu, Julius Berner, Chenlin Meng, Anima Anandkumar, Yang Song</td>
        <td>https://arxiv.org/pdf/2407.01521</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zhangbingliang2019/DAPS</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>120</td>
        <td>💡 未复现</td>
        <td>MV-DUSt3R+: Single-StageSceneReconstruction fromSparseViewsIn2Seconds</td>
        <td>Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan</td>
        <td>https://arxiv.org/pdf/2412.06974</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/mvdust3r</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>121</td>
        <td>💡 未复现</td>
        <td>DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</td>
        <td>Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</td>
        <td>https://arxiv.org/pdf/2503.01774</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/nv-tlabs/Difix3D</td>
        <td>oral,Award Candidate</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>122</td>
        <td>💡 未复现</td>
        <td>DIFFUSIONRENDERER: Neural Inverse and Forward Rendering with Video Diffusion Models</td>
        <td>Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang</td>
        <td>https://arxiv.org/pdf/2501.18590</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>123</td>
        <td>💡 未复现</td>
        <td>OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation</td>
        <td>Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao, Kaipeng Zhang</td>
        <td>https://arxiv.org/pdf/2411.18499</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/LanceZPF/OpenING</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>124</td>
        <td>💡 未复现</td>
        <td>RandAR: Decoder-only Autoregressive Visual Generation in Random Orders</td>
        <td>Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang</td>
        <td>https://arxiv.org/pdf/2412.01827</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ziqipang/RandAR</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>125</td>
        <td>💡 未复现</td>
        <td>AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea</td>
        <td>Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, Yueting Zhuang</td>
        <td>https://arxiv.org/pdf/2411.15738</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/DCDmllm/AnyEdit</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>126</td>
        <td>💡 未复现</td>
        <td>VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</td>
        <td>Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, Si Liu</td>
        <td>https://arxiv.org/pdf/2411.14794</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/hshjerry/VideoEspresso</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>127</td>
        <td>💡 未复现</td>
        <td>SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images</td>
        <td>Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, Zhi Wang</td>
        <td>https://arxiv.org/pdf/2410.01768</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/likyoo/SegEarth-OV</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>128</td>
        <td>💡 未复现</td>
        <td>Minority-Focused Text-to-Image Generation via Prompt Optimization</td>
        <td>Soobin Um, Jong Chul Ye</td>
        <td>https://arxiv.org/pdf/2410.07838</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/soobin-um/MinorityPrompt</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>129</td>
        <td>💡 未复现</td>
        <td>Autoregressive Distillation of Diffusion Transformers</td>
        <td>Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar Schönfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu</td>
        <td>https://arxiv.org/pdf/2504.11295</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/alsdudrla10/ARD</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>130</td>
        <td>💡 未复现</td>
        <td>Molmo and PixMo:Open Weights and Open Data for State-of-the-Art Vision-Language Mo</td>
        <td>Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi</td>
        <td>https://arxiv.org/pdf/2409.17146</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/allenai/molmo</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>131</td>
        <td>💡 未复现</td>
        <td>Continuous 3D Perception Model with Persistent State</td>
        <td>Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa</td>
        <td>https://arxiv.org/pdf/2501.12387</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/CUT3R/CUT3R</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>132</td>
        <td>💡 未复现</td>
        <td>Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</td>
        <td>Zicheng Zhang, Tengchuan Kou, Shushi Wang, Chunyi Li, Wei Sun, Wei Wang, Xiaoyu Li, Zongyu Wang, Xuezhi Cao, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai</td>
        <td>https://arxiv.org/pdf/2503.02357</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/zzc-1998/Q-Eval</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>133</td>
        <td>💡 未复现</td>
        <td>Video-XL:Extra-Long Vision Language Model for Hour-Scale Video Understanding</td>
        <td>Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, Bo Zhao</td>
        <td>https://arxiv.org/pdf/2409.14485</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/VectorSpaceLab/Video-XL</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>134</td>
        <td>💡 未复现</td>
        <td>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</td>
        <td>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu</td>
        <td>https://arxiv.org/pdf/2501.08331</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Eyeline-Research/Go-with-the-Flow</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>135</td>
        <td>💡 未复现</td>
        <td>CleanDIFT: Diffusion Features without Noise</td>
        <td>Nick Stracke, Stefan Andreas Baumann, Kolja Bauer, Frank Fundel, Björn Ommer</td>
        <td>https://arxiv.org/pdf/2412.03439</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/CompVis/cleandift</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>136</td>
        <td>💡 未复现</td>
        <td>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner</td>
        <td>Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</td>
        <td>https://arxiv.org/pdf/2405.14979</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/wyysf-98/CraftsMan3D</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>137</td>
        <td>💡 未复现</td>
        <td>DreamRelation: Bridging Customization and Relation Generation</td>
        <td>Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li</td>
        <td>https://arxiv.org/pdf/2410.23280</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Shi-qingyu/DreamRelation</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>138</td>
        <td>💡 未复现</td>
        <td>Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens</td>
        <td>Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, Hanwang Zhang</td>
        <td>https://arxiv.org/pdf/2504.14666</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/selftok-team/SelftokTokenizer/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>139</td>
        <td>💡 未复现</td>
        <td>3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion</td>
        <td>Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, Liang Pan, Dahua Lin, Ziwei Liu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/3DTopia/3DTopia-XL</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>140</td>
        <td>💡 未复现</td>
        <td>AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</td>
        <td>Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu</td>
        <td>https://arxiv.org/abs/2412.14123</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/gastruc/AnySat</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>141</td>
        <td>💡 未复现</td>
        <td>Cross-modal Causal Relation Alignment for Video Question Grounding</td>
        <td>Weixing Chen, Yang Liu, Binglin Chen, Jiandong Su, Yongsen Zheng, Liang Lin</td>
        <td>https://arxiv.org/abs/2503.07635</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/WissingChen/CRA-GQA</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>142</td>
        <td>💡 未复现</td>
        <td>DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</td>
        <td>Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan</td>
        <td>https://arxiv.org/abs/2409.02095</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Tencent/DepthCrafter</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>143</td>
        <td>💡 未复现</td>
        <td>DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery</td>
        <td>Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang</td>
        <td>https://arxiv.org/abs/2503.16964</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/BITyia/DroneSplat</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>144</td>
        <td>💡 未复现</td>
        <td>Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility</td>
        <td>Yidi Li, Jun Xiao, Zhengda Lu, Yiqun Wang, Haiyong Jiang</td>
        <td>https://arxiv.org/abs/2505.21377</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/chenxinl/Dream3DVG</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>145</td>
        <td>💡 未复现</td>
        <td>Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</td>
        <td>Jie Tian, Xiaoye Qu, Zhenyi Lu, Wei Wei, Sichen Liu, Yu Cheng</td>
        <td>https://arxiv.org/abs/2503.00948</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Chuge0335/EDG</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>146</td>
        <td>💡 未复现</td>
        <td>ETAP: Event-based Tracking of Any Point</td>
        <td>Friedhelm Hamann, Daniel Gehrig, Filbert Febryanto, Kostas Daniilidis, Guillermo Gallego</td>
        <td>https://arxiv.org/abs/2412.00133</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/tub-rip/ETAP</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>147</td>
        <td>💡 未复现</td>
        <td>Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality</td>
        <td>Liyan Chen, Gregory P. Meyer, Zaiwei Zhang, Eric M. Wolff, Paul Vernaza</td>
        <td>https://arxiv.org/abs/2412.16481</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/liyanc/Flash3DTransformer</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>148</td>
        <td>💡 未复现</td>
        <td>Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution</td>
        <td>Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh</td>
        <td>https://arxiv.org/abs/2412.15213</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/qihao067/CrossFlow</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>149</td>
        <td>💡 未复现</td>
        <td>Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis</td>
        <td>Yu Yuan, Xijun Wang, Yichen Sheng, Prateek Chennuri, Xingguang Zhang, Stanley Chan</td>
        <td>https://arxiv.org/abs/2412.02168</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/pandayuanyu/generative-photography</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>150</td>
        <td>💡 未复现</td>
        <td>Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</td>
        <td>Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin</td>
        <td>https://arxiv.org/abs/2412.15211</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://relight-to-reconstruct.github.io/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>151</td>
        <td>💡 未复现</td>
        <td>Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction</td>
        <td>Seungtae Nam, Xiangyu Sun, Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park</td>
        <td>https://arxiv.org/abs/2412.06234</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/stnamjef/GenerativeDensification</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>152</td>
        <td>💡 未复现</td>
        <td>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</td>
        <td>Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao</td>
        <td>https://arxiv.org/abs/2503.03751</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/nv-tlabs/GEN3C</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>153</td>
        <td>💡 未复现</td>
        <td>Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity</td>
        <td>Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, Nong Sang</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/pipixin321/HolmesVAU</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>154</td>
        <td>💡 未复现</td>
        <td>HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation</td>
        <td>Mehdi Zayene, Jannik Endres, Albias Havolli, Charles Corbière, Salim Cherkaoui, Alexandre Kontouli, Alexandre Alahi</td>
        <td>https://arxiv.org/abs/2411.18335</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/vita-epfl/Helvipad</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>155</td>
        <td>💡 未复现</td>
        <td>ImViD: Immersive Volumetric Videos for Enhanced VR Engagement</td>
        <td>Zhengxian Yang, Shi Pan, Shengqi Wang, Haoxiang Wang, Li Lin, Guanjun Li, Zhengqi Wen, Borong Lin, Jianhua Tao, Tao Yu</td>
        <td>https://arxiv.org/abs/2503.14359</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Metaverse-AI-Lab-THU/ImViD</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>156</td>
        <td>💡 未复现</td>
        <td>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning</td>
        <td>Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/hanxunyu/Inst3D-LMM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>157</td>
        <td>💡 未复现</td>
        <td>Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene</td>
        <td>Shengqiong Wu, Hao Fei, Jingkang Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, Tat-seng Chua</td>
        <td>https://arxiv.org/abs/2503.15019</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/ChocoWu/PSG-4D-LLM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>158</td>
        <td>💡 未复现</td>
        <td>Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</td>
        <td>Xin Zhang, Robby T. Tan</td>
        <td>https://arxiv.org/abs/2504.03193</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/devinxzhang/MFuser</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>159</td>
        <td>💡 未复现</td>
        <td>MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps</td>
        <td>Valentin Gabeff, Haozhe Qi, Brendan Flaherty, Gencer Sumbül, Alexander Mathis, Devis Tuia</td>
        <td>https://arxiv.org/abs/2503.18223</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/eceo-epfl/MammAlps</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>160</td>
        <td>💡 未复现</td>
        <td>Matrix3D: Large Photogrammetry Model All-in-One</td>
        <td>Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, Shiwei Li</td>
        <td>https://arxiv.org/abs/2502.07685</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/apple/ml-matrix3d</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>161</td>
        <td>💡 未复现</td>
        <td>MITracker: Multi-View Integration for Visual Object Tracking</td>
        <td>Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang</td>
        <td>https://arxiv.org/abs/2502.20111</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/XuM007/MITracker</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>162</td>
        <td>💡 未复现</td>
        <td>Open-Canopy: Towards Very High Resolution Forest Monitoring</td>
        <td>Fajwel Fogel, Yohann Perron, Nikola Besic, Laurent Saint-André, Agnès Pellissier-Tanon, Martin Schwartz, Thomas Boudras, Ibrahim Fayad, Alexandre d&#39;Aspremont, Loic Landrieu, Philippe Ciais</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/fajwel/Open-Canopy</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>163</td>
        <td>💡 未复现</td>
        <td>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</td>
        <td>Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu</td>
        <td>https://arxiv.org/abs/2412.00115</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/fudan-generative-vision/OpenHumanVid</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>164</td>
        <td>💡 未复现</td>
        <td>OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit</td>
        <td>Benquan Wang, Ruyi An, Jin-Kyu So, Sergei Kurdiumov, Eng Aik Chan, Giorgio Adamo, Yuhan Peng, Yewen Li, Bo An</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Deep-See/OpticalNet</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>165</td>
        <td>💡 未复现</td>
        <td>Optimizing for the Shortest Path in Denoising Diffusion Model</td>
        <td>Ping Chen, Xingpeng Zhang, Zhaoxiang Liu, Huan Hu, Xiang Liu, Kai Wang, Min Wang, Yanlin Qian, Shiguo Lian</td>
        <td>https://arxiv.org/abs/2503.03265</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/UnicomAI/ShortDF</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>166</td>
        <td>💡 未复现</td>
        <td>Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval</td>
        <td>Yuanmin Tang, Xiaoting Qin, Jue Zhang, Jing Yu, Gaopeng Gou, Gang Xiong, Qingwei Ling, Saravan Rajmohan, Dongmei Zhang, Qi Wu</td>
        <td>https://arxiv.org/abs/2412.11077</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Pter61/osrcir</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>167</td>
        <td>💡 未复现</td>
        <td>SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</td>
        <td>Shaoan Xie, Lingjing Lingjing, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>168</td>
        <td>💡 未复现</td>
        <td>Structured 3D Latents for Scalable and Versatile 3D Generation</td>
        <td>Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, Jiaolong Yang</td>
        <td>https://arxiv.org/abs/2412.01506</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/microsoft/TRELLIS</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>169</td>
        <td>💡 未复现</td>
        <td>StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer</td>
        <td>Ruojun Xu, Weijie Xi, Xiaodi Wang, Yongbo Mao, Zach Cheng</td>
        <td>https://arxiv.org/abs/2501.11319</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/bytedance/StyleSSP</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>170</td>
        <td>💡 未复现</td>
        <td>Towards Autonomous Micromobility through Scalable Urban Simulation</td>
        <td>Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou</td>
        <td>https://arxiv.org/abs/2505.00690</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/metadriverse/urban-sim</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>171</td>
        <td>💡 未复现</td>
        <td>UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models</td>
        <td>Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao</td>
        <td>https://arxiv.org/abs/2412.11441</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/TheLaoLab/UIBDiffusion</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>172</td>
        <td>💡 未复现</td>
        <td>UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning</td>
        <td>Weiqi Yan, Lvhai Chen, Huaijia Kou, Shengchuan Zhang, Yan Zhang, Liujuan Cao</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/Heartfirey/UCOD-DPL</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>173</td>
        <td>💡 未复现</td>
        <td>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</td>
        <td>Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang</td>
        <td>https://arxiv.org/abs/2501.12375</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/DepthAnything/Video-Depth-Anything</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>174</td>
        <td>💡 未复现</td>
        <td>World-consistent Video Diffusion with Explicit 3D Modeling</td>
        <td>Qihang Zhang, Shuangfei Zhai, Miguel Ángel Bautista Martin, Kevin Miao, Alexander Toshev, Joshua Susskind, Jiatao Gu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://zqh0253.github.io/wvd/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>175</td>
        <td>💡 未复现</td>
        <td>Your ViT is Secretly an Image Segmentation Model</td>
        <td>Tommie Kerssies, Niccolò Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, Daan de Geus</td>
        <td>https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/tue-mps/EoMT</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>176</td>
        <td>💡 未复现</td>
        <td>WonderWorld: Interactive 3D Scene Generation from a Single Image</td>
        <td>Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, Jiajun Wu</td>
        <td>https://arxiv.org/abs/2406.09394</td>
        <td>2025</td>
        <td>CVPR</td>
        <td>https://github.com/KovenYu/WonderWorld</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>177</td>
        <td>💡 未复现</td>
        <td>Relightable Gaussian Codec Avatars</td>
        <td>Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam</td>
        <td>https://arxiv.org/pdf/2312.03704</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/goliath</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>178</td>
        <td>💡 未复现</td>
        <td>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</td>
        <td>Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, Jingren Zhou</td>
        <td>https://arxiv.org/pdf/2311.17002</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ali-vilab/Ranni</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>179</td>
        <td>💡 未复现</td>
        <td>Rethinking Inductive Biases for Surface Normal Estimation</td>
        <td>Gwangbin Bae, Andrew J. Davison</td>
        <td>https://arxiv.org/pdf/2403.00712</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/baegwangbin/DSINE</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>180</td>
        <td>💡 未复现</td>
        <td>PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness</td>
        <td>Anh-Quan Cao, Angela Dai, Raoul de Charette</td>
        <td>https://arxiv.org/abs/2312.02158</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/astra-vision/PaSCo</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>181</td>
        <td>💡 未复现</td>
        <td>Transcriptomics-guided Slide Representation Learning in Computational Pathology</td>
        <td>Guillaume Jaume, Lukas Oldenburg, Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Thomas Peeters, Andrew H. Song, Faisal Mahmood</td>
        <td>https://arxiv.org/pdf/2405.11618</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/mahmoodlab/TANGLE</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>182</td>
        <td>💡 未复现</td>
        <td>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</td>
        <td>Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn</td>
        <td>https://arxiv.org/pdf/2312.09168</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/DiffusionLight/DiffusionLight?tab=readme-ov-file</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>183</td>
        <td>💡 未复现</td>
        <td>URHand: Universal Relightable Hands</td>
        <td>Zhaoxi Chen, Gyeongsik Moon, Kaiwen Guo, Chen Cao, Stanislav Pidhorskyi, Tomas Simon, Rohan Joshi, Yuan Dong, Yichen Xu, Bernardo Pires, He Wen, Lucas Evans, Bo Peng, Julia Buffalini, Autumn Trimble, Kevyn McPhail, Melissa Schoeller, Shoou-I Yu, Javier Romero, Michael Zollhöfer, Yaser Sheikh, Ziwei Liu, Shunsuke Saito</td>
        <td>https://arxiv.org/pdf/2401.05334</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/goliath</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>184</td>
        <td>💡 未复现</td>
        <td>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</td>
        <td>Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler</td>
        <td>https://arxiv.org/pdf/2312.02145</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/prs-eth/marigold</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>185</td>
        <td>💡 未复现</td>
        <td>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</td>
        <td>Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang</td>
        <td>https://arxiv.org/pdf/2311.12198</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/XPandora/PhysGaussian</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>186</td>
        <td>💡 未复现</td>
        <td>HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</td>
        <td>Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</td>
        <td>https://arxiv.org/pdf/2311.17061</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/alvinliu0/HumanGaussian</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>187</td>
        <td>💡 未复现</td>
        <td>Prompt Highlighter: Interactive Control for Multi-Modal LLMs</td>
        <td>Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia</td>
        <td>https://arxiv.org/pdf/2312.04302</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/dvlab-research/Prompt-Highlighter/</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>188</td>
        <td>💡 未复现</td>
        <td>Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation</td>
        <td>Qi Yang, Xing Nie, Tong Li, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang</td>
        <td>https://arxiv.org/pdf/2312.06462</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://arxiv.org/pdf/2312.06462</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>189</td>
        <td>💡 未复现</td>
        <td>Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution</td>
        <td>Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, Chen Change Loy</td>
        <td>https://arxiv.org/pdf/2312.06640</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/sczhou/Upscale-A-Video</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>190</td>
        <td>💡 未复现</td>
        <td>Putting the Object Back into Video Object Segmentation</td>
        <td>Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, Alexander Schwing</td>
        <td>https://arxiv.org/pdf/2310.12982</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/hkchengrex/Cutie</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>191</td>
        <td>💡 未复现</td>
        <td>InstanceDiffusion: Instance-level Control for Image Generation</td>
        <td>Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</td>
        <td>https://arxiv.org/pdf/2402.03290</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/frank-xwang/InstanceDiffusion</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>192</td>
        <td>💡 未复现</td>
        <td>OMG-Seg: Is One Model Good Enough For All Segmentation?</td>
        <td>Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy</td>
        <td>https://arxiv.org/pdf/2401.10229</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/lxtGH/OMG-Seg</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>193</td>
        <td>💡 未复现</td>
        <td>Towards Language-Driven Video Inpainting via Multimodal Large Language Models</td>
        <td>Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy</td>
        <td>https://arxiv.org/pdf/2401.10226</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/jianzongwu/Language-Driven-Video-Inpainting</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>194</td>
        <td>💡 未复现</td>
        <td>VBench: Comprehensive Benchmark Suite for Video Generative Models</td>
        <td>Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu</td>
        <td>https://arxiv.org/pdf/2311.17982</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Vchitect/VBench</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>195</td>
        <td>💡 未复现</td>
        <td>PIGEON: Predicting Image Geolocations</td>
        <td>Lukas Haas, Michal Skreta, Silas Alberti, Chelsea Finn</td>
        <td>https://arxiv.org/pdf/2307.05845</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LukasHaas/PIGEON</td>
        <td>oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>196</td>
        <td>💡 未复现</td>
        <td>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</td>
        <td>Yuming Gu, You Xie, Hongyi Xu, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, Linjie Luo</td>
        <td>https://arxiv.org/pdf/2312.13016</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/FreedomGu/DiffPortrait3D/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>197</td>
        <td>💡 未复现</td>
        <td>Domain Prompt Learning with Quaternion Networks</td>
        <td>Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, Xiaokang Yang</td>
        <td>https://arxiv.org/abs/2312.08878</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/caoql98/DPLQ</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>198</td>
        <td>💡 未复现</td>
        <td>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</td>
        <td>Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai</td>
        <td>https://arxiv.org/pdf/2306.14435</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Yujun-Shi/DragDiffusion</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>199</td>
        <td>💡 未复现</td>
        <td>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps</td>
        <td>Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen</td>
        <td>https://arxiv.org/pdf/2312.00094</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/zju-pi/diff-sampler</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>200</td>
        <td>💡 未复现</td>
        <td>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</td>
        <td>Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, Diana Marculescu</td>
        <td>https://arxiv.org/pdf/2312.17681</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://jeff-liangf.github.io/projects/flowvid/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>201</td>
        <td>💡 未复现</td>
        <td>General Object Foundation Model for Images and Videos at Scale</td>
        <td>Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai</td>
        <td>https://arxiv.org/pdf/2312.09158</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/FoundationVision/GLEE</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>202</td>
        <td>💡 未复现</td>
        <td>Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding</td>
        <td>Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_Insect-Foundation_A_Foundation_Model_and_Large-scale_1M_Dataset_for_Visual_CVPR_2024_paper.html</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://uark-cviu.github.io/projects/insect-foundation/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>203</td>
        <td>💡 未复现</td>
        <td>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</td>
        <td>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang</td>
        <td>https://arxiv.org/abs/2403.18036</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/afford-motion/afford-motion</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>204</td>
        <td>💡 未复现</td>
        <td>Object Recognition as Next Token Predictio</td>
        <td>Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim</td>
        <td>https://arxiv.org/abs/2312.02142</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/KaiyuYue/nxtp</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>205</td>
        <td>💡 未复现</td>
        <td>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</td>
        <td>Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, Pinar Yanardag</td>
        <td>https://arxiv.org/abs/2312.04524</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/rehglab/RAVE</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>206</td>
        <td>💡 未复现</td>
        <td>Readout Guidance: Learning Control from Diffusion Features</td>
        <td>Grace Luo, Trevor Darrell, Oliver Wang, Dan B Goldman, Aleksander Holynski</td>
        <td>https://arxiv.org/abs/2312.02150</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/google-research/readout_guidance</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>207</td>
        <td>💡 未复现</td>
        <td>Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark</td>
        <td>Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard</td>
        <td>https://arxiv.org/abs/2403.18821</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/facebookresearch/real-acoustic-fields</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>208</td>
        <td>💡 未复现</td>
        <td>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D</td>
        <td>Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, Xiaoguang Han</td>
        <td>https://arxiv.org/abs/2311.16918</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/modelscope/RichDreamer</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>209</td>
        <td>💡 未复现</td>
        <td>RobustSAM: Segment Anything Robustly on Degraded Images</td>
        <td>Wei-Ting Chen, Yu-Jiet Vong, Sy-Yen Kuo, Sizhuo Ma, Jian Wang</td>
        <td>https://arxiv.org/abs/2406.09627</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/robustsam/RobustSAM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>210</td>
        <td>💡 未复现</td>
        <td>Scaling Up Dynamic Human-Scene Interaction Modeling</td>
        <td>Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang</td>
        <td>https://arxiv.org/abs/2403.08629</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/jnnan/trumans_utils</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>211</td>
        <td>💡 未复现</td>
        <td>SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection</td>
        <td>Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek</td>
        <td>https://arxiv.org/abs/2402.17323</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>212</td>
        <td>💡 未复现</td>
        <td>SpatialTracker: Tracking Any 2D Pixels in 3D Space</td>
        <td>Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou</td>
        <td>https://arxiv.org/abs/2404.04319</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/henry123-boy/SpaTracker</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>213</td>
        <td>💡 未复现</td>
        <td>TFMQ-DM:Temporal Feature Maintenance Quantization for Diffusion Models</td>
        <td>Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu</td>
        <td>https://openaccess.thecvf.com/content/CVPR2024/html/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.html</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ModelTC/TFMQ-DM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>214</td>
        <td>💡 未复现</td>
        <td>Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval</td>
        <td>Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao</td>
        <td>https://arxiv.org/abs/2403.17998</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/patrick-0817/T-MASS-text-video-retrieval</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>215</td>
        <td>💡 未复现</td>
        <td>Towards Learning a Generalist Model for Embodied Navigation</td>
        <td>Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang</td>
        <td>https://arxiv.org/abs/2312.02010</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/LaVi-Lab/NaviLLM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>216</td>
        <td>💡 未复现</td>
        <td>UniMODE: Unified Monocular 3D Object Detection</td>
        <td>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao</td>
        <td>https://arxiv.org/abs/2402.18573</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/Lizhuoling/UniMODE</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>217</td>
        <td>💡 未复现</td>
        <td>Unsupervised Keypoints from Pretrained Diffusion Models</td>
        <td>Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi</td>
        <td>https://arxiv.org/abs/2312.00065</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/ubc-vision/StableKeypoints</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>218</td>
        <td>💡 未复现</td>
        <td>VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models</td>
        <td>Xiang Li, Qianli Shen, Kenji Kawaguchi</td>
        <td>https://arxiv.org/abs/2312.00057</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/South7X/VA3</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>219</td>
        <td>💡 未复现</td>
        <td>VecFusion: Vector Font Generation with Diffusion</td>
        <td>Vikas Thamizharasan, Difan Liu, Shantanu Agarwal, Matthew Fisher, Michael Gharbi, Oliver Wang, Alec Jacobson, Evangelos Kalogerakis</td>
        <td>https://arxiv.org/abs/2312.10540</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://vikastmz.github.io/VecFusion/</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>220</td>
        <td>💡 未复现</td>
        <td>Wonder3D: Single Image to 3D using Cross-Domain Diffusion</td>
        <td>Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang</td>
        <td>https://arxiv.org/abs/2310.15008</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/xxlong0/Wonder3D</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>221</td>
        <td>💡 未复现</td>
        <td>VTimeLLM: Empower LLM to Grasp Video Moments</td>
        <td>Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu</td>
        <td>https://arxiv.org/abs/2311.18445</td>
        <td>2024</td>
        <td>CVPR</td>
        <td>https://github.com/huangb23/VTimeLLM</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>222</td>
        <td>💡 未复现</td>
        <td>LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</td>
        <td>Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo</td>
        <td>https://arxiv.org/pdf/2503.10625v1</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/aigc3d/LHM?tab=readme-ov-file</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>223</td>
        <td>💡 未复现</td>
        <td>EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer</td>
        <td>Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, Jiaming Liu</td>
        <td>https://arxiv.org/abs/2503.07027</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Xiaojiu-z/EasyControl</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>224</td>
        <td>💡 未复现</td>
        <td>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</td>
        <td>Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</td>
        <td>https://arxiv.org/pdf/2403.14627</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/donydchen/mvsplat</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>225</td>
        <td>💡 未复现</td>
        <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
        <td>Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang</td>
        <td>https://arxiv.org/abs/2312.00451</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/VITA-Group/FSGS</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>226</td>
        <td>💡 未复现</td>
        <td>ZigMa: A DiT-style Zigzag Mamba Diffusion Model</td>
        <td>Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Schusterbauer, Björn Ommer</td>
        <td>https://arxiv.org/pdf/2403.13802</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/CompVis/zigma</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>227</td>
        <td>💡 未复现</td>
        <td>Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors</td>
        <td>Tongkun Guan, Wei Shen, Xue Yang, Xuehui Wang, Xiaokang Yang<br></td>
        <td>https://arxiv.org/pdf/2312.05286</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/SJTU-DeepVisionLab/FreeReal</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>228</td>
        <td>💡 未复现</td>
        <td>PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer</td>
        <td>Tongkun Guan, Chengyu Lin, Wei Shen, Xiaokang Yang</td>
        <td>https://arxiv.org/pdf/2407.07764</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/SJTU-DeepVisionLab/PosFormer</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>229</td>
        <td>💡 未复现</td>
        <td>Fully Sparse 3D Occupancy Prediction</td>
        <td>Haisong Liu, Yang Chen, Haiguang Wang, Zetong Yang, Tianyu Li, Jia Zeng, Li Chen, Hongyang Li, Limin Wang</td>
        <td>https://arxiv.org/abs/2312.17118</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/MCG-NJU/SparseOcc</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>230</td>
        <td>💡 未复现</td>
        <td>NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields</td>
        <td>Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</td>
        <td>https://arxiv.org/abs/2404.01300</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/zubair-irshad/NeRF-MAE</td>
        <td>Poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>231</td>
        <td>💡 未复现</td>
        <td>ControlCap: Controllable Region-level Captioning</td>
        <td>Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Fang Wan, Qixiang Ye</td>
        <td>https://arxiv.org/abs/2401.17910</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/callsys/ControlCap</td>
        <td>Poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>232</td>
        <td>💡 未复现</td>
        <td>GiT: Towards Generalist Vision Transformer through Universal Language Interface</td>
        <td>Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang</td>
        <td>https://arxiv.org/abs/2403.09394</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/Haiyang-W/GiT</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>233</td>
        <td>💡 未复现</td>
        <td>Relation DETR: Exploring Explicit Position Relation Prior for Object Detection</td>
        <td>Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen, Xuguang Lan</td>
        <td>https://arxiv.org/abs/2407.11699v1</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/xiuqhou/Relation-DETR</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>234</td>
        <td>💡 未复现</td>
        <td>FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification</td>
        <td>Yu Tian, Congcong Wen, Min Shi, Muhammad Muneeb Afzal, Hao Huang, Muhammad Osama Khan, Yan Luo, Yi Fang, Mengyu Wang</td>
        <td>https://arxiv.org/abs/2407.08813</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>235</td>
        <td>💡 未复现</td>
        <td>OneRestore: A Universal Restoration Framework for Composite Degradation</td>
        <td>Yu Guo, Yuan Gao, Yuxu Lu, Huilin Zhu, Ryan Wen Liu, Shengfeng He</td>
        <td>https://arxiv.org/abs/2407.04621</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/gy65896/OneRestore</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>236</td>
        <td>💡 未复现</td>
        <td>VideoStudio: Generating Consistent-Content and Multi-Scene Videos</td>
        <td>Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei</td>
        <td>https://arxiv.org/pdf/2401.01256</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/FuchenUSTC/VideoStudio</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>237</td>
        <td>💡 未复现</td>
        <td>Zero-shot Object Counting with Good Exemplars</td>
        <td>Huilin Zhu, Jingling Yuan, Zhengwei Yang, Yu Guo, Zheng Wang, Xian Zhong, Shengfeng He</td>
        <td>https://arxiv.org/abs/2407.04948</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/HopooLinZ/VA-Count</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>238</td>
        <td>💡 未复现</td>
        <td>SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments</td>
        <td>Niklas Gard, Anna Hilsmann, Peter Eisert</td>
        <td>https://arxiv.org/abs/2404.10527</td>
        <td>2024</td>
        <td>ECCV</td>
        <td>https://github.com/fraunhoferhhi/spvloc</td>
        <td>Oral</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>239</td>
        <td>💡 未复现</td>
        <td>Stereo Any Video: Temporally Consistent Stereo Matching</td>
        <td>Junpeng Jing;Weixun Luo;Ye Mao;  Krystian Mikolajczyk</td>
        <td>https://arxiv.org/html/2503.05549v1#S4</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/TomTomTommi/stereoanyvideo</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>240</td>
        <td>💡 未复现</td>
        <td>InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</td>
        <td>Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, Xin Lu</td>
        <td>https://arxiv.org/pdf/2503.16418</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/bytedance/InfiniteYou</td>
        <td>highlight</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>241</td>
        <td>💡 未复现</td>
        <td>MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization</td>
        <td>Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin</td>
        <td>https://arxiv.org/abs/2408.02555</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/buaacyw/MeshAnythingV2</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>242</td>
        <td>💡 未复现</td>
        <td>From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers</td>
        <td>Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang</td>
        <td>https://arxiv.org/abs/2503.06923</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Shenyi-Z/TaylorSeer</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>243</td>
        <td>💡 未复现</td>
        <td>MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion</td>
        <td>Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, Yu-Gang Jiang</td>
        <td>https://arxiv.org/abs/2405.20325</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/Francis-Rings/MotionFollower?tab=readme-ov-file</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>244</td>
        <td>💡 未复现</td>
        <td>AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</td>
        <td>Zijie Wu, Chaohui Yu, Fan Wang, Xiang Bai</td>
        <td>https://arxiv.org/abs/2506.09982</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/JarrentWu1031/AnimateAnyMesh</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>245</td>
        <td>💡 未复现</td>
        <td>VSSD: Vision Mamba with Non-Causal State Space Duality</td>
        <td>Yuheng Shi, Minjing Dong, Mingjia Li, Chang Xu</td>
        <td>https://arxiv.org/abs/2407.18559</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/YuHengsss/VSSD?tab=readme-ov-file</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>246</td>
        <td>💡 未复现</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Balanced Image Stylization with Style Matching Score&quot;</td>
        <td>Yuxin Jiang, Liming Jiang, Shuai Yang, Jia-Wei Liu, Ivor Tsang, Mike Zheng Shou</td>
        <td>https://arxiv.org/abs/2503.07601</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/showlab/SMS</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>247</td>
        <td>💡 未复现</td>
        <td>FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model</td>
        <td>Yukang Cao、Chenyang Si、Jinghao Wang、Ziwei Liu</td>
        <td>https://arxiv.org/html/2507.01953v1#S4</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/yukangcao/FreeMorph</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>248</td>
        <td>💡 未复现</td>
        <td>GENMO: A GENeralist Model for Human MOtion</td>
        <td>Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, Ye Yuan<br></td>
        <td>https://arxiv.org/abs/2505.01425</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://research.nvidia.com/labs/dair/genmo/</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>249</td>
        <td>💡 未复现</td>
        <td>Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers</td>
        <td>Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, Wenhu Chen</td>
        <td>https://arxiv.org/abs/2503.11579</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/TIGER-AI-Lab/Vamba</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>250</td>
        <td>💡 未复现</td>
        <td>Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning</td>
        <td>Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen</td>
        <td>https://arxiv.org/abs/2506.02327</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/scott-yjyang/MeWM</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>251</td>
        <td>💡 未复现</td>
        <td>Where, What, Why: Towards Explainable Driver Attention Prediction</td>
        <td>Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, Xiaobo Xia, Chao Gou</td>
        <td>https://arxiv.org/pdf/2506.23088</td>
        <td>2025</td>
        <td>ICCV</td>
        <td>https://github.com/yuchen2199/Explainable-Driver-Attention-Prediction</td>
        <td>poster</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
</table>